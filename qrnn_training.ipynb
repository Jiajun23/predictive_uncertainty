{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QRNN Training\n",
    "\n",
    "This notebook trains several *quantile regression neural networks* (QRNNs) on differently sized training sets with different numbers of channels (5 and 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KERAS_BACKEND=tensorflow\n",
      "env: OMP_NUM_THREADS=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env KERAS_BACKEND=tensorflow\n",
    "%env OMP_NUM_THREADS=4\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib_settings\n",
    "from typhon.retrieval.qrnn import QRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = np.load(\"data/x_train_5.npy\")\n",
    "y_train = np.load(\"data/y_train_5.npy\")\n",
    "quantiles = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8/8 [==============================] - 0s - loss: 97.0083 - val_loss: 94.0189\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s - loss: 92.9479 - val_loss: 92.0918\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s - loss: 93.2582 - val_loss: 89.0061\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s - loss: 87.5402 - val_loss: 83.4942\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s - loss: 79.5998 - val_loss: 72.3627\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s - loss: 64.0521 - val_loss: 49.0884\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s - loss: 36.9218 - val_loss: 26.1376\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s - loss: 20.7449 - val_loss: 18.8369\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s - loss: 15.9743 - val_loss: 15.1117\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s - loss: 12.7492 - val_loss: 12.6238\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s - loss: 9.7702 - val_loss: 10.7988\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s - loss: 9.0828 - val_loss: 9.6807\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s - loss: 8.0079 - val_loss: 8.7322\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s - loss: 7.5374 - val_loss: 8.1869\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s - loss: 6.4251 - val_loss: 7.6692\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s - loss: 6.2525 - val_loss: 7.2684\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s - loss: 5.6452 - val_loss: 7.0134\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s - loss: 6.1385 - val_loss: 6.7410\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s - loss: 5.5837 - val_loss: 6.6561\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s - loss: 5.2614 - val_loss: 6.3138\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s - loss: 5.0351 - val_loss: 6.2354\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s - loss: 5.0703 - val_loss: 6.0674\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s - loss: 4.9009 - val_loss: 5.6639\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s - loss: 4.7008 - val_loss: 5.6104\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s - loss: 4.6399 - val_loss: 5.7059\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s - loss: 4.4447 - val_loss: 5.4193\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s - loss: 4.3068 - val_loss: 5.3903\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s - loss: 4.3935 - val_loss: 5.2887\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s - loss: 4.0719 - val_loss: 4.9652\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s - loss: 4.1252 - val_loss: 5.1450\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s - loss: 4.1048 - val_loss: 5.0707\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 0s - loss: 3.6931 - val_loss: 5.2615\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 0s - loss: 4.1864 - val_loss: 4.9457\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 0s - loss: 3.8293 - val_loss: 4.8475\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 0s - loss: 3.9416 - val_loss: 5.0407\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 0s - loss: 3.7006 - val_loss: 4.7261\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 0s - loss: 3.3772 - val_loss: 4.8486\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 0s - loss: 3.7963 - val_loss: 4.8136\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 0s - loss: 3.6118 - val_loss: 4.6735\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 0s - loss: 3.7343 - val_loss: 4.5778\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 0s - loss: 3.4386 - val_loss: 4.9823\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 0s - loss: 3.3168 - val_loss: 4.5523\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 0s - loss: 3.5040 - val_loss: 4.9608\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 0s - loss: 3.3523 - val_loss: 4.3731\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 0s - loss: 3.6468 - val_loss: 4.6126\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 0s - loss: 3.1944 - val_loss: 4.2840\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 0s - loss: 3.4027 - val_loss: 4.7214\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 0s - loss: 3.3029 - val_loss: 4.3685\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 0s - loss: 3.0715 - val_loss: 4.3179\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 0s - loss: 3.4480 - val_loss: 4.2084\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 0s - loss: 3.3521 - val_loss: 4.5538\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 0s - loss: 3.4194 - val_loss: 4.2042\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 0s - loss: 3.5927 - val_loss: 4.6550\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 0s - loss: 3.4811 - val_loss: 3.9638\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 0s - loss: 3.5164 - val_loss: 4.6484\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 0s - loss: 3.3981 - val_loss: 4.5086\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 0s - loss: 3.4149 - val_loss: 4.0255\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 0s - loss: 3.3244 - val_loss: 4.3978\n",
      "Epoch 59/200\n",
      "8/8 [==============================] - 0s - loss: 3.6034 - val_loss: 4.4774\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 0s - loss: 3.4896 - val_loss: 4.7529\n",
      "Epoch 61/200\n",
      "8/8 [==============================] - 0s - loss: 3.5414 - val_loss: 4.8451\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 0s - loss: 3.4754 - val_loss: 4.4675\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 0s - loss: 3.3354 - val_loss: 4.7631\n",
      "Epoch 64/200\n",
      "8/8 [==============================] - 0s - loss: 3.5038 - val_loss: 4.5507\n",
      "Epoch 65/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 3.4681\n",
      " Reduced learning rate to 0.01\n",
      "8/8 [==============================] - 0s - loss: 3.6080 - val_loss: 4.2968\n",
      "Epoch 66/200\n",
      "8/8 [==============================] - 0s - loss: 2.7281 - val_loss: 3.7460\n",
      "Epoch 67/200\n",
      "8/8 [==============================] - 0s - loss: 2.5601 - val_loss: 3.7238\n",
      "Epoch 68/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.7943\n",
      " Reduced learning rate to 0.005\n",
      "8/8 [==============================] - 0s - loss: 2.8390 - val_loss: 3.7772\n",
      "Epoch 69/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.3793\n",
      " Reduced learning rate to 0.0025\n",
      "8/8 [==============================] - 0s - loss: 2.5327 - val_loss: 3.8956\n",
      "Epoch 70/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.6378\n",
      " Reduced learning rate to 0.00125\n",
      "8/8 [==============================] - 0s - loss: 2.6589 - val_loss: 3.8489\n",
      "Epoch 71/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.4618\n",
      " Reduced learning rate to 0.000625\n",
      "8/8 [==============================] - 0s - loss: 2.8027 - val_loss: 3.7862\n",
      "Epoch 72/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.8327\n",
      " Reduced learning rate to 0.0003125\n",
      "8/8 [==============================] - 0s - loss: 2.6306 - val_loss: 3.8473\n",
      "Epoch 73/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 3.3803\n",
      " Reduced learning rate to 0.00015625\n",
      "8/8 [==============================] - 0s - loss: 2.8891 - val_loss: 3.8891\n",
      "Epoch 74/200\n",
      "8/8 [==============================] - 0s - loss: 2.8822 - val_loss: 3.6481\n",
      "Epoch 75/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.8203\n",
      " Reduced learning rate to 7.8125e-05\n",
      "8/8 [==============================] - 0s - loss: 2.4925 - val_loss: 3.7546\n",
      "Epoch 76/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.9496\n",
      " Reduced learning rate to 3.90625e-05\n",
      "8/8 [==============================] - 0s - loss: 2.7103 - val_loss: 3.7826\n",
      "Epoch 77/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.5517\n",
      " Reduced learning rate to 1.95312e-05\n",
      "8/8 [==============================] - 0s - loss: 2.5647 - val_loss: 3.7273\n",
      "Epoch 78/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.3533\n",
      " Reduced learning rate to 9.76562e-06\n",
      "8/8 [==============================] - 0s - loss: 2.6392 - val_loss: 3.6873\n",
      "Epoch 79/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.7660\n",
      " Reduced learning rate to 4.88281e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s - loss: 2.7653 - val_loss: 3.7151\n",
      "Epoch 80/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.3740\n",
      " Reduced learning rate to 2.44141e-06\n",
      "8/8 [==============================] - 0s - loss: 2.7179 - val_loss: 3.7832\n",
      "Epoch 81/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.7021\n",
      " Reduced learning rate to 1.2207e-06\n",
      "8/8 [==============================] - 0s - loss: 2.9824 - val_loss: 3.7581\n",
      "Epoch 82/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.2870\n",
      " Reduced learning rate to 6.10352e-07\n",
      "8/8 [==============================] - 0s - loss: 2.5311 - val_loss: 3.7334\n",
      "Epoch 1/200\n",
      "8/8 [==============================] - 0s - loss: 95.9085 - val_loss: 94.3889\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s - loss: 95.1578 - val_loss: 93.0042\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s - loss: 91.6665 - val_loss: 90.9902\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s - loss: 91.8662 - val_loss: 87.7041\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s - loss: 85.8121 - val_loss: 81.9667\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s - loss: 78.5239 - val_loss: 70.9501\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s - loss: 61.1002 - val_loss: 48.8304\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s - loss: 38.7604 - val_loss: 25.4676\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s - loss: 21.1970 - val_loss: 19.0313\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s - loss: 16.2285 - val_loss: 15.3810\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s - loss: 12.7212 - val_loss: 12.7783\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s - loss: 10.6301 - val_loss: 10.8455\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s - loss: 8.9978 - val_loss: 9.6021\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s - loss: 7.8214 - val_loss: 8.6780\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s - loss: 6.9050 - val_loss: 7.8176\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s - loss: 6.4300 - val_loss: 7.3963\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s - loss: 6.1276 - val_loss: 7.1106\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s - loss: 5.7147 - val_loss: 6.8308\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s - loss: 5.5627 - val_loss: 6.6376\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s - loss: 5.1655 - val_loss: 6.3516\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s - loss: 5.1265 - val_loss: 6.1897\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s - loss: 5.0043 - val_loss: 5.8702\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s - loss: 4.7011 - val_loss: 5.7116\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s - loss: 4.9028 - val_loss: 5.5903\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s - loss: 4.7650 - val_loss: 5.5794\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s - loss: 4.1626 - val_loss: 5.3635\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s - loss: 4.2111 - val_loss: 5.2816\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s - loss: 3.9880 - val_loss: 5.3268\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s - loss: 4.2042 - val_loss: 5.2283\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s - loss: 4.0173 - val_loss: 5.0675\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s - loss: 4.0295 - val_loss: 5.0599\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 0s - loss: 3.6853 - val_loss: 4.8762\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 0s - loss: 3.7691 - val_loss: 4.8853\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 0s - loss: 3.6897 - val_loss: 4.9875\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 0s - loss: 4.1253 - val_loss: 4.9996\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 0s - loss: 3.7133 - val_loss: 4.9005\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 0s - loss: 3.6332 - val_loss: 4.7283\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 0s - loss: 3.4673 - val_loss: 4.8365\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 0s - loss: 3.5910 - val_loss: 5.2520\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 0s - loss: 3.5603 - val_loss: 4.8093\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 0s - loss: 3.5109 - val_loss: 4.6237\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 0s - loss: 3.7483 - val_loss: 4.5785\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 0s - loss: 3.4734 - val_loss: 4.5059\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 0s - loss: 3.2463 - val_loss: 4.4692\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 0s - loss: 3.5777 - val_loss: 4.7458\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 0s - loss: 3.4862 - val_loss: 4.2987\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 0s - loss: 3.3470 - val_loss: 4.2937\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 0s - loss: 3.4431 - val_loss: 4.4593\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 0s - loss: 3.2608 - val_loss: 4.5085\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 0s - loss: 3.4239 - val_loss: 4.4528\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 0s - loss: 3.0118 - val_loss: 4.1135\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 0s - loss: 3.2320 - val_loss: 4.4864\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 0s - loss: 3.2158 - val_loss: 4.3952\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 0s - loss: 3.2043 - val_loss: 4.4844\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 0s - loss: 3.5355 - val_loss: 4.2268\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 0s - loss: 3.4040 - val_loss: 4.0211\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 0s - loss: 2.7061 - val_loss: 4.6178\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 0s - loss: 3.4771 - val_loss: 4.4485\n",
      "Epoch 59/200\n",
      "8/8 [==============================] - 0s - loss: 3.3448 - val_loss: 4.6124\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 0s - loss: 3.4128 - val_loss: 4.8500\n",
      "Epoch 61/200\n",
      "8/8 [==============================] - 0s - loss: 3.7130 - val_loss: 4.2851\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 0s - loss: 3.3437 - val_loss: 4.5635\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 0s - loss: 3.3552 - val_loss: 4.8507\n",
      "Epoch 64/200\n",
      "8/8 [==============================] - 0s - loss: 3.6061 - val_loss: 4.2285\n",
      "Epoch 65/200\n",
      "8/8 [==============================] - 0s - loss: 3.4407 - val_loss: 4.6814\n",
      "Epoch 66/200\n",
      "8/8 [==============================] - 0s - loss: 3.3058 - val_loss: 4.5276\n",
      "Epoch 67/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 3.4575\n",
      " Reduced learning rate to 0.01\n",
      "8/8 [==============================] - 0s - loss: 3.2577 - val_loss: 4.3974\n",
      "Epoch 68/200\n",
      "8/8 [==============================] - 0s - loss: 2.6764 - val_loss: 3.7537\n",
      "Epoch 69/200\n",
      "8/8 [==============================] - 0s - loss: 2.8915 - val_loss: 3.7189\n",
      "Epoch 70/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.3899\n",
      " Reduced learning rate to 0.005\n",
      "8/8 [==============================] - 0s - loss: 2.6508 - val_loss: 3.7916\n",
      "Epoch 71/200\n",
      "8/8 [==============================] - 0s - loss: 2.8203 - val_loss: 3.6309\n",
      "Epoch 72/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 3.2185\n",
      " Reduced learning rate to 0.0025\n",
      "8/8 [==============================] - 0s - loss: 2.5871 - val_loss: 3.6324\n",
      "Epoch 73/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.7993\n",
      " Reduced learning rate to 0.00125\n",
      "8/8 [==============================] - 0s - loss: 2.7786 - val_loss: 3.8043\n",
      "Epoch 74/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.3103\n",
      " Reduced learning rate to 0.000625\n",
      "8/8 [==============================] - 0s - loss: 2.5894 - val_loss: 3.6943\n",
      "Epoch 75/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.3015\n",
      " Reduced learning rate to 0.0003125\n",
      "8/8 [==============================] - 0s - loss: 2.9668 - val_loss: 3.6751\n",
      "Epoch 76/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.6310\n",
      " Reduced learning rate to 0.00015625\n",
      "8/8 [==============================] - 0s - loss: 2.5349 - val_loss: 3.6392\n",
      "Epoch 77/200\n",
      "8/8 [==============================] - 0s - loss: 2.7005 - val_loss: 3.5259\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/8 [==>...........................] - ETA: 0s - loss: 2.5560\n",
      " Reduced learning rate to 7.8125e-05\n",
      "8/8 [==============================] - 0s - loss: 2.5672 - val_loss: 3.6100\n",
      "Epoch 79/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.2373\n",
      " Reduced learning rate to 3.90625e-05\n",
      "8/8 [==============================] - 0s - loss: 2.5771 - val_loss: 3.7640\n",
      "Epoch 80/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 3.0421\n",
      " Reduced learning rate to 1.95312e-05\n",
      "8/8 [==============================] - 0s - loss: 2.5379 - val_loss: 3.7364\n",
      "Epoch 81/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.2200\n",
      " Reduced learning rate to 9.76562e-06\n",
      "8/8 [==============================] - 0s - loss: 2.5846 - val_loss: 3.6085\n",
      "Epoch 82/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.4312\n",
      " Reduced learning rate to 4.88281e-06\n",
      "8/8 [==============================] - 0s - loss: 2.6293 - val_loss: 3.6565\n",
      "Epoch 83/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.5040\n",
      " Reduced learning rate to 2.44141e-06\n",
      "8/8 [==============================] - 0s - loss: 2.8930 - val_loss: 3.6489\n",
      "Epoch 84/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.2209\n",
      " Reduced learning rate to 1.2207e-06\n",
      "8/8 [==============================] - 0s - loss: 2.4292 - val_loss: 3.7120\n",
      "Epoch 85/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.3548\n",
      " Reduced learning rate to 6.10352e-07\n",
      "8/8 [==============================] - 0s - loss: 2.5021 - val_loss: 3.7057\n",
      "Epoch 1/200\n",
      "8/8 [==============================] - 0s - loss: 96.8594 - val_loss: 94.6482\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s - loss: 95.0602 - val_loss: 93.3614\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s - loss: 95.9327 - val_loss: 91.5616\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s - loss: 93.1550 - val_loss: 88.6419\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s - loss: 89.7508 - val_loss: 83.4303\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s - loss: 79.2218 - val_loss: 73.3291\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s - loss: 64.8951 - val_loss: 52.7931\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s - loss: 42.3884 - val_loss: 27.7917\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s - loss: 22.5714 - val_loss: 19.3325\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s - loss: 16.5040 - val_loss: 15.3498\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s - loss: 12.5606 - val_loss: 12.6553\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s - loss: 10.5385 - val_loss: 10.6000\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s - loss: 8.7176 - val_loss: 9.3596\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s - loss: 8.0699 - val_loss: 8.3055\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s - loss: 6.8252 - val_loss: 7.7953\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s - loss: 6.3945 - val_loss: 7.3730\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s - loss: 5.6329 - val_loss: 7.0956\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s - loss: 5.7477 - val_loss: 6.8100\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s - loss: 5.5431 - val_loss: 6.5588\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s - loss: 5.3158 - val_loss: 6.2808\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s - loss: 4.7180 - val_loss: 6.2531\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s - loss: 4.8638 - val_loss: 6.0202\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s - loss: 5.0890 - val_loss: 5.9611\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s - loss: 4.7057 - val_loss: 5.7062\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s - loss: 4.6352 - val_loss: 5.6177\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s - loss: 4.6470 - val_loss: 5.3876\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s - loss: 4.2369 - val_loss: 5.3638\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s - loss: 4.1364 - val_loss: 5.3736\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s - loss: 3.8991 - val_loss: 5.2033\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s - loss: 3.9993 - val_loss: 5.1421\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s - loss: 4.0506 - val_loss: 5.1850\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 0s - loss: 3.7525 - val_loss: 5.0098\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 0s - loss: 3.9591 - val_loss: 4.8862\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 0s - loss: 3.6671 - val_loss: 4.9312\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 0s - loss: 3.4384 - val_loss: 4.8811\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 0s - loss: 3.6298 - val_loss: 4.6973\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 0s - loss: 3.4370 - val_loss: 4.8338\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 0s - loss: 3.5531 - val_loss: 4.9308\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 0s - loss: 3.3212 - val_loss: 4.6587\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 0s - loss: 3.7092 - val_loss: 4.8731\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 0s - loss: 3.2886 - val_loss: 4.8044\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 0s - loss: 3.4773 - val_loss: 4.6505\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 0s - loss: 3.2662 - val_loss: 4.7215\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 0s - loss: 3.4675 - val_loss: 4.5002\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 0s - loss: 3.5063 - val_loss: 4.6110\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 0s - loss: 3.3918 - val_loss: 4.8141\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 0s - loss: 3.6949 - val_loss: 4.2607\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 0s - loss: 3.7862 - val_loss: 4.1992\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 0s - loss: 3.3649 - val_loss: 4.4098\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 0s - loss: 3.2988 - val_loss: 4.5891\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 0s - loss: 3.6284 - val_loss: 4.7888\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 0s - loss: 3.3851 - val_loss: 4.3066\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 0s - loss: 3.4859 - val_loss: 4.9950\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 0s - loss: 3.3174 - val_loss: 4.4908\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 0s - loss: 3.5023 - val_loss: 4.3707\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 0s - loss: 3.7179 - val_loss: 4.4855\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 0s - loss: 3.7188 - val_loss: 4.5975\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 0s - loss: 3.6678 - val_loss: 4.7412\n",
      "Epoch 59/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 3.2945\n",
      " Reduced learning rate to 0.01\n",
      "8/8 [==============================] - 0s - loss: 3.2008 - val_loss: 4.7639\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 0s - loss: 2.9202 - val_loss: 3.8227\n",
      "Epoch 61/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.3942\n",
      " Reduced learning rate to 0.005\n",
      "8/8 [==============================] - 0s - loss: 2.9201 - val_loss: 3.8973\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 0s - loss: 2.9246 - val_loss: 3.8174\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 0s - loss: 3.0407 - val_loss: 3.8064\n",
      "Epoch 64/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 3.8223\n",
      " Reduced learning rate to 0.0025\n",
      "8/8 [==============================] - 0s - loss: 2.8027 - val_loss: 3.8764\n",
      "Epoch 65/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.6664\n",
      " Reduced learning rate to 0.00125\n",
      "8/8 [==============================] - 0s - loss: 2.6172 - val_loss: 3.8816\n",
      "Epoch 66/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 3.2763\n",
      " Reduced learning rate to 0.000625\n",
      "8/8 [==============================] - 0s - loss: 2.7897 - val_loss: 3.8488\n",
      "Epoch 67/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 3.6533\n",
      " Reduced learning rate to 0.0003125\n",
      "8/8 [==============================] - 0s - loss: 2.8327 - val_loss: 4.0039\n",
      "Epoch 68/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.5929\n",
      " Reduced learning rate to 0.00015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s - loss: 2.9352 - val_loss: 3.9760\n",
      "Epoch 69/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 3.0450\n",
      " Reduced learning rate to 7.8125e-05\n",
      "8/8 [==============================] - 0s - loss: 2.9172 - val_loss: 3.9089\n",
      "Epoch 70/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.7654\n",
      " Reduced learning rate to 3.90625e-05\n",
      "8/8 [==============================] - 0s - loss: 2.8973 - val_loss: 3.8195\n",
      "Epoch 71/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.5253\n",
      " Reduced learning rate to 1.95312e-05\n",
      "8/8 [==============================] - 0s - loss: 2.8407 - val_loss: 3.9600\n",
      "Epoch 72/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 3.3650\n",
      " Reduced learning rate to 9.76562e-06\n",
      "8/8 [==============================] - 0s - loss: 2.8211 - val_loss: 3.8950\n",
      "Epoch 73/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.6304\n",
      " Reduced learning rate to 4.88281e-06\n",
      "8/8 [==============================] - 0s - loss: 2.5894 - val_loss: 3.8646\n",
      "Epoch 74/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 3.4222\n",
      " Reduced learning rate to 2.44141e-06\n",
      "8/8 [==============================] - 0s - loss: 2.8395 - val_loss: 3.9048\n",
      "Epoch 75/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.9402\n",
      " Reduced learning rate to 1.2207e-06\n",
      "8/8 [==============================] - 0s - loss: 2.7949 - val_loss: 3.8196\n",
      "Epoch 76/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 3.1923\n",
      " Reduced learning rate to 6.10352e-07\n",
      "8/8 [==============================] - 0s - loss: 2.6136 - val_loss: 3.9512\n",
      "Epoch 1/200\n",
      "8/8 [==============================] - 0s - loss: 97.1882 - val_loss: 94.3321\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s - loss: 94.5279 - val_loss: 92.5596\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s - loss: 95.0280 - val_loss: 89.6797\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s - loss: 89.7630 - val_loss: 84.4603\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s - loss: 81.3070 - val_loss: 73.6544\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s - loss: 67.7828 - val_loss: 51.2032\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s - loss: 41.5312 - val_loss: 26.4310\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s - loss: 21.6737 - val_loss: 18.7307\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s - loss: 15.2951 - val_loss: 15.1149\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s - loss: 12.5238 - val_loss: 12.4331\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s - loss: 10.0215 - val_loss: 10.6641\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s - loss: 9.0474 - val_loss: 9.3869\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s - loss: 7.6264 - val_loss: 8.6306\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s - loss: 7.2912 - val_loss: 7.9111\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s - loss: 6.4172 - val_loss: 7.4483\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s - loss: 5.8976 - val_loss: 7.0604\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s - loss: 5.6898 - val_loss: 6.7876\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s - loss: 5.9256 - val_loss: 6.4890\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s - loss: 5.2858 - val_loss: 6.2401\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s - loss: 5.0182 - val_loss: 6.2874\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s - loss: 5.1225 - val_loss: 6.0496\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s - loss: 4.3264 - val_loss: 5.9754\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s - loss: 4.7614 - val_loss: 5.8170\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s - loss: 4.5000 - val_loss: 5.6602\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s - loss: 4.2170 - val_loss: 5.4978\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s - loss: 4.2274 - val_loss: 5.4935\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s - loss: 4.1155 - val_loss: 5.3242\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s - loss: 4.0580 - val_loss: 5.2354\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s - loss: 3.7942 - val_loss: 5.0782\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s - loss: 4.0539 - val_loss: 5.1628\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s - loss: 4.1417 - val_loss: 5.1303\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 0s - loss: 3.8701 - val_loss: 5.1028\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 0s - loss: 3.8423 - val_loss: 5.0976\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 0s - loss: 3.8954 - val_loss: 4.9446\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 0s - loss: 3.9872 - val_loss: 4.9185\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 0s - loss: 3.7145 - val_loss: 4.8722\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 0s - loss: 3.7638 - val_loss: 4.7589\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 0s - loss: 3.5890 - val_loss: 4.6249\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 0s - loss: 3.9916 - val_loss: 4.8856\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 0s - loss: 3.6884 - val_loss: 4.7924\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 0s - loss: 3.2443 - val_loss: 4.5190\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 0s - loss: 3.3784 - val_loss: 4.4616\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 0s - loss: 3.2247 - val_loss: 4.5537\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 0s - loss: 3.1902 - val_loss: 4.7706\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 0s - loss: 3.4108 - val_loss: 4.6906\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 0s - loss: 3.4273 - val_loss: 4.5604\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 0s - loss: 3.3697 - val_loss: 5.0347\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 0s - loss: 3.7581 - val_loss: 4.5482\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 0s - loss: 3.5239 - val_loss: 4.3767\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 0s - loss: 3.4032 - val_loss: 4.6513\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 0s - loss: 3.5041 - val_loss: 4.4341\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 0s - loss: 3.3308 - val_loss: 4.2565\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 0s - loss: 3.6034 - val_loss: 4.3402\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 0s - loss: 3.1944 - val_loss: 4.3952\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 0s - loss: 3.1483 - val_loss: 4.3128\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 0s - loss: 3.4160 - val_loss: 4.4024\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 0s - loss: 3.2166 - val_loss: 3.9442\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 0s - loss: 3.0164 - val_loss: 4.1761\n",
      "Epoch 59/200\n",
      "8/8 [==============================] - 0s - loss: 3.6647 - val_loss: 4.6940\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 0s - loss: 3.3179 - val_loss: 4.5197\n",
      "Epoch 61/200\n",
      "8/8 [==============================] - 0s - loss: 3.5890 - val_loss: 4.5583\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 0s - loss: 3.7078 - val_loss: 4.6580\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 0s - loss: 3.4175 - val_loss: 4.5684\n",
      "Epoch 64/200\n",
      "8/8 [==============================] - 0s - loss: 3.2813 - val_loss: 4.4022\n",
      "Epoch 65/200\n",
      "8/8 [==============================] - 0s - loss: 3.5280 - val_loss: 4.6446\n",
      "Epoch 66/200\n",
      "8/8 [==============================] - 0s - loss: 3.5244 - val_loss: 4.6878\n",
      "Epoch 67/200\n",
      "8/8 [==============================] - 0s - loss: 3.5121 - val_loss: 4.3829\n",
      "Epoch 68/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 3.3922\n",
      " Reduced learning rate to 0.01\n",
      "8/8 [==============================] - 0s - loss: 3.2639 - val_loss: 4.5008\n",
      "Epoch 69/200\n",
      "8/8 [==============================] - 0s - loss: 2.8951 - val_loss: 3.7794\n",
      "Epoch 70/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.3741\n",
      " Reduced learning rate to 0.005\n",
      "8/8 [==============================] - 0s - loss: 2.6872 - val_loss: 3.8314\n",
      "Epoch 71/200\n",
      "8/8 [==============================] - 0s - loss: 2.6877 - val_loss: 3.6930\n",
      "Epoch 72/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s - loss: 2.6864 - val_loss: 3.6118\n",
      "Epoch 73/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.9557\n",
      " Reduced learning rate to 0.0025\n",
      "8/8 [==============================] - 0s - loss: 2.7709 - val_loss: 3.8089\n",
      "Epoch 74/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.9208\n",
      " Reduced learning rate to 0.00125\n",
      "8/8 [==============================] - 0s - loss: 2.5255 - val_loss: 3.8362\n",
      "Epoch 75/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.7968\n",
      " Reduced learning rate to 0.000625\n",
      "8/8 [==============================] - 0s - loss: 2.6302 - val_loss: 3.7220\n",
      "Epoch 76/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.8241\n",
      " Reduced learning rate to 0.0003125\n",
      "8/8 [==============================] - 0s - loss: 2.6946 - val_loss: 3.7177\n",
      "Epoch 77/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.1992\n",
      " Reduced learning rate to 0.00015625\n",
      "8/8 [==============================] - 0s - loss: 2.6510 - val_loss: 3.6959\n",
      "Epoch 78/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.8581\n",
      " Reduced learning rate to 7.8125e-05\n",
      "8/8 [==============================] - 0s - loss: 2.8333 - val_loss: 3.6863\n",
      "Epoch 79/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.4566\n",
      " Reduced learning rate to 3.90625e-05\n",
      "8/8 [==============================] - 0s - loss: 2.5670 - val_loss: 3.6784\n",
      "Epoch 80/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.5502\n",
      " Reduced learning rate to 1.95312e-05\n",
      "8/8 [==============================] - 0s - loss: 2.7319 - val_loss: 3.6461\n",
      "Epoch 81/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.8805\n",
      " Reduced learning rate to 9.76562e-06\n",
      "8/8 [==============================] - 0s - loss: 2.5528 - val_loss: 3.7064\n",
      "Epoch 82/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.7933\n",
      " Reduced learning rate to 4.88281e-06\n",
      "8/8 [==============================] - 0s - loss: 2.5705 - val_loss: 3.8899\n",
      "Epoch 83/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.3498\n",
      " Reduced learning rate to 2.44141e-06\n",
      "8/8 [==============================] - 0s - loss: 2.7591 - val_loss: 3.7126\n",
      "Epoch 84/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 3.1633\n",
      " Reduced learning rate to 1.2207e-06\n",
      "8/8 [==============================] - 0s - loss: 2.8856 - val_loss: 3.6813\n",
      "Epoch 85/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 3.2678\n",
      " Reduced learning rate to 6.10352e-07\n",
      "8/8 [==============================] - 0s - loss: 2.6848 - val_loss: 3.6917\n",
      "Epoch 1/200\n",
      "8/8 [==============================] - 0s - loss: 95.2727 - val_loss: 94.0655\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s - loss: 92.0607 - val_loss: 92.2253\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s - loss: 93.3048 - val_loss: 89.3695\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s - loss: 91.3267 - val_loss: 84.4346\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s - loss: 82.9160 - val_loss: 75.0434\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s - loss: 69.8808 - val_loss: 55.3257\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s - loss: 44.4096 - val_loss: 31.4987\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s - loss: 27.9572 - val_loss: 21.7244\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s - loss: 18.3039 - val_loss: 17.3800\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s - loss: 14.3615 - val_loss: 14.4574\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s - loss: 11.6219 - val_loss: 12.2478\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s - loss: 9.9876 - val_loss: 10.6448\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s - loss: 8.3537 - val_loss: 9.4425\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s - loss: 8.1598 - val_loss: 8.5121\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s - loss: 7.1094 - val_loss: 8.0136\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s - loss: 6.5488 - val_loss: 7.4657\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s - loss: 6.3156 - val_loss: 7.1175\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s - loss: 5.5524 - val_loss: 6.8484\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s - loss: 5.2976 - val_loss: 6.6964\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s - loss: 5.4660 - val_loss: 6.1596\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s - loss: 5.2888 - val_loss: 6.2470\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s - loss: 4.8548 - val_loss: 6.0882\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s - loss: 4.6090 - val_loss: 5.9450\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s - loss: 4.6290 - val_loss: 5.7078\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s - loss: 4.4220 - val_loss: 5.6994\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s - loss: 4.3219 - val_loss: 5.5471\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s - loss: 4.3866 - val_loss: 5.4961\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s - loss: 4.1776 - val_loss: 5.3882\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s - loss: 4.1137 - val_loss: 5.1008\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s - loss: 4.2447 - val_loss: 5.1753\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s - loss: 4.0667 - val_loss: 5.0893\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 0s - loss: 4.1260 - val_loss: 4.9170\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 0s - loss: 3.7833 - val_loss: 4.9142\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 0s - loss: 3.8907 - val_loss: 4.9357\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 0s - loss: 3.5568 - val_loss: 5.0455\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 0s - loss: 3.4836 - val_loss: 4.8662\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 0s - loss: 3.8016 - val_loss: 4.7952\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 0s - loss: 3.3414 - val_loss: 4.7308\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 0s - loss: 3.4771 - val_loss: 4.8394\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 0s - loss: 3.8860 - val_loss: 4.8209\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 0s - loss: 3.6364 - val_loss: 4.9171\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 0s - loss: 3.7510 - val_loss: 4.9436\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 0s - loss: 3.7449 - val_loss: 4.5412\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 0s - loss: 3.2147 - val_loss: 5.0538\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 0s - loss: 3.6015 - val_loss: 4.7546\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 0s - loss: 3.6244 - val_loss: 4.7684\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 0s - loss: 3.2209 - val_loss: 4.3243\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 0s - loss: 3.2427 - val_loss: 4.3257\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 0s - loss: 3.4366 - val_loss: 4.4898\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 0s - loss: 3.3128 - val_loss: 4.3087\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 0s - loss: 3.2888 - val_loss: 4.5991\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 0s - loss: 3.5665 - val_loss: 4.3271\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 0s - loss: 3.4833 - val_loss: 4.7313\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 0s - loss: 3.7439 - val_loss: 4.7335\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 0s - loss: 3.3689 - val_loss: 4.6813\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 0s - loss: 3.6201 - val_loss: 4.4919\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 0s - loss: 3.2996 - val_loss: 4.3276\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 0s - loss: 3.2728 - val_loss: 4.7944\n",
      "Epoch 59/200\n",
      "8/8 [==============================] - 0s - loss: 4.0987 - val_loss: 4.3068\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 0s - loss: 3.2988 - val_loss: 4.3707\n",
      "Epoch 61/200\n",
      "8/8 [==============================] - 0s - loss: 3.2357 - val_loss: 4.3140\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 0s - loss: 3.6738 - val_loss: 4.2496\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 0s - loss: 3.2378 - val_loss: 4.4135\n",
      "Epoch 64/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s - loss: 3.6219 - val_loss: 4.7066\n",
      "Epoch 65/200\n",
      "8/8 [==============================] - 0s - loss: 3.4110 - val_loss: 4.6752\n",
      "Epoch 66/200\n",
      "8/8 [==============================] - 0s - loss: 3.8978 - val_loss: 4.4351\n",
      "Epoch 67/200\n",
      "8/8 [==============================] - 0s - loss: 3.2595 - val_loss: 4.7202\n",
      "Epoch 68/200\n",
      "8/8 [==============================] - 0s - loss: 3.5484 - val_loss: 4.3988\n",
      "Epoch 69/200\n",
      "8/8 [==============================] - 0s - loss: 3.5649 - val_loss: 4.3460\n",
      "Epoch 70/200\n",
      "8/8 [==============================] - 0s - loss: 3.3598 - val_loss: 4.5302\n",
      "Epoch 71/200\n",
      "8/8 [==============================] - 0s - loss: 3.5175 - val_loss: 4.5577\n",
      "Epoch 72/200\n",
      "8/8 [==============================] - 0s - loss: 3.3983 - val_loss: 4.4055\n",
      "Epoch 73/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 3.0277\n",
      " Reduced learning rate to 0.01\n",
      "8/8 [==============================] - 0s - loss: 3.4541 - val_loss: 4.5554\n",
      "Epoch 74/200\n",
      "8/8 [==============================] - 0s - loss: 2.7459 - val_loss: 3.6131\n",
      "Epoch 75/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.7070\n",
      " Reduced learning rate to 0.005\n",
      "8/8 [==============================] - 0s - loss: 2.6305 - val_loss: 3.6653\n",
      "Epoch 76/200\n",
      "8/8 [==============================] - 0s - loss: 2.7925 - val_loss: 3.5270\n",
      "Epoch 77/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.4405\n",
      " Reduced learning rate to 0.0025\n",
      "8/8 [==============================] - 0s - loss: 2.4666 - val_loss: 3.7040\n",
      "Epoch 78/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.3387\n",
      " Reduced learning rate to 0.00125\n",
      "8/8 [==============================] - 0s - loss: 2.7142 - val_loss: 3.5561\n",
      "Epoch 79/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.3519\n",
      " Reduced learning rate to 0.000625\n",
      "8/8 [==============================] - 0s - loss: 2.5954 - val_loss: 3.7437\n",
      "Epoch 80/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.8272\n",
      " Reduced learning rate to 0.0003125\n",
      "8/8 [==============================] - 0s - loss: 2.6720 - val_loss: 3.5322\n",
      "Epoch 81/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.5968\n",
      " Reduced learning rate to 0.00015625\n",
      "8/8 [==============================] - 0s - loss: 2.4569 - val_loss: 3.7173\n",
      "Epoch 82/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.5107\n",
      " Reduced learning rate to 7.8125e-05\n",
      "8/8 [==============================] - 0s - loss: 2.6281 - val_loss: 3.5925\n",
      "Epoch 83/200\n",
      "8/8 [==============================] - 0s - loss: 2.6600 - val_loss: 3.5063\n",
      "Epoch 84/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.1513\n",
      " Reduced learning rate to 3.90625e-05\n",
      "8/8 [==============================] - 0s - loss: 2.6821 - val_loss: 3.7557\n",
      "Epoch 85/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 3.2654\n",
      " Reduced learning rate to 1.95312e-05\n",
      "8/8 [==============================] - 0s - loss: 2.6552 - val_loss: 3.7969\n",
      "Epoch 86/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.4297\n",
      " Reduced learning rate to 9.76562e-06\n",
      "8/8 [==============================] - 0s - loss: 2.5043 - val_loss: 3.5683\n",
      "Epoch 87/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.0946\n",
      " Reduced learning rate to 4.88281e-06\n",
      "8/8 [==============================] - 0s - loss: 2.7103 - val_loss: 3.6465\n",
      "Epoch 88/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 3.3371\n",
      " Reduced learning rate to 2.44141e-06\n",
      "8/8 [==============================] - 0s - loss: 2.7726 - val_loss: 3.6663\n",
      "Epoch 89/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.6181\n",
      " Reduced learning rate to 1.2207e-06\n",
      "8/8 [==============================] - 0s - loss: 2.7960 - val_loss: 3.7040\n",
      "Epoch 90/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 2.2376\n",
      " Reduced learning rate to 6.10352e-07\n",
      "8/8 [==============================] - 0s - loss: 2.6083 - val_loss: 3.6927\n",
      "Epoch 1/200\n",
      "17/17 [==============================] - 0s - loss: 93.6711 - val_loss: 91.6271\n",
      "Epoch 2/200\n",
      "17/17 [==============================] - 0s - loss: 88.2396 - val_loss: 83.2713\n",
      "Epoch 3/200\n",
      "17/17 [==============================] - 0s - loss: 71.2039 - val_loss: 47.3047\n",
      "Epoch 4/200\n",
      "17/17 [==============================] - 0s - loss: 26.2503 - val_loss: 15.4524\n",
      "Epoch 5/200\n",
      "17/17 [==============================] - 0s - loss: 12.2085 - val_loss: 9.8266\n",
      "Epoch 6/200\n",
      "17/17 [==============================] - 0s - loss: 8.3411 - val_loss: 7.4571\n",
      "Epoch 7/200\n",
      "17/17 [==============================] - 0s - loss: 6.8099 - val_loss: 6.2643\n",
      "Epoch 8/200\n",
      "17/17 [==============================] - 0s - loss: 5.5697 - val_loss: 5.6627\n",
      "Epoch 9/200\n",
      "17/17 [==============================] - 0s - loss: 5.1903 - val_loss: 5.1839\n",
      "Epoch 10/200\n",
      "17/17 [==============================] - 0s - loss: 5.0627 - val_loss: 4.8801\n",
      "Epoch 11/200\n",
      "17/17 [==============================] - 0s - loss: 4.5771 - val_loss: 4.4455\n",
      "Epoch 12/200\n",
      "17/17 [==============================] - 0s - loss: 4.3413 - val_loss: 4.4037\n",
      "Epoch 13/200\n",
      "17/17 [==============================] - 0s - loss: 4.2887 - val_loss: 4.1606\n",
      "Epoch 14/200\n",
      "17/17 [==============================] - 0s - loss: 4.0876 - val_loss: 3.9643\n",
      "Epoch 15/200\n",
      "17/17 [==============================] - 0s - loss: 3.7100 - val_loss: 3.9081\n",
      "Epoch 16/200\n",
      "17/17 [==============================] - 0s - loss: 3.7233 - val_loss: 3.8067\n",
      "Epoch 17/200\n",
      "17/17 [==============================] - 0s - loss: 3.7624 - val_loss: 3.6544\n",
      "Epoch 18/200\n",
      "17/17 [==============================] - 0s - loss: 3.4740 - val_loss: 3.6191\n",
      "Epoch 19/200\n",
      "17/17 [==============================] - 0s - loss: 3.5901 - val_loss: 3.5946\n",
      "Epoch 20/200\n",
      "17/17 [==============================] - 0s - loss: 3.5629 - val_loss: 3.6673\n",
      "Epoch 21/200\n",
      "17/17 [==============================] - 0s - loss: 3.3933 - val_loss: 3.5246\n",
      "Epoch 22/200\n",
      "17/17 [==============================] - 0s - loss: 3.2711 - val_loss: 3.3918\n",
      "Epoch 23/200\n",
      "17/17 [==============================] - 0s - loss: 3.5147 - val_loss: 3.3936\n",
      "Epoch 24/200\n",
      "17/17 [==============================] - 0s - loss: 3.4776 - val_loss: 3.4180\n",
      "Epoch 25/200\n",
      "17/17 [==============================] - 0s - loss: 3.2726 - val_loss: 3.6765\n",
      "Epoch 26/200\n",
      "17/17 [==============================] - 0s - loss: 3.2931 - val_loss: 3.6918\n",
      "Epoch 27/200\n",
      "17/17 [==============================] - 0s - loss: 3.5653 - val_loss: 3.6366\n",
      "Epoch 28/200\n",
      "17/17 [==============================] - 0s - loss: 3.6623 - val_loss: 3.4367\n",
      "Epoch 29/200\n",
      "17/17 [==============================] - 0s - loss: 3.3454 - val_loss: 3.6943\n",
      "Epoch 30/200\n",
      "17/17 [==============================] - 0s - loss: 3.5319 - val_loss: 3.4978\n",
      "Epoch 31/200\n",
      "17/17 [==============================] - 0s - loss: 3.5588 - val_loss: 3.5474\n",
      "Epoch 32/200\n",
      "17/17 [==============================] - 0s - loss: 3.4039 - val_loss: 3.5715\n",
      "Epoch 33/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 4.0963\n",
      " Reduced learning rate to 0.01\n",
      "17/17 [==============================] - 0s - loss: 3.1951 - val_loss: 4.4667\n",
      "Epoch 34/200\n",
      "17/17 [==============================] - 0s - loss: 2.8367 - val_loss: 2.7287\n",
      "Epoch 35/200\n",
      "17/17 [==============================] - 0s - loss: 2.5967 - val_loss: 2.7215\n",
      "Epoch 36/200\n",
      "17/17 [==============================] - 0s - loss: 2.6503 - val_loss: 2.6349\n",
      "Epoch 37/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.3579\n",
      " Reduced learning rate to 0.005\n",
      "17/17 [==============================] - 0s - loss: 2.7755 - val_loss: 2.6707\n",
      "Epoch 38/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 3.7995\n",
      " Reduced learning rate to 0.0025\n",
      "17/17 [==============================] - 0s - loss: 2.4844 - val_loss: 2.6642\n",
      "Epoch 39/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.4586\n",
      " Reduced learning rate to 0.00125\n",
      "17/17 [==============================] - 0s - loss: 2.8468 - val_loss: 2.6558\n",
      "Epoch 40/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.2761\n",
      " Reduced learning rate to 0.000625\n",
      "17/17 [==============================] - 0s - loss: 2.4506 - val_loss: 2.7424\n",
      "Epoch 41/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.3900\n",
      " Reduced learning rate to 0.0003125\n",
      "17/17 [==============================] - 0s - loss: 2.7398 - val_loss: 2.7213\n",
      "Epoch 42/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/17 [>.............................] - ETA: 0s - loss: 2.8620\n",
      " Reduced learning rate to 0.00015625\n",
      "17/17 [==============================] - 0s - loss: 2.4432 - val_loss: 2.6372\n",
      "Epoch 43/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.1510\n",
      " Reduced learning rate to 7.8125e-05\n",
      "17/17 [==============================] - 0s - loss: 2.5960 - val_loss: 2.6646\n",
      "Epoch 44/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.5293\n",
      " Reduced learning rate to 3.90625e-05\n",
      "17/17 [==============================] - 0s - loss: 2.6769 - val_loss: 2.6987\n",
      "Epoch 45/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 3.0218\n",
      " Reduced learning rate to 1.95312e-05\n",
      "17/17 [==============================] - 0s - loss: 2.5607 - val_loss: 2.7041\n",
      "Epoch 46/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.0735\n",
      " Reduced learning rate to 9.76562e-06\n",
      "17/17 [==============================] - 0s - loss: 2.3869 - val_loss: 2.7000\n",
      "Epoch 47/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.4537\n",
      " Reduced learning rate to 4.88281e-06\n",
      "17/17 [==============================] - 0s - loss: 2.6354 - val_loss: 2.7558\n",
      "Epoch 48/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.3150\n",
      " Reduced learning rate to 2.44141e-06\n",
      "17/17 [==============================] - 0s - loss: 2.5557 - val_loss: 2.7169\n",
      "Epoch 49/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.1474\n",
      " Reduced learning rate to 1.2207e-06\n",
      "17/17 [==============================] - 0s - loss: 2.6522 - val_loss: 2.7339\n",
      "Epoch 50/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.3868\n",
      " Reduced learning rate to 6.10352e-07\n",
      "17/17 [==============================] - 0s - loss: 2.4762 - val_loss: 2.6837\n",
      "Epoch 1/200\n",
      "17/17 [==============================] - 0s - loss: 93.5674 - val_loss: 90.8906\n",
      "Epoch 2/200\n",
      "17/17 [==============================] - 0s - loss: 88.6423 - val_loss: 79.6424\n",
      "Epoch 3/200\n",
      "17/17 [==============================] - 0s - loss: 61.3390 - val_loss: 32.9316\n",
      "Epoch 4/200\n",
      "17/17 [==============================] - 0s - loss: 20.2750 - val_loss: 13.6412\n",
      "Epoch 5/200\n",
      "17/17 [==============================] - 0s - loss: 11.0601 - val_loss: 9.3620\n",
      "Epoch 6/200\n",
      "17/17 [==============================] - 0s - loss: 8.1616 - val_loss: 7.3466\n",
      "Epoch 7/200\n",
      "17/17 [==============================] - 0s - loss: 6.7161 - val_loss: 6.3251\n",
      "Epoch 8/200\n",
      "17/17 [==============================] - 0s - loss: 5.9356 - val_loss: 5.6317\n",
      "Epoch 9/200\n",
      "17/17 [==============================] - 0s - loss: 5.0601 - val_loss: 5.1912\n",
      "Epoch 10/200\n",
      "17/17 [==============================] - 0s - loss: 5.0167 - val_loss: 4.8716\n",
      "Epoch 11/200\n",
      "17/17 [==============================] - 0s - loss: 4.5086 - val_loss: 4.5015\n",
      "Epoch 12/200\n",
      "17/17 [==============================] - 0s - loss: 4.3056 - val_loss: 4.2496\n",
      "Epoch 13/200\n",
      "17/17 [==============================] - 0s - loss: 3.9696 - val_loss: 4.2689\n",
      "Epoch 14/200\n",
      "17/17 [==============================] - 0s - loss: 3.9702 - val_loss: 4.0911\n",
      "Epoch 15/200\n",
      "17/17 [==============================] - 0s - loss: 3.6261 - val_loss: 3.8261\n",
      "Epoch 16/200\n",
      "17/17 [==============================] - 0s - loss: 3.8450 - val_loss: 3.8882\n",
      "Epoch 17/200\n",
      "17/17 [==============================] - 0s - loss: 3.7545 - val_loss: 3.8842\n",
      "Epoch 18/200\n",
      "17/17 [==============================] - 0s - loss: 3.6920 - val_loss: 3.7824\n",
      "Epoch 19/200\n",
      "17/17 [==============================] - 0s - loss: 3.3772 - val_loss: 3.3912\n",
      "Epoch 20/200\n",
      "17/17 [==============================] - 0s - loss: 3.5277 - val_loss: 3.4697\n",
      "Epoch 21/200\n",
      "17/17 [==============================] - 0s - loss: 3.3644 - val_loss: 3.7176\n",
      "Epoch 22/200\n",
      "17/17 [==============================] - 0s - loss: 3.7666 - val_loss: 3.5735\n",
      "Epoch 23/200\n",
      "17/17 [==============================] - 0s - loss: 3.5382 - val_loss: 3.4654\n",
      "Epoch 24/200\n",
      "17/17 [==============================] - 0s - loss: 3.3619 - val_loss: 3.6291\n",
      "Epoch 25/200\n",
      "17/17 [==============================] - 0s - loss: 3.3870 - val_loss: 3.2890\n",
      "Epoch 26/200\n",
      "17/17 [==============================] - 0s - loss: 3.3187 - val_loss: 3.7215\n",
      "Epoch 27/200\n",
      "17/17 [==============================] - 0s - loss: 3.7694 - val_loss: 3.5605\n",
      "Epoch 28/200\n",
      "17/17 [==============================] - 0s - loss: 3.4580 - val_loss: 3.7145\n",
      "Epoch 29/200\n",
      "17/17 [==============================] - 0s - loss: 3.3360 - val_loss: 3.7762\n",
      "Epoch 30/200\n",
      "17/17 [==============================] - 0s - loss: 3.5189 - val_loss: 3.4149\n",
      "Epoch 31/200\n",
      "17/17 [==============================] - 0s - loss: 3.5060 - val_loss: 3.5369\n",
      "Epoch 32/200\n",
      "17/17 [==============================] - 0s - loss: 3.4499 - val_loss: 3.8972\n",
      "Epoch 33/200\n",
      "17/17 [==============================] - 0s - loss: 3.4966 - val_loss: 3.5257\n",
      "Epoch 34/200\n",
      "17/17 [==============================] - 0s - loss: 3.5384 - val_loss: 3.1274\n",
      "Epoch 35/200\n",
      "17/17 [==============================] - 0s - loss: 3.3147 - val_loss: 3.6027\n",
      "Epoch 36/200\n",
      "17/17 [==============================] - 0s - loss: 3.4408 - val_loss: 3.1254\n",
      "Epoch 37/200\n",
      "17/17 [==============================] - 0s - loss: 3.4890 - val_loss: 3.3806\n",
      "Epoch 38/200\n",
      "17/17 [==============================] - 0s - loss: 3.4489 - val_loss: 3.4042\n",
      "Epoch 39/200\n",
      "17/17 [==============================] - 0s - loss: 3.4077 - val_loss: 4.3369\n",
      "Epoch 40/200\n",
      "17/17 [==============================] - 0s - loss: 3.4974 - val_loss: 3.6980\n",
      "Epoch 41/200\n",
      "17/17 [==============================] - 0s - loss: 3.4587 - val_loss: 3.2739\n",
      "Epoch 42/200\n",
      "17/17 [==============================] - 0s - loss: 3.4725 - val_loss: 3.5518\n",
      "Epoch 43/200\n",
      "17/17 [==============================] - 0s - loss: 3.4351 - val_loss: 3.5409\n",
      "Epoch 44/200\n",
      "17/17 [==============================] - 0s - loss: 3.4724 - val_loss: 3.8385\n",
      "Epoch 45/200\n",
      "17/17 [==============================] - 0s - loss: 3.3795 - val_loss: 4.0767\n",
      "Epoch 46/200\n",
      "17/17 [==============================] - 0s - loss: 3.3580 - val_loss: 3.3075\n",
      "Epoch 47/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 3.0775\n",
      " Reduced learning rate to 0.01\n",
      "17/17 [==============================] - 0s - loss: 3.3195 - val_loss: 3.6266\n",
      "Epoch 48/200\n",
      "17/17 [==============================] - 0s - loss: 2.4353 - val_loss: 2.5956\n",
      "Epoch 49/200\n",
      "17/17 [==============================] - 0s - loss: 2.4389 - val_loss: 2.5499\n",
      "Epoch 50/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.0822\n",
      " Reduced learning rate to 0.005\n",
      "17/17 [==============================] - 0s - loss: 2.2293 - val_loss: 2.5787\n",
      "Epoch 51/200\n",
      "17/17 [==============================] - 0s - loss: 2.3311 - val_loss: 2.4504\n",
      "Epoch 52/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.3600\n",
      " Reduced learning rate to 0.0025\n",
      "17/17 [==============================] - 0s - loss: 2.4995 - val_loss: 2.4754\n",
      "Epoch 53/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.9043\n",
      " Reduced learning rate to 0.00125\n",
      "17/17 [==============================] - 0s - loss: 2.3806 - val_loss: 2.5353\n",
      "Epoch 54/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.1583\n",
      " Reduced learning rate to 0.000625\n",
      "17/17 [==============================] - 0s - loss: 2.2801 - val_loss: 2.4759\n",
      "Epoch 55/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.2512\n",
      " Reduced learning rate to 0.0003125\n",
      "17/17 [==============================] - 0s - loss: 2.4214 - val_loss: 2.5171\n",
      "Epoch 56/200\n",
      "17/17 [==============================] - 0s - loss: 2.2863 - val_loss: 2.4479\n",
      "Epoch 57/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.0952\n",
      " Reduced learning rate to 0.00015625\n",
      "17/17 [==============================] - 0s - loss: 2.2875 - val_loss: 2.5296\n",
      "Epoch 58/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.3853\n",
      " Reduced learning rate to 7.8125e-05\n",
      "17/17 [==============================] - 0s - loss: 2.3557 - val_loss: 2.5253\n",
      "Epoch 59/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.4403\n",
      " Reduced learning rate to 3.90625e-05\n",
      "17/17 [==============================] - 0s - loss: 2.2637 - val_loss: 2.4603\n",
      "Epoch 60/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.3461\n",
      " Reduced learning rate to 1.95312e-05\n",
      "17/17 [==============================] - 0s - loss: 2.2618 - val_loss: 2.4654\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/17 [>.............................] - ETA: 0s - loss: 2.0886\n",
      " Reduced learning rate to 9.76562e-06\n",
      "17/17 [==============================] - 0s - loss: 2.3972 - val_loss: 2.4938\n",
      "Epoch 62/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.3405\n",
      " Reduced learning rate to 4.88281e-06\n",
      "17/17 [==============================] - 0s - loss: 2.3866 - val_loss: 2.4815\n",
      "Epoch 63/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.2436\n",
      " Reduced learning rate to 2.44141e-06\n",
      "17/17 [==============================] - 0s - loss: 2.4223 - val_loss: 2.4509\n",
      "Epoch 64/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.3728\n",
      " Reduced learning rate to 1.2207e-06\n",
      "17/17 [==============================] - 0s - loss: 2.2697 - val_loss: 2.5247\n",
      "Epoch 65/200\n",
      "17/17 [==============================] - 0s - loss: 2.2562 - val_loss: 2.4376\n",
      "Epoch 66/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.2410\n",
      " Reduced learning rate to 6.10352e-07\n",
      "17/17 [==============================] - 0s - loss: 2.3523 - val_loss: 2.5393\n",
      "Epoch 1/200\n",
      "17/17 [==============================] - 0s - loss: 94.9478 - val_loss: 91.7728\n",
      "Epoch 2/200\n",
      "17/17 [==============================] - 0s - loss: 86.8447 - val_loss: 84.0748\n",
      "Epoch 3/200\n",
      "17/17 [==============================] - 0s - loss: 73.0640 - val_loss: 53.5820\n",
      "Epoch 4/200\n",
      "17/17 [==============================] - 0s - loss: 32.9959 - val_loss: 19.0371\n",
      "Epoch 5/200\n",
      "17/17 [==============================] - 0s - loss: 14.6139 - val_loss: 11.3872\n",
      "Epoch 6/200\n",
      "17/17 [==============================] - 0s - loss: 9.7608 - val_loss: 8.4515\n",
      "Epoch 7/200\n",
      "17/17 [==============================] - 0s - loss: 7.5272 - val_loss: 6.9037\n",
      "Epoch 8/200\n",
      "17/17 [==============================] - 0s - loss: 6.3432 - val_loss: 6.1753\n",
      "Epoch 9/200\n",
      "17/17 [==============================] - 0s - loss: 5.4285 - val_loss: 5.5971\n",
      "Epoch 10/200\n",
      "17/17 [==============================] - 0s - loss: 5.4689 - val_loss: 5.1945\n",
      "Epoch 11/200\n",
      "17/17 [==============================] - 0s - loss: 4.6295 - val_loss: 4.8477\n",
      "Epoch 12/200\n",
      "17/17 [==============================] - 0s - loss: 4.2131 - val_loss: 4.6495\n",
      "Epoch 13/200\n",
      "17/17 [==============================] - 0s - loss: 4.1718 - val_loss: 4.4850\n",
      "Epoch 14/200\n",
      "17/17 [==============================] - 0s - loss: 4.2006 - val_loss: 4.1855\n",
      "Epoch 15/200\n",
      "17/17 [==============================] - 0s - loss: 3.9035 - val_loss: 4.0064\n",
      "Epoch 16/200\n",
      "17/17 [==============================] - 0s - loss: 3.6074 - val_loss: 3.8860\n",
      "Epoch 17/200\n",
      "17/17 [==============================] - 0s - loss: 3.6399 - val_loss: 3.7149\n",
      "Epoch 18/200\n",
      "17/17 [==============================] - 0s - loss: 3.6452 - val_loss: 3.8811\n",
      "Epoch 19/200\n",
      "17/17 [==============================] - 0s - loss: 3.7174 - val_loss: 3.6508\n",
      "Epoch 20/200\n",
      "17/17 [==============================] - 0s - loss: 3.5620 - val_loss: 3.4617\n",
      "Epoch 21/200\n",
      "17/17 [==============================] - 0s - loss: 3.4487 - val_loss: 3.7637\n",
      "Epoch 22/200\n",
      "17/17 [==============================] - 0s - loss: 3.2944 - val_loss: 3.5311\n",
      "Epoch 23/200\n",
      "17/17 [==============================] - 0s - loss: 3.3852 - val_loss: 3.8401\n",
      "Epoch 24/200\n",
      "17/17 [==============================] - 0s - loss: 3.4303 - val_loss: 3.5474\n",
      "Epoch 25/200\n",
      "17/17 [==============================] - 0s - loss: 3.3760 - val_loss: 3.7088\n",
      "Epoch 26/200\n",
      "17/17 [==============================] - 0s - loss: 3.4574 - val_loss: 3.5803\n",
      "Epoch 27/200\n",
      "17/17 [==============================] - 0s - loss: 3.5141 - val_loss: 3.5932\n",
      "Epoch 28/200\n",
      "17/17 [==============================] - 0s - loss: 3.1714 - val_loss: 3.2360\n",
      "Epoch 29/200\n",
      "17/17 [==============================] - 0s - loss: 3.5560 - val_loss: 3.4267\n",
      "Epoch 30/200\n",
      "17/17 [==============================] - 0s - loss: 3.5770 - val_loss: 3.5673\n",
      "Epoch 31/200\n",
      "17/17 [==============================] - 0s - loss: 3.4288 - val_loss: 3.7640\n",
      "Epoch 32/200\n",
      "17/17 [==============================] - 0s - loss: 3.2674 - val_loss: 3.3495\n",
      "Epoch 33/200\n",
      "17/17 [==============================] - 0s - loss: 3.3645 - val_loss: 3.7508\n",
      "Epoch 34/200\n",
      "17/17 [==============================] - 0s - loss: 3.6077 - val_loss: 3.9692\n",
      "Epoch 35/200\n",
      "17/17 [==============================] - 0s - loss: 3.3456 - val_loss: 3.3011\n",
      "Epoch 36/200\n",
      "17/17 [==============================] - 0s - loss: 3.5119 - val_loss: 3.7460\n",
      "Epoch 37/200\n",
      "17/17 [==============================] - 0s - loss: 3.2951 - val_loss: 3.4953\n",
      "Epoch 38/200\n",
      "17/17 [==============================] - 0s - loss: 3.0694 - val_loss: 3.6967\n",
      "Epoch 39/200\n",
      "17/17 [==============================] - 0s - loss: 3.3114 - val_loss: 3.2186\n",
      "Epoch 40/200\n",
      "17/17 [==============================] - 0s - loss: 3.4325 - val_loss: 3.6888\n",
      "Epoch 41/200\n",
      "17/17 [==============================] - 0s - loss: 3.3004 - val_loss: 3.6016\n",
      "Epoch 42/200\n",
      "17/17 [==============================] - 0s - loss: 3.4462 - val_loss: 3.5473\n",
      "Epoch 43/200\n",
      "17/17 [==============================] - 0s - loss: 3.3234 - val_loss: 3.2425\n",
      "Epoch 44/200\n",
      "17/17 [==============================] - 0s - loss: 3.3803 - val_loss: 3.4292\n",
      "Epoch 45/200\n",
      "17/17 [==============================] - 0s - loss: 3.2641 - val_loss: 3.2928\n",
      "Epoch 46/200\n",
      "17/17 [==============================] - 0s - loss: 3.3802 - val_loss: 3.1025\n",
      "Epoch 47/200\n",
      "17/17 [==============================] - 0s - loss: 3.1794 - val_loss: 3.7861\n",
      "Epoch 48/200\n",
      "17/17 [==============================] - 0s - loss: 3.2394 - val_loss: 3.6444\n",
      "Epoch 49/200\n",
      "17/17 [==============================] - 0s - loss: 3.3765 - val_loss: 3.6052\n",
      "Epoch 50/200\n",
      "17/17 [==============================] - 0s - loss: 3.1589 - val_loss: 3.8799\n",
      "Epoch 51/200\n",
      "17/17 [==============================] - 0s - loss: 3.3845 - val_loss: 3.3942\n",
      "Epoch 52/200\n",
      "17/17 [==============================] - 0s - loss: 3.4626 - val_loss: 3.4755\n",
      "Epoch 53/200\n",
      "17/17 [==============================] - 0s - loss: 3.3406 - val_loss: 3.3282\n",
      "Epoch 54/200\n",
      "17/17 [==============================] - 0s - loss: 3.3079 - val_loss: 3.7966\n",
      "Epoch 55/200\n",
      "17/17 [==============================] - 0s - loss: 3.2895 - val_loss: 3.4144\n",
      "Epoch 56/200\n",
      "17/17 [==============================] - 0s - loss: 3.1389 - val_loss: 2.8346\n",
      "Epoch 57/200\n",
      "17/17 [==============================] - 0s - loss: 3.3378 - val_loss: 3.5451\n",
      "Epoch 58/200\n",
      "17/17 [==============================] - 0s - loss: 3.5292 - val_loss: 3.5773\n",
      "Epoch 59/200\n",
      "17/17 [==============================] - 0s - loss: 3.2358 - val_loss: 3.4100\n",
      "Epoch 60/200\n",
      "17/17 [==============================] - 0s - loss: 3.2922 - val_loss: 3.6181\n",
      "Epoch 61/200\n",
      "17/17 [==============================] - 0s - loss: 3.3270 - val_loss: 3.3157\n",
      "Epoch 62/200\n",
      "17/17 [==============================] - 0s - loss: 3.2058 - val_loss: 3.9087\n",
      "Epoch 63/200\n",
      "17/17 [==============================] - 0s - loss: 3.2786 - val_loss: 3.3621\n",
      "Epoch 64/200\n",
      "17/17 [==============================] - 0s - loss: 3.2431 - val_loss: 2.7213\n",
      "Epoch 65/200\n",
      "17/17 [==============================] - 0s - loss: 3.1485 - val_loss: 3.2494\n",
      "Epoch 66/200\n",
      "17/17 [==============================] - 0s - loss: 3.3247 - val_loss: 3.0373\n",
      "Epoch 67/200\n",
      "17/17 [==============================] - 0s - loss: 3.0946 - val_loss: 3.4813\n",
      "Epoch 68/200\n",
      "17/17 [==============================] - 0s - loss: 3.2272 - val_loss: 3.0407\n",
      "Epoch 69/200\n",
      "17/17 [==============================] - 0s - loss: 3.2991 - val_loss: 3.8746\n",
      "Epoch 70/200\n",
      "17/17 [==============================] - 0s - loss: 3.1601 - val_loss: 3.2673\n",
      "Epoch 71/200\n",
      "17/17 [==============================] - 0s - loss: 3.1506 - val_loss: 3.6371\n",
      "Epoch 72/200\n",
      "17/17 [==============================] - 0s - loss: 3.1513 - val_loss: 3.1723\n",
      "Epoch 73/200\n",
      "17/17 [==============================] - 0s - loss: 3.3360 - val_loss: 3.8564\n",
      "Epoch 74/200\n",
      "17/17 [==============================] - 0s - loss: 3.2869 - val_loss: 3.1596\n",
      "Epoch 75/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 3.0375\n",
      " Reduced learning rate to 0.01\n",
      "17/17 [==============================] - 0s - loss: 3.1975 - val_loss: 3.2907\n",
      "Epoch 76/200\n",
      "17/17 [==============================] - 0s - loss: 2.2382 - val_loss: 2.2815\n",
      "Epoch 77/200\n",
      "17/17 [==============================] - 0s - loss: 2.1727 - val_loss: 2.1956\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/17 [>.............................] - ETA: 0s - loss: 2.3196\n",
      " Reduced learning rate to 0.005\n",
      "17/17 [==============================] - 0s - loss: 2.1059 - val_loss: 2.2027\n",
      "Epoch 79/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.0110\n",
      " Reduced learning rate to 0.0025\n",
      "17/17 [==============================] - 0s - loss: 2.0571 - val_loss: 2.2416\n",
      "Epoch 80/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 1.8566\n",
      " Reduced learning rate to 0.00125\n",
      "17/17 [==============================] - 0s - loss: 2.0176 - val_loss: 2.2745\n",
      "Epoch 81/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.0455\n",
      " Reduced learning rate to 0.000625\n",
      "17/17 [==============================] - 0s - loss: 2.1006 - val_loss: 2.2279\n",
      "Epoch 82/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.2306\n",
      " Reduced learning rate to 0.0003125\n",
      "17/17 [==============================] - 0s - loss: 2.2990 - val_loss: 2.3491\n",
      "Epoch 83/200\n",
      "17/17 [==============================] - 0s - loss: 1.9852 - val_loss: 2.1768\n",
      "Epoch 84/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 1.9958\n",
      " Reduced learning rate to 0.00015625\n",
      "17/17 [==============================] - 0s - loss: 2.0612 - val_loss: 2.2953\n",
      "Epoch 85/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.2288\n",
      " Reduced learning rate to 7.8125e-05\n",
      "17/17 [==============================] - 0s - loss: 2.1734 - val_loss: 2.1837\n",
      "Epoch 86/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 1.8403\n",
      " Reduced learning rate to 3.90625e-05\n",
      "17/17 [==============================] - 0s - loss: 2.0440 - val_loss: 2.2229\n",
      "Epoch 87/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.0942\n",
      " Reduced learning rate to 1.95312e-05\n",
      "17/17 [==============================] - 0s - loss: 2.0579 - val_loss: 2.2135\n",
      "Epoch 88/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 1.8581\n",
      " Reduced learning rate to 9.76562e-06\n",
      "17/17 [==============================] - 0s - loss: 2.1613 - val_loss: 2.2456\n",
      "Epoch 89/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.2793\n",
      " Reduced learning rate to 4.88281e-06\n",
      "17/17 [==============================] - 0s - loss: 2.0599 - val_loss: 2.3325\n",
      "Epoch 90/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 1.9863\n",
      " Reduced learning rate to 2.44141e-06\n",
      "17/17 [==============================] - 0s - loss: 2.1003 - val_loss: 2.2140\n",
      "Epoch 91/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 1.8972\n",
      " Reduced learning rate to 1.2207e-06\n",
      "17/17 [==============================] - 0s - loss: 2.0266 - val_loss: 2.1997\n",
      "Epoch 92/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 1.8910\n",
      " Reduced learning rate to 6.10352e-07\n",
      "17/17 [==============================] - 0s - loss: 2.0106 - val_loss: 2.2133\n",
      "Epoch 1/200\n",
      "17/17 [==============================] - 0s - loss: 93.1984 - val_loss: 92.4986\n",
      "Epoch 2/200\n",
      "17/17 [==============================] - 0s - loss: 92.2791 - val_loss: 87.4961\n",
      "Epoch 3/200\n",
      "17/17 [==============================] - 0s - loss: 81.6314 - val_loss: 70.8652\n",
      "Epoch 4/200\n",
      "17/17 [==============================] - 0s - loss: 49.2784 - val_loss: 27.2606\n",
      "Epoch 5/200\n",
      "17/17 [==============================] - 0s - loss: 18.5157 - val_loss: 13.3472\n",
      "Epoch 6/200\n",
      "17/17 [==============================] - 0s - loss: 10.6914 - val_loss: 8.8086\n",
      "Epoch 7/200\n",
      "17/17 [==============================] - 0s - loss: 7.8763 - val_loss: 6.9263\n",
      "Epoch 8/200\n",
      "17/17 [==============================] - 0s - loss: 6.1208 - val_loss: 5.9543\n",
      "Epoch 9/200\n",
      "17/17 [==============================] - 0s - loss: 5.3098 - val_loss: 5.4664\n",
      "Epoch 10/200\n",
      "17/17 [==============================] - 0s - loss: 5.1752 - val_loss: 5.0343\n",
      "Epoch 11/200\n",
      "17/17 [==============================] - 0s - loss: 4.6538 - val_loss: 4.7323\n",
      "Epoch 12/200\n",
      "17/17 [==============================] - 0s - loss: 4.6204 - val_loss: 4.5700\n",
      "Epoch 13/200\n",
      "17/17 [==============================] - 0s - loss: 4.1344 - val_loss: 4.3165\n",
      "Epoch 14/200\n",
      "17/17 [==============================] - 0s - loss: 3.7707 - val_loss: 4.0853\n",
      "Epoch 15/200\n",
      "17/17 [==============================] - 0s - loss: 3.7656 - val_loss: 3.8815\n",
      "Epoch 16/200\n",
      "17/17 [==============================] - 0s - loss: 3.8317 - val_loss: 3.8110\n",
      "Epoch 17/200\n",
      "17/17 [==============================] - 0s - loss: 3.6589 - val_loss: 3.8196\n",
      "Epoch 18/200\n",
      "17/17 [==============================] - 0s - loss: 3.4225 - val_loss: 3.7174\n",
      "Epoch 19/200\n",
      "17/17 [==============================] - 0s - loss: 3.7582 - val_loss: 3.4582\n",
      "Epoch 20/200\n",
      "17/17 [==============================] - 0s - loss: 3.5239 - val_loss: 4.0463\n",
      "Epoch 21/200\n",
      "17/17 [==============================] - 0s - loss: 3.4904 - val_loss: 3.4994\n",
      "Epoch 22/200\n",
      "17/17 [==============================] - 0s - loss: 3.5069 - val_loss: 3.5078\n",
      "Epoch 23/200\n",
      "17/17 [==============================] - 0s - loss: 3.3053 - val_loss: 3.5618\n",
      "Epoch 24/200\n",
      "17/17 [==============================] - 0s - loss: 3.4074 - val_loss: 3.4256\n",
      "Epoch 25/200\n",
      "17/17 [==============================] - 0s - loss: 3.2956 - val_loss: 3.2888\n",
      "Epoch 26/200\n",
      "17/17 [==============================] - 0s - loss: 3.2998 - val_loss: 3.7028\n",
      "Epoch 27/200\n",
      "17/17 [==============================] - 0s - loss: 3.7716 - val_loss: 3.5603\n",
      "Epoch 28/200\n",
      "17/17 [==============================] - 0s - loss: 3.3889 - val_loss: 3.6230\n",
      "Epoch 29/200\n",
      "17/17 [==============================] - 0s - loss: 3.6991 - val_loss: 3.8709\n",
      "Epoch 30/200\n",
      "17/17 [==============================] - 0s - loss: 3.3922 - val_loss: 3.6246\n",
      "Epoch 31/200\n",
      "17/17 [==============================] - 0s - loss: 3.3691 - val_loss: 3.4960\n",
      "Epoch 32/200\n",
      "17/17 [==============================] - 0s - loss: 3.3155 - val_loss: 3.6453\n",
      "Epoch 33/200\n",
      "17/17 [==============================] - 0s - loss: 3.4636 - val_loss: 3.8631\n",
      "Epoch 34/200\n",
      "17/17 [==============================] - 0s - loss: 3.2827 - val_loss: 3.6205\n",
      "Epoch 35/200\n",
      "17/17 [==============================] - 0s - loss: 3.5181 - val_loss: 3.3611\n",
      "Epoch 36/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 3.3862\n",
      " Reduced learning rate to 0.01\n",
      "17/17 [==============================] - 0s - loss: 3.5096 - val_loss: 3.4399\n",
      "Epoch 37/200\n",
      "17/17 [==============================] - 0s - loss: 2.7621 - val_loss: 2.7624\n",
      "Epoch 38/200\n",
      "17/17 [==============================] - 0s - loss: 2.5658 - val_loss: 2.6528\n",
      "Epoch 39/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 3.0754\n",
      " Reduced learning rate to 0.005\n",
      "17/17 [==============================] - 0s - loss: 2.6066 - val_loss: 2.6985\n",
      "Epoch 40/200\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 2.6396\n",
      " Reduced learning rate to 0.0025\n",
      "17/17 [==============================] - 0s - loss: 2.6228 - val_loss: 2.6939\n",
      "Epoch 41/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.4237\n",
      " Reduced learning rate to 0.00125\n",
      "17/17 [==============================] - 0s - loss: 2.4219 - val_loss: 2.6679\n",
      "Epoch 42/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.4504\n",
      " Reduced learning rate to 0.000625\n",
      "17/17 [==============================] - 0s - loss: 2.3949 - val_loss: 2.6575\n",
      "Epoch 43/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.4034\n",
      " Reduced learning rate to 0.0003125\n",
      "17/17 [==============================] - 0s - loss: 2.4200 - val_loss: 2.6709\n",
      "Epoch 44/200\n",
      "17/17 [==============================] - 0s - loss: 2.4664 - val_loss: 2.6422\n",
      "Epoch 45/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 3.1348\n",
      " Reduced learning rate to 0.00015625\n",
      "17/17 [==============================] - 0s - loss: 2.5944 - val_loss: 2.6464\n",
      "Epoch 46/200\n",
      "17/17 [==============================] - 0s - loss: 2.4424 - val_loss: 2.6161\n",
      "Epoch 47/200\n",
      "17/17 [==============================] - 0s - loss: 2.5193 - val_loss: 2.6129\n",
      "Epoch 48/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.9748\n",
      " Reduced learning rate to 7.8125e-05\n",
      "17/17 [==============================] - 0s - loss: 2.5075 - val_loss: 2.6622\n",
      "Epoch 49/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.3326\n",
      " Reduced learning rate to 3.90625e-05\n",
      "17/17 [==============================] - 0s - loss: 2.4797 - val_loss: 2.6696\n",
      "Epoch 50/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s - loss: 2.6294 - val_loss: 2.6028\n",
      "Epoch 51/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.0614\n",
      " Reduced learning rate to 1.95312e-05\n",
      "17/17 [==============================] - 0s - loss: 2.7702 - val_loss: 2.6713\n",
      "Epoch 52/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.7023\n",
      " Reduced learning rate to 9.76562e-06\n",
      "17/17 [==============================] - 0s - loss: 2.4749 - val_loss: 2.6907\n",
      "Epoch 53/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.1620\n",
      " Reduced learning rate to 4.88281e-06\n",
      "17/17 [==============================] - 0s - loss: 2.4449 - val_loss: 2.8018\n",
      "Epoch 54/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 3.0437\n",
      " Reduced learning rate to 2.44141e-06\n",
      "17/17 [==============================] - 0s - loss: 2.3773 - val_loss: 2.6076\n",
      "Epoch 55/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.7194\n",
      " Reduced learning rate to 1.2207e-06\n",
      "17/17 [==============================] - 0s - loss: 2.3761 - val_loss: 2.7843\n",
      "Epoch 56/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.2333\n",
      " Reduced learning rate to 6.10352e-07\n",
      "17/17 [==============================] - 0s - loss: 2.3854 - val_loss: 2.6676\n",
      "Epoch 1/200\n",
      "17/17 [==============================] - 0s - loss: 93.4343 - val_loss: 92.0234\n",
      "Epoch 2/200\n",
      "17/17 [==============================] - 0s - loss: 90.2791 - val_loss: 84.9394\n",
      "Epoch 3/200\n",
      "17/17 [==============================] - 0s - loss: 74.7621 - val_loss: 53.6569\n",
      "Epoch 4/200\n",
      "17/17 [==============================] - 0s - loss: 32.3826 - val_loss: 18.5067\n",
      "Epoch 5/200\n",
      "17/17 [==============================] - 0s - loss: 14.2484 - val_loss: 11.4111\n",
      "Epoch 6/200\n",
      "17/17 [==============================] - 0s - loss: 9.3209 - val_loss: 8.5895\n",
      "Epoch 7/200\n",
      "17/17 [==============================] - 0s - loss: 7.8123 - val_loss: 7.1377\n",
      "Epoch 8/200\n",
      "17/17 [==============================] - 0s - loss: 6.3902 - val_loss: 6.2223\n",
      "Epoch 9/200\n",
      "17/17 [==============================] - 0s - loss: 5.8616 - val_loss: 5.6713\n",
      "Epoch 10/200\n",
      "17/17 [==============================] - 0s - loss: 4.9734 - val_loss: 5.2493\n",
      "Epoch 11/200\n",
      "17/17 [==============================] - 0s - loss: 5.0799 - val_loss: 4.8730\n",
      "Epoch 12/200\n",
      "17/17 [==============================] - 0s - loss: 4.3654 - val_loss: 4.5542\n",
      "Epoch 13/200\n",
      "17/17 [==============================] - 0s - loss: 4.6546 - val_loss: 4.4260\n",
      "Epoch 14/200\n",
      "17/17 [==============================] - 0s - loss: 4.3164 - val_loss: 4.1624\n",
      "Epoch 15/200\n",
      "17/17 [==============================] - 0s - loss: 3.7545 - val_loss: 4.0208\n",
      "Epoch 16/200\n",
      "17/17 [==============================] - 0s - loss: 3.6734 - val_loss: 3.8954\n",
      "Epoch 17/200\n",
      "17/17 [==============================] - 0s - loss: 3.7315 - val_loss: 3.7766\n",
      "Epoch 18/200\n",
      "17/17 [==============================] - 0s - loss: 3.6325 - val_loss: 3.7378\n",
      "Epoch 19/200\n",
      "17/17 [==============================] - 0s - loss: 3.4603 - val_loss: 3.7843\n",
      "Epoch 20/200\n",
      "17/17 [==============================] - 0s - loss: 3.7623 - val_loss: 3.6030\n",
      "Epoch 21/200\n",
      "17/17 [==============================] - 0s - loss: 3.5749 - val_loss: 3.3328\n",
      "Epoch 22/200\n",
      "17/17 [==============================] - 0s - loss: 3.5603 - val_loss: 3.4732\n",
      "Epoch 23/200\n",
      "17/17 [==============================] - 0s - loss: 3.4794 - val_loss: 3.7681\n",
      "Epoch 24/200\n",
      "17/17 [==============================] - 0s - loss: 3.2200 - val_loss: 3.3772\n",
      "Epoch 25/200\n",
      "17/17 [==============================] - 0s - loss: 3.2285 - val_loss: 3.7392\n",
      "Epoch 26/200\n",
      "17/17 [==============================] - 0s - loss: 3.5678 - val_loss: 3.3405\n",
      "Epoch 27/200\n",
      "17/17 [==============================] - 0s - loss: 3.2418 - val_loss: 3.7836\n",
      "Epoch 28/200\n",
      "17/17 [==============================] - 0s - loss: 3.5471 - val_loss: 3.5918\n",
      "Epoch 29/200\n",
      "17/17 [==============================] - 0s - loss: 3.7280 - val_loss: 3.4263\n",
      "Epoch 30/200\n",
      "17/17 [==============================] - 0s - loss: 3.5833 - val_loss: 3.2942\n",
      "Epoch 31/200\n",
      "17/17 [==============================] - 0s - loss: 3.3613 - val_loss: 4.3104\n",
      "Epoch 32/200\n",
      "17/17 [==============================] - 0s - loss: 3.3691 - val_loss: 3.8105\n",
      "Epoch 33/200\n",
      "17/17 [==============================] - 0s - loss: 3.5317 - val_loss: 3.6599\n",
      "Epoch 34/200\n",
      "17/17 [==============================] - 0s - loss: 3.4500 - val_loss: 3.6851\n",
      "Epoch 35/200\n",
      "17/17 [==============================] - 0s - loss: 3.5519 - val_loss: 3.4964\n",
      "Epoch 36/200\n",
      "17/17 [==============================] - 0s - loss: 3.3590 - val_loss: 3.6337\n",
      "Epoch 37/200\n",
      "17/17 [==============================] - 0s - loss: 3.2219 - val_loss: 3.7026\n",
      "Epoch 38/200\n",
      "17/17 [==============================] - 0s - loss: 3.2303 - val_loss: 3.4292\n",
      "Epoch 39/200\n",
      "17/17 [==============================] - 0s - loss: 3.5282 - val_loss: 3.8915\n",
      "Epoch 40/200\n",
      "17/17 [==============================] - 0s - loss: 3.4251 - val_loss: 3.2980\n",
      "Epoch 41/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 3.2788\n",
      " Reduced learning rate to 0.01\n",
      "17/17 [==============================] - 0s - loss: 3.3804 - val_loss: 3.6684\n",
      "Epoch 42/200\n",
      "17/17 [==============================] - 0s - loss: 2.5721 - val_loss: 2.6571\n",
      "Epoch 43/200\n",
      "17/17 [==============================] - 0s - loss: 2.3480 - val_loss: 2.6414\n",
      "Epoch 44/200\n",
      "17/17 [==============================] - 0s - loss: 2.4780 - val_loss: 2.5939\n",
      "Epoch 45/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.2729\n",
      " Reduced learning rate to 0.005\n",
      "17/17 [==============================] - 0s - loss: 2.3525 - val_loss: 2.6194\n",
      "Epoch 46/200\n",
      "17/17 [==============================] - 0s - loss: 2.4877 - val_loss: 2.5468\n",
      "Epoch 47/200\n",
      "17/17 [==============================] - 0s - loss: 2.3740 - val_loss: 2.4373\n",
      "Epoch 48/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.4407\n",
      " Reduced learning rate to 0.0025\n",
      "17/17 [==============================] - 0s - loss: 2.3212 - val_loss: 2.5313\n",
      "Epoch 49/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.1895\n",
      " Reduced learning rate to 0.00125\n",
      "17/17 [==============================] - 0s - loss: 2.2647 - val_loss: 2.5847\n",
      "Epoch 50/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.2830\n",
      " Reduced learning rate to 0.000625\n",
      "17/17 [==============================] - 0s - loss: 2.4966 - val_loss: 2.5967\n",
      "Epoch 51/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.0331\n",
      " Reduced learning rate to 0.0003125\n",
      "17/17 [==============================] - 0s - loss: 2.2429 - val_loss: 2.5308\n",
      "Epoch 52/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.0643\n",
      " Reduced learning rate to 0.00015625\n",
      "17/17 [==============================] - 0s - loss: 2.4495 - val_loss: 2.5710\n",
      "Epoch 53/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.4141\n",
      " Reduced learning rate to 7.8125e-05\n",
      "17/17 [==============================] - 0s - loss: 2.4162 - val_loss: 2.5965\n",
      "Epoch 54/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.1572\n",
      " Reduced learning rate to 3.90625e-05\n",
      "17/17 [==============================] - 0s - loss: 2.4977 - val_loss: 2.5710\n",
      "Epoch 55/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.1680\n",
      " Reduced learning rate to 1.95312e-05\n",
      "17/17 [==============================] - 0s - loss: 2.5121 - val_loss: 2.5842\n",
      "Epoch 56/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.4094\n",
      " Reduced learning rate to 9.76562e-06\n",
      "17/17 [==============================] - 0s - loss: 2.4083 - val_loss: 2.6179\n",
      "Epoch 57/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.5725\n",
      " Reduced learning rate to 4.88281e-06\n",
      "17/17 [==============================] - 0s - loss: 2.3359 - val_loss: 2.4940\n",
      "Epoch 58/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.1567\n",
      " Reduced learning rate to 2.44141e-06\n",
      "17/17 [==============================] - 0s - loss: 2.5763 - val_loss: 2.5376\n",
      "Epoch 59/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 3.6630\n",
      " Reduced learning rate to 1.2207e-06\n",
      "17/17 [==============================] - 0s - loss: 2.5911 - val_loss: 2.5018\n",
      "Epoch 60/200\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 2.3318\n",
      " Reduced learning rate to 6.10352e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s - loss: 2.3059 - val_loss: 2.5669\n",
      "Epoch 1/200\n",
      "87/87 [==============================] - 1s - loss: 59.0405 - val_loss: 11.3215\n",
      "Epoch 2/200\n",
      "87/87 [==============================] - 0s - loss: 6.7215 - val_loss: 4.9977\n",
      "Epoch 3/200\n",
      "87/87 [==============================] - 0s - loss: 4.0861 - val_loss: 3.9308\n",
      "Epoch 4/200\n",
      "87/87 [==============================] - 0s - loss: 3.5020 - val_loss: 3.6123\n",
      "Epoch 5/200\n",
      "87/87 [==============================] - 0s - loss: 3.3203 - val_loss: 3.2578\n",
      "Epoch 6/200\n",
      "87/87 [==============================] - 0s - loss: 3.3861 - val_loss: 3.3018\n",
      "Epoch 7/200\n",
      "87/87 [==============================] - 0s - loss: 3.3180 - val_loss: 3.4500\n",
      "Epoch 8/200\n",
      "87/87 [==============================] - 0s - loss: 3.2621 - val_loss: 3.2078\n",
      "Epoch 9/200\n",
      "87/87 [==============================] - 0s - loss: 3.3053 - val_loss: 3.4014\n",
      "Epoch 10/200\n",
      "87/87 [==============================] - 0s - loss: 3.3680 - val_loss: 3.3489\n",
      "Epoch 11/200\n",
      "87/87 [==============================] - 0s - loss: 3.2514 - val_loss: 3.8154\n",
      "Epoch 12/200\n",
      "87/87 [==============================] - 0s - loss: 3.2779 - val_loss: 3.1032\n",
      "Epoch 13/200\n",
      "87/87 [==============================] - 0s - loss: 3.2832 - val_loss: 2.9077\n",
      "Epoch 14/200\n",
      "87/87 [==============================] - 0s - loss: 3.2366 - val_loss: 2.9907\n",
      "Epoch 15/200\n",
      "87/87 [==============================] - 0s - loss: 3.1467 - val_loss: 3.6001\n",
      "Epoch 16/200\n",
      "87/87 [==============================] - 0s - loss: 3.1357 - val_loss: 2.8191\n",
      "Epoch 17/200\n",
      "87/87 [==============================] - 0s - loss: 3.1219 - val_loss: 3.1325\n",
      "Epoch 18/200\n",
      "87/87 [==============================] - 0s - loss: 3.1478 - val_loss: 3.2297\n",
      "Epoch 19/200\n",
      "87/87 [==============================] - 0s - loss: 3.1304 - val_loss: 3.1020\n",
      "Epoch 20/200\n",
      "87/87 [==============================] - 0s - loss: 3.0700 - val_loss: 3.4064\n",
      "Epoch 21/200\n",
      "87/87 [==============================] - 0s - loss: 3.0266 - val_loss: 2.9946\n",
      "Epoch 22/200\n",
      "87/87 [==============================] - 0s - loss: 3.0059 - val_loss: 4.1112\n",
      "Epoch 23/200\n",
      "87/87 [==============================] - 0s - loss: 3.0240 - val_loss: 3.0506\n",
      "Epoch 24/200\n",
      "87/87 [==============================] - 0s - loss: 2.9967 - val_loss: 2.8352\n",
      "Epoch 25/200\n",
      "87/87 [==============================] - 0s - loss: 3.0132 - val_loss: 3.3205\n",
      "Epoch 26/200\n",
      "87/87 [==============================] - 0s - loss: 2.9856 - val_loss: 3.4969\n",
      "Epoch 27/200\n",
      "78/87 [=========================>....] - ETA: 0s - loss: 2.9181- ETA: 0s - loss: 2.90\n",
      " Reduced learning rate to 0.01\n",
      "87/87 [==============================] - 0s - loss: 2.9268 - val_loss: 3.0979\n",
      "Epoch 28/200\n",
      "87/87 [==============================] - 0s - loss: 1.9768 - val_loss: 2.0565\n",
      "Epoch 29/200\n",
      "87/87 [==============================] - 0s - loss: 1.9351 - val_loss: 2.0283\n",
      "Epoch 30/200\n",
      "81/87 [==========================>...] - ETA: 0s - loss: 1.8927\n",
      " Reduced learning rate to 0.005\n",
      "87/87 [==============================] - 0s - loss: 1.8885 - val_loss: 2.0543\n",
      "Epoch 31/200\n",
      "87/87 [==============================] - 0s - loss: 1.9488 - val_loss: 2.0132\n",
      "Epoch 32/200\n",
      "87/87 [==============================] - 0s - loss: 1.8743 - val_loss: 1.9895\n",
      "Epoch 33/200\n",
      "81/87 [==========================>...] - ETA: 0s - loss: 1.9281\n",
      " Reduced learning rate to 0.0025\n",
      "87/87 [==============================] - 0s - loss: 1.9165 - val_loss: 2.0051\n",
      "Epoch 34/200\n",
      "87/87 [==============================] - 0s - loss: 1.9168 - val_loss: 1.9866\n",
      "Epoch 35/200\n",
      "87/87 [==============================] - 0s - loss: 1.8912 - val_loss: 1.9650\n",
      "Epoch 36/200\n",
      "80/87 [==========================>...] - ETA: 0s - loss: 1.8997\n",
      " Reduced learning rate to 0.00125\n",
      "87/87 [==============================] - 0s - loss: 1.8889 - val_loss: 2.0002\n",
      "Epoch 37/200\n",
      "76/87 [=========================>....] - ETA: 0s - loss: 1.9605\n",
      " Reduced learning rate to 0.000625\n",
      "87/87 [==============================] - 0s - loss: 1.9645 - val_loss: 2.0018\n",
      "Epoch 38/200\n",
      "81/87 [==========================>...] - ETA: 0s - loss: 1.8877\n",
      " Reduced learning rate to 0.0003125\n",
      "87/87 [==============================] - 0s - loss: 1.8993 - val_loss: 1.9900\n",
      "Epoch 39/200\n",
      "81/87 [==========================>...] - ETA: 0s - loss: 1.9432\n",
      " Reduced learning rate to 0.00015625\n",
      "87/87 [==============================] - 0s - loss: 1.9348 - val_loss: 2.0190\n",
      "Epoch 40/200\n",
      "79/87 [==========================>...] - ETA: 0s - loss: 1.8735\n",
      " Reduced learning rate to 7.8125e-05\n",
      "87/87 [==============================] - 0s - loss: 1.8771 - val_loss: 2.0141\n",
      "Epoch 41/200\n",
      "87/87 [==============================] - 0s - loss: 1.8843 - val_loss: 1.9644\n",
      "Epoch 42/200\n",
      "87/87 [==============================] - 0s - loss: 1.8646 - val_loss: 1.9494\n",
      "Epoch 43/200\n",
      "81/87 [==========================>...] - ETA: 0s - loss: 1.9090\n",
      " Reduced learning rate to 3.90625e-05\n",
      "87/87 [==============================] - 0s - loss: 1.9109 - val_loss: 1.9761\n",
      "Epoch 44/200\n",
      "79/87 [==========================>...] - ETA: 0s - loss: 1.8452\n",
      " Reduced learning rate to 1.95312e-05\n",
      "87/87 [==============================] - 0s - loss: 1.8598 - val_loss: 1.9912\n",
      "Epoch 45/200\n",
      "81/87 [==========================>...] - ETA: 0s - loss: 1.8856\n",
      " Reduced learning rate to 9.76562e-06\n",
      "87/87 [==============================] - 0s - loss: 1.8876 - val_loss: 1.9537\n",
      "Epoch 46/200\n",
      "81/87 [==========================>...] - ETA: 0s - loss: 1.8909\n",
      " Reduced learning rate to 4.88281e-06\n",
      "87/87 [==============================] - 0s - loss: 1.8902 - val_loss: 1.9612\n",
      "Epoch 47/200\n",
      "79/87 [==========================>...] - ETA: 0s - loss: 1.8749\n",
      " Reduced learning rate to 2.44141e-06\n",
      "87/87 [==============================] - 0s - loss: 1.8741 - val_loss: 1.9923\n",
      "Epoch 48/200\n",
      "79/87 [==========================>...] - ETA: 0s - loss: 1.9201\n",
      " Reduced learning rate to 1.2207e-06\n",
      "87/87 [==============================] - 0s - loss: 1.9188 - val_loss: 1.9514\n",
      "Epoch 49/200\n",
      "79/87 [==========================>...] - ETA: 0s - loss: 1.8810\n",
      " Reduced learning rate to 6.10352e-07\n",
      "87/87 [==============================] - 0s - loss: 1.8750 - val_loss: 2.0111\n",
      "Epoch 1/200\n",
      "87/87 [==============================] - 0s - loss: 55.8973 - val_loss: 10.3948\n",
      "Epoch 2/200\n",
      "87/87 [==============================] - 0s - loss: 6.1598 - val_loss: 4.5426\n",
      "Epoch 3/200\n",
      "87/87 [==============================] - 0s - loss: 3.8839 - val_loss: 3.9378\n",
      "Epoch 4/200\n",
      "87/87 [==============================] - 0s - loss: 3.3741 - val_loss: 3.6385\n",
      "Epoch 5/200\n",
      "87/87 [==============================] - 0s - loss: 3.2720 - val_loss: 3.3713\n",
      "Epoch 6/200\n",
      "87/87 [==============================] - 0s - loss: 3.2491 - val_loss: 3.7674\n",
      "Epoch 7/200\n",
      "87/87 [==============================] - 0s - loss: 3.2755 - val_loss: 3.3552\n",
      "Epoch 8/200\n",
      "87/87 [==============================] - 0s - loss: 3.3077 - val_loss: 3.1408\n",
      "Epoch 9/200\n",
      "87/87 [==============================] - 0s - loss: 3.2838 - val_loss: 3.4451\n",
      "Epoch 10/200\n",
      "87/87 [==============================] - 0s - loss: 3.2220 - val_loss: 3.6772\n",
      "Epoch 11/200\n",
      "87/87 [==============================] - 0s - loss: 3.2900 - val_loss: 3.3162\n",
      "Epoch 12/200\n",
      "87/87 [==============================] - 0s - loss: 3.2029 - val_loss: 3.5064\n",
      "Epoch 13/200\n",
      "87/87 [==============================] - 0s - loss: 3.2836 - val_loss: 3.9943\n",
      "Epoch 14/200\n",
      "87/87 [==============================] - 0s - loss: 3.2732 - val_loss: 3.4121\n",
      "Epoch 15/200\n",
      "87/87 [==============================] - 0s - loss: 3.1722 - val_loss: 3.8880\n",
      "Epoch 16/200\n",
      "87/87 [==============================] - 0s - loss: 3.1839 - val_loss: 3.1690\n",
      "Epoch 17/200\n",
      "87/87 [==============================] - 0s - loss: 3.1213 - val_loss: 3.0259\n",
      "Epoch 18/200\n",
      "87/87 [==============================] - 0s - loss: 3.1423 - val_loss: 3.2651\n",
      "Epoch 19/200\n",
      "87/87 [==============================] - 0s - loss: 3.1337 - val_loss: 3.7217\n",
      "Epoch 20/200\n",
      "87/87 [==============================] - 0s - loss: 3.0612 - val_loss: 3.0417\n",
      "Epoch 21/200\n",
      "87/87 [==============================] - 0s - loss: 3.0327 - val_loss: 3.2263\n",
      "Epoch 22/200\n",
      "87/87 [==============================] - 0s - loss: 2.9963 - val_loss: 2.8238\n",
      "Epoch 23/200\n",
      "87/87 [==============================] - 0s - loss: 3.0868 - val_loss: 3.0520\n",
      "Epoch 24/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s - loss: 2.9648 - val_loss: 3.4061\n",
      "Epoch 25/200\n",
      "87/87 [==============================] - 0s - loss: 3.0115 - val_loss: 3.4755\n",
      "Epoch 26/200\n",
      "87/87 [==============================] - 0s - loss: 2.9356 - val_loss: 3.4726\n",
      "Epoch 27/200\n",
      "87/87 [==============================] - 0s - loss: 2.9646 - val_loss: 2.7900\n",
      "Epoch 28/200\n",
      "87/87 [==============================] - 0s - loss: 2.9288 - val_loss: 3.0258\n",
      "Epoch 29/200\n",
      "87/87 [==============================] - 0s - loss: 2.9747 - val_loss: 3.2273\n",
      "Epoch 30/200\n",
      "87/87 [==============================] - 0s - loss: 2.8498 - val_loss: 3.2465\n",
      "Epoch 31/200\n",
      "87/87 [==============================] - 0s - loss: 2.9519 - val_loss: 3.1412\n",
      "Epoch 32/200\n",
      "87/87 [==============================] - 0s - loss: 2.9281 - val_loss: 2.9678\n",
      "Epoch 33/200\n",
      "87/87 [==============================] - 0s - loss: 2.9085 - val_loss: 2.5614\n",
      "Epoch 34/200\n",
      "87/87 [==============================] - 0s - loss: 2.8350 - val_loss: 2.8874\n",
      "Epoch 35/200\n",
      "87/87 [==============================] - 0s - loss: 2.8411 - val_loss: 3.3432\n",
      "Epoch 36/200\n",
      "87/87 [==============================] - 0s - loss: 2.8462 - val_loss: 2.8509\n",
      "Epoch 37/200\n",
      "87/87 [==============================] - 0s - loss: 2.8267 - val_loss: 2.7934\n",
      "Epoch 38/200\n",
      "87/87 [==============================] - 0s - loss: 2.7786 - val_loss: 2.8423\n",
      "Epoch 39/200\n",
      "87/87 [==============================] - 0s - loss: 2.8476 - val_loss: 3.1826\n",
      "Epoch 40/200\n",
      "87/87 [==============================] - 0s - loss: 2.7278 - val_loss: 3.4217\n",
      "Epoch 41/200\n",
      "87/87 [==============================] - 0s - loss: 2.7784 - val_loss: 3.5032\n",
      "Epoch 42/200\n",
      "87/87 [==============================] - 0s - loss: 2.7632 - val_loss: 2.4604\n",
      "Epoch 43/200\n",
      "87/87 [==============================] - 0s - loss: 2.7276 - val_loss: 2.8884\n",
      "Epoch 44/200\n",
      "87/87 [==============================] - 0s - loss: 2.7265 - val_loss: 2.7009\n",
      "Epoch 45/200\n",
      "87/87 [==============================] - 0s - loss: 2.7511 - val_loss: 3.1874ss: 2.74\n",
      "Epoch 46/200\n",
      "87/87 [==============================] - 0s - loss: 2.7212 - val_loss: 2.8360\n",
      "Epoch 47/200\n",
      "87/87 [==============================] - 0s - loss: 2.7281 - val_loss: 2.9165ss: 2.\n",
      "Epoch 48/200\n",
      "87/87 [==============================] - 0s - loss: 2.7159 - val_loss: 2.7106\n",
      "Epoch 49/200\n",
      "87/87 [==============================] - 0s - loss: 2.7001 - val_loss: 2.8799\n",
      "Epoch 50/200\n",
      "87/87 [==============================] - 0s - loss: 2.6689 - val_loss: 2.4707\n",
      "Epoch 51/200\n",
      "87/87 [==============================] - 0s - loss: 2.6447 - val_loss: 2.5246\n",
      "Epoch 52/200\n",
      "87/87 [==============================] - 0s - loss: 2.6962 - val_loss: 3.5523\n",
      "Epoch 53/200\n",
      "82/87 [===========================>..] - ETA: 0s - loss: 2.5334\n",
      " Reduced learning rate to 0.01\n",
      "87/87 [==============================] - 0s - loss: 2.5583 - val_loss: 2.6875\n",
      "Epoch 54/200\n",
      "87/87 [==============================] - 0s - loss: 1.8338 - val_loss: 1.8850\n",
      "Epoch 55/200\n",
      "87/87 [==============================] - 0s - loss: 1.8127 - val_loss: 1.8695\n",
      "Epoch 56/200\n",
      "80/87 [==========================>...] - ETA: 0s - loss: 1.8432\n",
      " Reduced learning rate to 0.005\n",
      "87/87 [==============================] - 0s - loss: 1.8447 - val_loss: 1.8782\n",
      "Epoch 57/200\n",
      "80/87 [==========================>...] - ETA: 0s - loss: 1.8038\n",
      " Reduced learning rate to 0.0025\n",
      "87/87 [==============================] - 0s - loss: 1.7969 - val_loss: 1.8723\n",
      "Epoch 58/200\n",
      "87/87 [==============================] - 0s - loss: 1.8325 - val_loss: 1.8642\n",
      "Epoch 59/200\n",
      "79/87 [==========================>...] - ETA: 0s - loss: 1.8061\n",
      " Reduced learning rate to 0.00125\n",
      "87/87 [==============================] - 0s - loss: 1.8070 - val_loss: 1.9101\n",
      "Epoch 60/200\n",
      "80/87 [==========================>...] - ETA: 0s - loss: 1.8098\n",
      " Reduced learning rate to 0.000625\n",
      "87/87 [==============================] - 0s - loss: 1.8091 - val_loss: 1.8758\n",
      "Epoch 61/200\n",
      "76/87 [=========================>....] - ETA: 0s - loss: 1.8135\n",
      " Reduced learning rate to 0.0003125\n",
      "87/87 [==============================] - 0s - loss: 1.8066 - val_loss: 1.8722\n",
      "Epoch 62/200\n",
      "87/87 [==============================] - 0s - loss: 1.7755 - val_loss: 1.8378\n",
      "Epoch 63/200\n",
      "79/87 [==========================>...] - ETA: 0s - loss: 1.7710\n",
      " Reduced learning rate to 0.00015625\n",
      "87/87 [==============================] - 0s - loss: 1.7613 - val_loss: 1.8577\n",
      "Epoch 64/200\n",
      "85/87 [============================>.] - ETA: 0s - loss: 1.7954\n",
      " Reduced learning rate to 7.8125e-05\n",
      "87/87 [==============================] - 0s - loss: 1.7931 - val_loss: 1.8710\n",
      "Epoch 65/200\n",
      "84/87 [===========================>..] - ETA: 0s - loss: 1.8207\n",
      " Reduced learning rate to 3.90625e-05\n",
      "87/87 [==============================] - 0s - loss: 1.8233 - val_loss: 1.8533\n",
      "Epoch 66/200\n",
      "87/87 [==============================] - 0s - loss: 1.8071 - val_loss: 1.8372\n",
      "Epoch 67/200\n",
      "79/87 [==========================>...] - ETA: 0s - loss: 1.7787\n",
      " Reduced learning rate to 1.95312e-05\n",
      "87/87 [==============================] - 0s - loss: 1.7735 - val_loss: 1.8826\n",
      "Epoch 68/200\n",
      "81/87 [==========================>...] - ETA: 0s - loss: 1.7928\n",
      " Reduced learning rate to 9.76562e-06\n",
      "87/87 [==============================] - 0s - loss: 1.7978 - val_loss: 1.8470\n",
      "Epoch 69/200\n",
      "76/87 [=========================>....] - ETA: 0s - loss: 1.7780\n",
      " Reduced learning rate to 4.88281e-06\n",
      "87/87 [==============================] - 0s - loss: 1.7724 - val_loss: 1.8620\n",
      "Epoch 70/200\n",
      "80/87 [==========================>...] - ETA: 0s - loss: 1.7906\n",
      " Reduced learning rate to 2.44141e-06\n",
      "87/87 [==============================] - 0s - loss: 1.7859 - val_loss: 1.8455\n",
      "Epoch 71/200\n",
      "78/87 [=========================>....] - ETA: 0s - loss: 1.8142\n",
      " Reduced learning rate to 1.2207e-06\n",
      "87/87 [==============================] - 0s - loss: 1.8043 - val_loss: 1.8642\n",
      "Epoch 72/200\n",
      "81/87 [==========================>...] - ETA: 0s - loss: 1.8020\n",
      " Reduced learning rate to 6.10352e-07\n",
      "87/87 [==============================] - 0s - loss: 1.8005 - val_loss: 1.8640\n",
      "Epoch 1/200\n",
      "87/87 [==============================] - 1s - loss: 63.6060 - val_loss: 11.5110\n",
      "Epoch 2/200\n",
      "87/87 [==============================] - 0s - loss: 6.5616 - val_loss: 4.9255\n",
      "Epoch 3/200\n",
      "87/87 [==============================] - 0s - loss: 4.0842 - val_loss: 3.7625\n",
      "Epoch 4/200\n",
      "87/87 [==============================] - 0s - loss: 3.5490 - val_loss: 3.4355\n",
      "Epoch 5/200\n",
      "87/87 [==============================] - 0s - loss: 3.2529 - val_loss: 3.6333\n",
      "Epoch 6/200\n",
      "87/87 [==============================] - 0s - loss: 3.3834 - val_loss: 3.3462\n",
      "Epoch 7/200\n",
      "87/87 [==============================] - 0s - loss: 3.2423 - val_loss: 3.4631\n",
      "Epoch 8/200\n",
      "87/87 [==============================] - 0s - loss: 3.3196 - val_loss: 3.7150\n",
      "Epoch 9/200\n",
      "87/87 [==============================] - 0s - loss: 3.3546 - val_loss: 3.4964\n",
      "Epoch 10/200\n",
      "87/87 [==============================] - 0s - loss: 3.3489 - val_loss: 3.3104\n",
      "Epoch 11/200\n",
      "87/87 [==============================] - 0s - loss: 3.3216 - val_loss: 3.8120\n",
      "Epoch 12/200\n",
      "87/87 [==============================] - 0s - loss: 3.2357 - val_loss: 3.1978\n",
      "Epoch 13/200\n",
      "87/87 [==============================] - 0s - loss: 3.2215 - val_loss: 3.7554\n",
      "Epoch 14/200\n",
      "87/87 [==============================] - 0s - loss: 3.1796 - val_loss: 3.3069\n",
      "Epoch 15/200\n",
      "87/87 [==============================] - 0s - loss: 3.1936 - val_loss: 3.5783\n",
      "Epoch 16/200\n",
      "87/87 [==============================] - 0s - loss: 3.1338 - val_loss: 3.0193\n",
      "Epoch 17/200\n",
      "87/87 [==============================] - 0s - loss: 3.1457 - val_loss: 3.1599\n",
      "Epoch 18/200\n",
      "87/87 [==============================] - 0s - loss: 3.1211 - val_loss: 3.1606\n",
      "Epoch 19/200\n",
      "87/87 [==============================] - 0s - loss: 3.1461 - val_loss: 3.7317\n",
      "Epoch 20/200\n",
      "87/87 [==============================] - 0s - loss: 3.1468 - val_loss: 2.5648\n",
      "Epoch 21/200\n",
      "87/87 [==============================] - 0s - loss: 3.0357 - val_loss: 2.8655\n",
      "Epoch 22/200\n",
      "87/87 [==============================] - 0s - loss: 3.0744 - val_loss: 3.1675\n",
      "Epoch 23/200\n",
      "87/87 [==============================] - 0s - loss: 2.9923 - val_loss: 3.6770\n",
      "Epoch 24/200\n",
      "87/87 [==============================] - 0s - loss: 3.0564 - val_loss: 2.9717\n",
      "Epoch 25/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s - loss: 2.9151 - val_loss: 3.1872\n",
      "Epoch 26/200\n",
      "87/87 [==============================] - 0s - loss: 2.9998 - val_loss: 3.1997\n",
      "Epoch 27/200\n",
      "87/87 [==============================] - 0s - loss: 2.9544 - val_loss: 2.7468\n",
      "Epoch 28/200\n",
      "87/87 [==============================] - 0s - loss: 2.8353 - val_loss: 3.3381\n",
      "Epoch 29/200\n",
      "87/87 [==============================] - 0s - loss: 2.9641 - val_loss: 3.1833\n",
      "Epoch 30/200\n",
      "87/87 [==============================] - 0s - loss: 2.9175 - val_loss: 2.9668\n",
      "Epoch 31/200\n",
      "82/87 [===========================>..] - ETA: 0s - loss: 2.9447\n",
      " Reduced learning rate to 0.01\n",
      "87/87 [==============================] - 0s - loss: 2.9370 - val_loss: 2.6840\n",
      "Epoch 32/200\n",
      "87/87 [==============================] - 0s - loss: 1.9489 - val_loss: 1.9970\n",
      "Epoch 33/200\n",
      "87/87 [==============================] - 0s - loss: 1.9361 - val_loss: 1.9945\n",
      "Epoch 34/200\n",
      "87/87 [==============================] - 0s - loss: 1.8669 - val_loss: 1.9795\n",
      "Epoch 35/200\n",
      "87/87 [==============================] - 0s - loss: 1.8896 - val_loss: 1.9631\n",
      "Epoch 36/200\n",
      "80/87 [==========================>...] - ETA: 0s - loss: 1.9071\n",
      " Reduced learning rate to 0.005\n",
      "87/87 [==============================] - 0s - loss: 1.9008 - val_loss: 1.9660\n",
      "Epoch 37/200\n",
      "81/87 [==========================>...] - ETA: 0s - loss: 1.8122\n",
      " Reduced learning rate to 0.0025\n",
      "87/87 [==============================] - 0s - loss: 1.8271 - val_loss: 1.9636\n",
      "Epoch 38/200\n",
      "79/87 [==========================>...] - ETA: 0s - loss: 1.8742\n",
      " Reduced learning rate to 0.00125\n",
      "87/87 [==============================] - 0s - loss: 1.8716 - val_loss: 1.9732\n",
      "Epoch 39/200\n",
      "87/87 [==============================] - 0s - loss: 1.8272 - val_loss: 1.9459\n",
      "Epoch 40/200\n",
      "82/87 [===========================>..] - ETA: 0s - loss: 1.8589\n",
      " Reduced learning rate to 0.000625\n",
      "87/87 [==============================] - 0s - loss: 1.8796 - val_loss: 1.9607\n",
      "Epoch 41/200\n",
      "81/87 [==========================>...] - ETA: 0s - loss: 1.8386\n",
      " Reduced learning rate to 0.0003125\n",
      "87/87 [==============================] - 0s - loss: 1.8356 - val_loss: 1.9533\n",
      "Epoch 42/200\n",
      "82/87 [===========================>..] - ETA: 0s - loss: 1.8925\n",
      " Reduced learning rate to 0.00015625\n",
      "87/87 [==============================] - 0s - loss: 1.8918 - val_loss: 1.9460\n",
      "Epoch 43/200\n",
      "87/87 [==============================] - 0s - loss: 1.8822 - val_loss: 1.9053\n",
      "Epoch 44/200\n",
      "80/87 [==========================>...] - ETA: 0s - loss: 1.8672\n",
      " Reduced learning rate to 7.8125e-05\n",
      "87/87 [==============================] - 0s - loss: 1.8664 - val_loss: 1.9394\n",
      "Epoch 45/200\n",
      "80/87 [==========================>...] - ETA: 0s - loss: 1.8403\n",
      " Reduced learning rate to 3.90625e-05\n",
      "87/87 [==============================] - 0s - loss: 1.8348 - val_loss: 1.9166\n",
      "Epoch 46/200\n",
      "81/87 [==========================>...] - ETA: 0s - loss: 1.8639\n",
      " Reduced learning rate to 1.95312e-05\n",
      "87/87 [==============================] - 0s - loss: 1.8620 - val_loss: 1.9686\n",
      "Epoch 47/200\n",
      "81/87 [==========================>...] - ETA: 0s - loss: 1.8114\n",
      " Reduced learning rate to 9.76562e-06\n",
      "87/87 [==============================] - 0s - loss: 1.8112 - val_loss: 1.9322\n",
      "Epoch 48/200\n",
      "81/87 [==========================>...] - ETA: 0s - loss: 1.8435\n",
      " Reduced learning rate to 4.88281e-06\n",
      "87/87 [==============================] - 0s - loss: 1.8454 - val_loss: 1.9343\n",
      "Epoch 49/200\n",
      "80/87 [==========================>...] - ETA: 0s - loss: 1.8322\n",
      " Reduced learning rate to 2.44141e-06\n",
      "87/87 [==============================] - 0s - loss: 1.8472 - val_loss: 1.9399\n",
      "Epoch 50/200\n",
      "82/87 [===========================>..] - ETA: 0s - loss: 1.8781\n",
      " Reduced learning rate to 1.2207e-06\n",
      "87/87 [==============================] - 0s - loss: 1.8749 - val_loss: 1.9292\n",
      "Epoch 51/200\n",
      "82/87 [===========================>..] - ETA: 0s - loss: 1.8626\n",
      " Reduced learning rate to 6.10352e-07\n",
      "87/87 [==============================] - 0s - loss: 1.8557 - val_loss: 1.9548\n",
      "Epoch 1/200\n",
      "87/87 [==============================] - 1s - loss: 59.4407 - val_loss: 10.7112\n",
      "Epoch 2/200\n",
      "87/87 [==============================] - 0s - loss: 6.1952 - val_loss: 4.7319\n",
      "Epoch 3/200\n",
      "87/87 [==============================] - 0s - loss: 4.0805 - val_loss: 3.6926\n",
      "Epoch 4/200\n",
      "87/87 [==============================] - 0s - loss: 3.4528 - val_loss: 3.4280\n",
      "Epoch 5/200\n",
      "87/87 [==============================] - 0s - loss: 3.3643 - val_loss: 3.5200\n",
      "Epoch 6/200\n",
      "87/87 [==============================] - 0s - loss: 3.3031 - val_loss: 3.9646\n",
      "Epoch 7/200\n",
      "87/87 [==============================] - 0s - loss: 3.2899 - val_loss: 3.2866\n",
      "Epoch 8/200\n",
      "87/87 [==============================] - 0s - loss: 3.3026 - val_loss: 3.3874\n",
      "Epoch 9/200\n",
      "87/87 [==============================] - 0s - loss: 3.2540 - val_loss: 3.4739\n",
      "Epoch 10/200\n",
      "87/87 [==============================] - 0s - loss: 3.3141 - val_loss: 3.2806\n",
      "Epoch 11/200\n",
      "87/87 [==============================] - 0s - loss: 3.2080 - val_loss: 3.7478\n",
      "Epoch 12/200\n",
      "87/87 [==============================] - 0s - loss: 3.2651 - val_loss: 3.9353\n",
      "Epoch 13/200\n",
      "87/87 [==============================] - 0s - loss: 3.2207 - val_loss: 3.3398\n",
      "Epoch 14/200\n",
      "87/87 [==============================] - 0s - loss: 3.2079 - val_loss: 3.8711\n",
      "Epoch 15/200\n",
      "87/87 [==============================] - 0s - loss: 3.1452 - val_loss: 3.5207\n",
      "Epoch 16/200\n",
      "87/87 [==============================] - 0s - loss: 3.1858 - val_loss: 3.2681\n",
      "Epoch 17/200\n",
      "87/87 [==============================] - 0s - loss: 3.0968 - val_loss: 3.0503\n",
      "Epoch 18/200\n",
      "87/87 [==============================] - 0s - loss: 3.0938 - val_loss: 3.3664\n",
      "Epoch 19/200\n",
      "87/87 [==============================] - 0s - loss: 3.1308 - val_loss: 2.8652\n",
      "Epoch 20/200\n",
      "87/87 [==============================] - 0s - loss: 3.0343 - val_loss: 3.1448\n",
      "Epoch 21/200\n",
      "87/87 [==============================] - 0s - loss: 3.0434 - val_loss: 2.8284\n",
      "Epoch 22/200\n",
      "87/87 [==============================] - 0s - loss: 3.0050 - val_loss: 3.4778\n",
      "Epoch 23/200\n",
      "87/87 [==============================] - 0s - loss: 3.0820 - val_loss: 3.4917\n",
      "Epoch 24/200\n",
      "87/87 [==============================] - 0s - loss: 3.0075 - val_loss: 3.0478\n",
      "Epoch 25/200\n",
      "87/87 [==============================] - 0s - loss: 3.0496 - val_loss: 3.3793ss: 3.0\n",
      "Epoch 26/200\n",
      "87/87 [==============================] - 0s - loss: 2.9638 - val_loss: 2.5330\n",
      "Epoch 27/200\n",
      "87/87 [==============================] - 0s - loss: 2.9861 - val_loss: 2.9771\n",
      "Epoch 28/200\n",
      "87/87 [==============================] - 0s - loss: 3.0301 - val_loss: 2.5767\n",
      "Epoch 29/200\n",
      "87/87 [==============================] - 0s - loss: 2.8908 - val_loss: 3.5628\n",
      "Epoch 30/200\n",
      "87/87 [==============================] - 0s - loss: 2.9262 - val_loss: 3.4788\n",
      "Epoch 31/200\n",
      "87/87 [==============================] - 0s - loss: 2.9405 - val_loss: 2.5630ss: 3.075 - ETA: 0s - loss: 2.966 - ETA: 0s - loss: 2.96\n",
      "Epoch 32/200\n",
      "87/87 [==============================] - 0s - loss: 2.8854 - val_loss: 2.7932\n",
      "Epoch 33/200\n",
      "87/87 [==============================] - ETA: 0s - loss: 2.916 - 0s - loss: 2.9109 - val_loss: 3.2378\n",
      "Epoch 34/200\n",
      "87/87 [==============================] - 0s - loss: 2.8506 - val_loss: 3.1296\n",
      "Epoch 35/200\n",
      "87/87 [==============================] - 0s - loss: 2.8619 - val_loss: 2.9940\n",
      "Epoch 36/200\n",
      "87/87 [==============================] - 0s - loss: 2.8272 - val_loss: 2.4875\n",
      "Epoch 37/200\n",
      "87/87 [==============================] - 0s - loss: 2.7742 - val_loss: 2.8993\n",
      "Epoch 38/200\n",
      "87/87 [==============================] - 0s - loss: 2.8509 - val_loss: 2.4382\n",
      "Epoch 39/200\n",
      "87/87 [==============================] - 0s - loss: 2.7587 - val_loss: 2.8799\n",
      "Epoch 40/200\n",
      "87/87 [==============================] - 0s - loss: 2.8528 - val_loss: 2.4540\n",
      "Epoch 41/200\n",
      "87/87 [==============================] - 0s - loss: 2.7465 - val_loss: 2.7085\n",
      "Epoch 42/200\n",
      "87/87 [==============================] - 0s - loss: 2.7448 - val_loss: 3.1589\n",
      "Epoch 43/200\n",
      "87/87 [==============================] - 0s - loss: 2.8011 - val_loss: 2.8942\n",
      "Epoch 44/200\n",
      "87/87 [==============================] - 0s - loss: 2.7833 - val_loss: 2.7598\n",
      "Epoch 45/200\n",
      "87/87 [==============================] - 0s - loss: 2.7342 - val_loss: 2.8342\n",
      "Epoch 46/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s - loss: 2.7198 - val_loss: 2.4323\n",
      "Epoch 47/200\n",
      "87/87 [==============================] - 0s - loss: 2.7245 - val_loss: 2.6315\n",
      "Epoch 48/200\n",
      "87/87 [==============================] - 0s - loss: 2.7016 - val_loss: 2.9354\n",
      "Epoch 49/200\n",
      "87/87 [==============================] - 0s - loss: 2.7484 - val_loss: 2.9382\n",
      "Epoch 50/200\n",
      "87/87 [==============================] - 0s - loss: 2.7127 - val_loss: 2.7910\n",
      "Epoch 51/200\n",
      "87/87 [==============================] - 0s - loss: 2.7162 - val_loss: 2.3879\n",
      "Epoch 52/200\n",
      "87/87 [==============================] - 0s - loss: 2.6806 - val_loss: 3.2508\n",
      "Epoch 53/200\n",
      "87/87 [==============================] - 0s - loss: 2.6549 - val_loss: 2.7902\n",
      "Epoch 54/200\n",
      "87/87 [==============================] - 0s - loss: 2.6592 - val_loss: 2.8328\n",
      "Epoch 55/200\n",
      "87/87 [==============================] - 0s - loss: 2.6590 - val_loss: 2.2611\n",
      "Epoch 56/200\n",
      "87/87 [==============================] - 0s - loss: 2.6083 - val_loss: 2.6079\n",
      "Epoch 57/200\n",
      "87/87 [==============================] - 0s - loss: 2.6436 - val_loss: 2.2640\n",
      "Epoch 58/200\n",
      "87/87 [==============================] - 0s - loss: 2.5772 - val_loss: 2.3437\n",
      "Epoch 59/200\n",
      "87/87 [==============================] - 0s - loss: 2.6264 - val_loss: 2.6380\n",
      "Epoch 60/200\n",
      "87/87 [==============================] - 0s - loss: 2.5801 - val_loss: 2.6273\n",
      "Epoch 61/200\n",
      "87/87 [==============================] - 0s - loss: 2.5746 - val_loss: 3.1876\n",
      "Epoch 62/200\n",
      "87/87 [==============================] - 0s - loss: 2.6021 - val_loss: 2.7676\n",
      "Epoch 63/200\n",
      "87/87 [==============================] - 0s - loss: 2.5971 - val_loss: 2.5908\n",
      "Epoch 64/200\n",
      "87/87 [==============================] - 0s - loss: 2.6003 - val_loss: 2.7668\n",
      "Epoch 65/200\n",
      "87/87 [==============================] - 0s - loss: 2.5513 - val_loss: 2.1060\n",
      "Epoch 66/200\n",
      "87/87 [==============================] - 0s - loss: 2.4759 - val_loss: 2.4513\n",
      "Epoch 67/200\n",
      "87/87 [==============================] - 0s - loss: 2.5388 - val_loss: 2.4412\n",
      "Epoch 68/200\n",
      "87/87 [==============================] - 0s - loss: 2.5238 - val_loss: 2.3724\n",
      "Epoch 69/200\n",
      "87/87 [==============================] - 0s - loss: 2.3775 - val_loss: 2.9360\n",
      "Epoch 70/200\n",
      "87/87 [==============================] - 0s - loss: 2.5512 - val_loss: 2.3027\n",
      "Epoch 71/200\n",
      "87/87 [==============================] - 0s - loss: 2.5699 - val_loss: 2.5997\n",
      "Epoch 72/200\n",
      "87/87 [==============================] - 0s - loss: 2.5519 - val_loss: 2.3440\n",
      "Epoch 73/200\n",
      "87/87 [==============================] - 0s - loss: 2.5388 - val_loss: 2.5686\n",
      "Epoch 74/200\n",
      "87/87 [==============================] - 0s - loss: 2.5006 - val_loss: 2.5799\n",
      "Epoch 75/200\n",
      "87/87 [==============================] - 0s - loss: 2.5522 - val_loss: 2.3403\n",
      "Epoch 76/200\n",
      "78/87 [=========================>....] - ETA: 0s - loss: 2.5196\n",
      " Reduced learning rate to 0.01\n",
      "87/87 [==============================] - 0s - loss: 2.5074 - val_loss: 2.3897\n",
      "Epoch 77/200\n",
      "87/87 [==============================] - 0s - loss: 1.7858 - val_loss: 1.8412\n",
      "Epoch 78/200\n",
      "87/87 [==============================] - 0s - loss: 1.7614 - val_loss: 1.8401\n",
      "Epoch 79/200\n",
      "80/87 [==========================>...] - ETA: 0s - loss: 1.8422\n",
      " Reduced learning rate to 0.005\n",
      "87/87 [==============================] - 0s - loss: 1.8330 - val_loss: 1.8604\n",
      "Epoch 80/200\n",
      "87/87 [==============================] - 0s - loss: 1.7575 - val_loss: 1.8377\n",
      "Epoch 81/200\n",
      "87/87 [==============================] - 0s - loss: 1.7807 - val_loss: 1.8115\n",
      "Epoch 82/200\n",
      "79/87 [==========================>...] - ETA: 0s - loss: 1.7373\n",
      " Reduced learning rate to 0.0025\n",
      "87/87 [==============================] - 0s - loss: 1.7393 - val_loss: 1.8127\n",
      "Epoch 83/200\n",
      "78/87 [=========================>....] - ETA: 0s - loss: 1.7698\n",
      " Reduced learning rate to 0.00125\n",
      "87/87 [==============================] - 0s - loss: 1.7884 - val_loss: 1.8216\n",
      "Epoch 84/200\n",
      "87/87 [==============================] - 0s - loss: 1.7656 - val_loss: 1.7965\n",
      "Epoch 85/200\n",
      "79/87 [==========================>...] - ETA: 0s - loss: 1.7772\n",
      " Reduced learning rate to 0.000625\n",
      "87/87 [==============================] - 0s - loss: 1.7759 - val_loss: 1.8251\n",
      "Epoch 86/200\n",
      "81/87 [==========================>...] - ETA: 0s - loss: 1.7517\n",
      " Reduced learning rate to 0.0003125\n",
      "87/87 [==============================] - 0s - loss: 1.7577 - val_loss: 1.8042\n",
      "Epoch 87/200\n",
      "87/87 [==============================] - 0s - loss: 1.7648 - val_loss: 1.7869\n",
      "Epoch 88/200\n",
      "78/87 [=========================>....] - ETA: 0s - loss: 1.7796\n",
      " Reduced learning rate to 0.00015625\n",
      "87/87 [==============================] - 0s - loss: 1.7789 - val_loss: 1.8087\n",
      "Epoch 89/200\n",
      "78/87 [=========================>....] - ETA: 0s - loss: 1.7589\n",
      " Reduced learning rate to 7.8125e-05\n",
      "87/87 [==============================] - 0s - loss: 1.7675 - val_loss: 1.8378\n",
      "Epoch 90/200\n",
      "78/87 [=========================>....] - ETA: 0s - loss: 1.7805\n",
      " Reduced learning rate to 3.90625e-05\n",
      "87/87 [==============================] - 0s - loss: 1.7702 - val_loss: 1.8534\n",
      "Epoch 91/200\n",
      "79/87 [==========================>...] - ETA: 0s - loss: 1.7728\n",
      " Reduced learning rate to 1.95312e-05\n",
      "87/87 [==============================] - 0s - loss: 1.7663 - val_loss: 1.8336\n",
      "Epoch 92/200\n",
      "78/87 [=========================>....] - ETA: 0s - loss: 1.7662\n",
      " Reduced learning rate to 9.76562e-06\n",
      "87/87 [==============================] - 0s - loss: 1.7555 - val_loss: 1.8162\n",
      "Epoch 93/200\n",
      "81/87 [==========================>...] - ETA: 0s - loss: 1.7615\n",
      " Reduced learning rate to 4.88281e-06\n",
      "87/87 [==============================] - 0s - loss: 1.7556 - val_loss: 1.8055\n",
      "Epoch 94/200\n",
      "80/87 [==========================>...] - ETA: 0s - loss: 1.8198\n",
      " Reduced learning rate to 2.44141e-06\n",
      "87/87 [==============================] - 0s - loss: 1.8212 - val_loss: 1.8324\n",
      "Epoch 95/200\n",
      "80/87 [==========================>...] - ETA: 0s - loss: 1.7646\n",
      " Reduced learning rate to 1.2207e-06\n",
      "87/87 [==============================] - 0s - loss: 1.7615 - val_loss: 1.8534\n",
      "Epoch 96/200\n",
      "78/87 [=========================>....] - ETA: 0s - loss: 1.7682\n",
      " Reduced learning rate to 6.10352e-07\n",
      "87/87 [==============================] - 0s - loss: 1.7688 - val_loss: 1.8332\n",
      "Epoch 1/200\n",
      "87/87 [==============================] - 1s - loss: 56.8169 - val_loss: 9.9935\n",
      "Epoch 2/200\n",
      "87/87 [==============================] - 0s - loss: 6.1812 - val_loss: 4.7702\n",
      "Epoch 3/200\n",
      "87/87 [==============================] - 0s - loss: 4.1945 - val_loss: 4.0342\n",
      "Epoch 4/200\n",
      "87/87 [==============================] - 0s - loss: 3.5297 - val_loss: 3.5120\n",
      "Epoch 5/200\n",
      "87/87 [==============================] - 0s - loss: 3.3512 - val_loss: 3.5319\n",
      "Epoch 6/200\n",
      "87/87 [==============================] - 0s - loss: 3.4094 - val_loss: 3.3461\n",
      "Epoch 7/200\n",
      "87/87 [==============================] - 0s - loss: 3.4129 - val_loss: 3.3983\n",
      "Epoch 8/200\n",
      "87/87 [==============================] - 0s - loss: 3.3512 - val_loss: 3.2850\n",
      "Epoch 9/200\n",
      "87/87 [==============================] - 0s - loss: 3.2665 - val_loss: 3.2244\n",
      "Epoch 10/200\n",
      "87/87 [==============================] - 0s - loss: 3.3244 - val_loss: 3.5500\n",
      "Epoch 11/200\n",
      "87/87 [==============================] - 0s - loss: 3.2500 - val_loss: 3.4309\n",
      "Epoch 12/200\n",
      "87/87 [==============================] - 0s - loss: 3.2926 - val_loss: 3.2559\n",
      "Epoch 13/200\n",
      "87/87 [==============================] - 0s - loss: 3.2931 - val_loss: 3.1001\n",
      "Epoch 14/200\n",
      "87/87 [==============================] - 0s - loss: 3.2260 - val_loss: 2.9653\n",
      "Epoch 15/200\n",
      "87/87 [==============================] - 0s - loss: 3.1353 - val_loss: 3.6116\n",
      "Epoch 16/200\n",
      "87/87 [==============================] - 0s - loss: 3.2167 - val_loss: 3.0542\n",
      "Epoch 17/200\n",
      "87/87 [==============================] - 0s - loss: 3.1812 - val_loss: 3.0601\n",
      "Epoch 18/200\n",
      "87/87 [==============================] - 0s - loss: 3.1554 - val_loss: 2.9205\n",
      "Epoch 19/200\n",
      "87/87 [==============================] - 0s - loss: 3.1006 - val_loss: 3.2789\n",
      "Epoch 20/200\n",
      "87/87 [==============================] - 0s - loss: 3.1753 - val_loss: 3.6706\n",
      "Epoch 21/200\n",
      "87/87 [==============================] - 0s - loss: 3.0927 - val_loss: 3.4583\n",
      "Epoch 22/200\n",
      "87/87 [==============================] - 0s - loss: 3.0464 - val_loss: 3.7479\n",
      "Epoch 23/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s - loss: 3.0534 - val_loss: 3.0245\n",
      "Epoch 24/200\n",
      "87/87 [==============================] - 0s - loss: 3.0563 - val_loss: 2.8903\n",
      "Epoch 25/200\n",
      "87/87 [==============================] - 0s - loss: 3.0171 - val_loss: 3.7161\n",
      "Epoch 26/200\n",
      "87/87 [==============================] - 0s - loss: 2.9736 - val_loss: 3.2275\n",
      "Epoch 27/200\n",
      "87/87 [==============================] - 0s - loss: 2.9638 - val_loss: 2.8162\n",
      "Epoch 28/200\n",
      "87/87 [==============================] - 0s - loss: 2.9511 - val_loss: 3.3315\n",
      "Epoch 29/200\n",
      "87/87 [==============================] - 0s - loss: 2.9487 - val_loss: 3.0612\n",
      "Epoch 30/200\n",
      "87/87 [==============================] - 0s - loss: 2.9711 - val_loss: 3.2214\n",
      "Epoch 31/200\n",
      "87/87 [==============================] - 0s - loss: 2.8870 - val_loss: 3.0080\n",
      "Epoch 32/200\n",
      "87/87 [==============================] - 0s - loss: 2.9089 - val_loss: 2.8958\n",
      "Epoch 33/200\n",
      "87/87 [==============================] - 0s - loss: 2.9253 - val_loss: 3.1765\n",
      "Epoch 34/200\n",
      "87/87 [==============================] - 0s - loss: 2.8901 - val_loss: 3.0392\n",
      "Epoch 35/200\n",
      "87/87 [==============================] - 0s - loss: 2.9129 - val_loss: 2.6741\n",
      "Epoch 36/200\n",
      "87/87 [==============================] - 0s - loss: 2.8603 - val_loss: 2.9553\n",
      "Epoch 37/200\n",
      "87/87 [==============================] - 0s - loss: 2.8742 - val_loss: 2.8138\n",
      "Epoch 38/200\n",
      "87/87 [==============================] - 0s - loss: 2.8461 - val_loss: 2.7605\n",
      "Epoch 39/200\n",
      "87/87 [==============================] - 0s - loss: 2.8063 - val_loss: 3.5239\n",
      "Epoch 40/200\n",
      "87/87 [==============================] - 0s - loss: 2.8424 - val_loss: 3.1516\n",
      "Epoch 41/200\n",
      "87/87 [==============================] - 0s - loss: 2.7935 - val_loss: 2.7775\n",
      "Epoch 42/200\n",
      "87/87 [==============================] - 0s - loss: 2.8039 - val_loss: 2.9416\n",
      "Epoch 43/200\n",
      "87/87 [==============================] - 0s - loss: 2.7244 - val_loss: 3.2637\n",
      "Epoch 44/200\n",
      "87/87 [==============================] - 0s - loss: 2.7841 - val_loss: 2.5851\n",
      "Epoch 45/200\n",
      "87/87 [==============================] - 0s - loss: 2.7094 - val_loss: 2.8059\n",
      "Epoch 46/200\n",
      "87/87 [==============================] - 0s - loss: 2.7148 - val_loss: 2.7054\n",
      "Epoch 47/200\n",
      "87/87 [==============================] - 0s - loss: 2.7436 - val_loss: 3.1194\n",
      "Epoch 48/200\n",
      "87/87 [==============================] - 0s - loss: 2.8413 - val_loss: 2.3425\n",
      "Epoch 49/200\n",
      "87/87 [==============================] - 0s - loss: 2.7293 - val_loss: 2.6492\n",
      "Epoch 50/200\n",
      "87/87 [==============================] - 0s - loss: 2.7171 - val_loss: 3.2630\n",
      "Epoch 51/200\n",
      "87/87 [==============================] - 0s - loss: 2.6799 - val_loss: 2.6799\n",
      "Epoch 52/200\n",
      "87/87 [==============================] - 0s - loss: 2.6916 - val_loss: 2.6990\n",
      "Epoch 53/200\n",
      "87/87 [==============================] - 0s - loss: 2.6874 - val_loss: 2.1311\n",
      "Epoch 54/200\n",
      "87/87 [==============================] - 0s - loss: 2.6438 - val_loss: 2.2093\n",
      "Epoch 55/200\n",
      "87/87 [==============================] - 0s - loss: 2.6412 - val_loss: 2.9655\n",
      "Epoch 56/200\n",
      "87/87 [==============================] - 0s - loss: 2.7337 - val_loss: 2.4567\n",
      "Epoch 57/200\n",
      "87/87 [==============================] - 0s - loss: 2.6466 - val_loss: 3.0574\n",
      "Epoch 58/200\n",
      "87/87 [==============================] - 0s - loss: 2.6077 - val_loss: 2.6789\n",
      "Epoch 59/200\n",
      "87/87 [==============================] - 0s - loss: 2.6876 - val_loss: 2.7045\n",
      "Epoch 60/200\n",
      "87/87 [==============================] - 0s - loss: 2.5991 - val_loss: 2.9304\n",
      "Epoch 61/200\n",
      "87/87 [==============================] - 0s - loss: 2.6283 - val_loss: 2.7718\n",
      "Epoch 62/200\n",
      "87/87 [==============================] - 0s - loss: 2.5549 - val_loss: 2.6959\n",
      "Epoch 63/200\n",
      "87/87 [==============================] - 0s - loss: 2.5661 - val_loss: 2.4021\n",
      "Epoch 64/200\n",
      "75/87 [========================>.....] - ETA: 0s - loss: 2.5741\n",
      " Reduced learning rate to 0.01\n",
      "87/87 [==============================] - 0s - loss: 2.5820 - val_loss: 2.6812\n",
      "Epoch 65/200\n",
      "87/87 [==============================] - 0s - loss: 1.8204 - val_loss: 1.8430\n",
      "Epoch 66/200\n",
      "71/87 [=======================>......] - ETA: 0s - loss: 1.8245\n",
      " Reduced learning rate to 0.005\n",
      "87/87 [==============================] - 0s - loss: 1.8104 - val_loss: 1.8928\n",
      "Epoch 67/200\n",
      "77/87 [=========================>....] - ETA: 0s - loss: 1.7779\n",
      " Reduced learning rate to 0.0025\n",
      "87/87 [==============================] - 0s - loss: 1.7747 - val_loss: 1.8639\n",
      "Epoch 68/200\n",
      "87/87 [==============================] - 0s - loss: 1.7867 - val_loss: 1.8329\n",
      "Epoch 69/200\n",
      "72/87 [=======================>......] - ETA: 0s - loss: 1.7784\n",
      " Reduced learning rate to 0.00125\n",
      "87/87 [==============================] - 0s - loss: 1.7773 - val_loss: 1.8580\n",
      "Epoch 70/200\n",
      "70/87 [=======================>......] - ETA: 0s - loss: 1.8045\n",
      " Reduced learning rate to 0.000625\n",
      "87/87 [==============================] - 0s - loss: 1.8154 - val_loss: 1.8514\n",
      "Epoch 71/200\n",
      "85/87 [============================>.] - ETA: 0s - loss: 1.7783\n",
      " Reduced learning rate to 0.0003125\n",
      "87/87 [==============================] - 0s - loss: 1.7752 - val_loss: 1.8843\n",
      "Epoch 72/200\n",
      "77/87 [=========================>....] - ETA: 0s - loss: 1.7935\n",
      " Reduced learning rate to 0.00015625\n",
      "87/87 [==============================] - 0s - loss: 1.7811 - val_loss: 1.8474\n",
      "Epoch 73/200\n",
      "86/87 [============================>.] - ETA: 0s - loss: 1.8481\n",
      " Reduced learning rate to 7.8125e-05\n",
      "87/87 [==============================] - 0s - loss: 1.8623 - val_loss: 1.8444\n",
      "Epoch 74/200\n",
      "72/87 [=======================>......] - ETA: 0s - loss: 1.7871\n",
      " Reduced learning rate to 3.90625e-05\n",
      "87/87 [==============================] - 0s - loss: 1.7945 - val_loss: 1.8568\n",
      "Epoch 75/200\n",
      "87/87 [==============================] - 0s - loss: 1.7971 - val_loss: 1.8190\n",
      "Epoch 76/200\n",
      "72/87 [=======================>......] - ETA: 0s - loss: 1.8011\n",
      " Reduced learning rate to 1.95312e-05\n",
      "87/87 [==============================] - 0s - loss: 1.7999 - val_loss: 1.8573\n",
      "Epoch 77/200\n",
      "71/87 [=======================>......] - ETA: 0s - loss: 1.7561\n",
      " Reduced learning rate to 9.76562e-06\n",
      "87/87 [==============================] - 0s - loss: 1.7530 - val_loss: 1.8324\n",
      "Epoch 78/200\n",
      "77/87 [=========================>....] - ETA: 0s - loss: 1.7919\n",
      " Reduced learning rate to 4.88281e-06\n",
      "87/87 [==============================] - 0s - loss: 1.7835 - val_loss: 1.8682\n",
      "Epoch 79/200\n",
      "78/87 [=========================>....] - ETA: 0s - loss: 1.7723\n",
      " Reduced learning rate to 2.44141e-06\n",
      "87/87 [==============================] - 0s - loss: 1.7724 - val_loss: 1.8373\n",
      "Epoch 80/200\n",
      "78/87 [=========================>....] - ETA: 0s - loss: 1.7978\n",
      " Reduced learning rate to 1.2207e-06\n",
      "87/87 [==============================] - 0s - loss: 1.7925 - val_loss: 1.8269\n",
      "Epoch 81/200\n",
      "78/87 [=========================>....] - ETA: 0s - loss: 1.7857\n",
      " Reduced learning rate to 6.10352e-07\n",
      "87/87 [==============================] - 0s - loss: 1.8011 - val_loss: 1.8302\n",
      "Epoch 1/200\n",
      "175/175 [==============================] - 1s - loss: 31.4916 - val_loss: 4.5625\n",
      "Epoch 2/200\n",
      "175/175 [==============================] - 0s - loss: 3.9921 - val_loss: 3.2212\n",
      "Epoch 3/200\n",
      "175/175 [==============================] - 0s - loss: 3.3338 - val_loss: 3.5824\n",
      "Epoch 4/200\n",
      "175/175 [==============================] - 0s - loss: 3.4139 - val_loss: 3.3896\n",
      "Epoch 5/200\n",
      "175/175 [==============================] - 0s - loss: 3.3512 - val_loss: 3.1958\n",
      "Epoch 6/200\n",
      "175/175 [==============================] - 0s - loss: 3.3351 - val_loss: 3.1649\n",
      "Epoch 7/200\n",
      "175/175 [==============================] - 0s - loss: 3.3111 - val_loss: 3.6690\n",
      "Epoch 8/200\n",
      "175/175 [==============================] - 0s - loss: 3.2129 - val_loss: 3.3032\n",
      "Epoch 9/200\n",
      "175/175 [==============================] - 0s - loss: 3.2089 - val_loss: 2.5763\n",
      "Epoch 10/200\n",
      "175/175 [==============================] - 0s - loss: 3.1316 - val_loss: 2.5558\n",
      "Epoch 11/200\n",
      "175/175 [==============================] - 0s - loss: 3.1262 - val_loss: 3.1107\n",
      "Epoch 12/200\n",
      "175/175 [==============================] - 0s - loss: 3.0805 - val_loss: 3.6488\n",
      "Epoch 13/200\n",
      "175/175 [==============================] - 0s - loss: 3.0275 - val_loss: 3.0426\n",
      "Epoch 14/200\n",
      "175/175 [==============================] - 0s - loss: 3.0619 - val_loss: 2.8627\n",
      "Epoch 15/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 0s - loss: 3.0024 - val_loss: 2.5800\n",
      "Epoch 16/200\n",
      "175/175 [==============================] - 0s - loss: 2.9552 - val_loss: 2.8247\n",
      "Epoch 17/200\n",
      "175/175 [==============================] - 0s - loss: 2.9223 - val_loss: 2.6616\n",
      "Epoch 18/200\n",
      "175/175 [==============================] - 0s - loss: 2.8694 - val_loss: 2.4533\n",
      "Epoch 19/200\n",
      "175/175 [==============================] - 0s - loss: 2.8975 - val_loss: 2.7846\n",
      "Epoch 20/200\n",
      "175/175 [==============================] - 0s - loss: 2.8347 - val_loss: 2.8369\n",
      "Epoch 21/200\n",
      "175/175 [==============================] - 0s - loss: 2.8299 - val_loss: 2.4085\n",
      "Epoch 22/200\n",
      "175/175 [==============================] - 0s - loss: 2.7624 - val_loss: 2.8299\n",
      "Epoch 23/200\n",
      "175/175 [==============================] - 0s - loss: 2.7485 - val_loss: 2.3632\n",
      "Epoch 24/200\n",
      "175/175 [==============================] - 0s - loss: 2.6991 - val_loss: 2.3903\n",
      "Epoch 25/200\n",
      "175/175 [==============================] - 0s - loss: 2.7403 - val_loss: 2.2242\n",
      "Epoch 26/200\n",
      "175/175 [==============================] - 0s - loss: 2.6989 - val_loss: 2.2968\n",
      "Epoch 27/200\n",
      "175/175 [==============================] - 0s - loss: 2.6735 - val_loss: 3.0483\n",
      "Epoch 28/200\n",
      "175/175 [==============================] - 0s - loss: 2.6333 - val_loss: 2.2690\n",
      "Epoch 29/200\n",
      "175/175 [==============================] - 0s - loss: 2.6556 - val_loss: 2.7792\n",
      "Epoch 30/200\n",
      "175/175 [==============================] - 0s - loss: 2.6207 - val_loss: 2.8160\n",
      "Epoch 31/200\n",
      "175/175 [==============================] - 0s - loss: 2.6216 - val_loss: 2.2089\n",
      "Epoch 32/200\n",
      "175/175 [==============================] - 0s - loss: 2.5776 - val_loss: 2.3342\n",
      "Epoch 33/200\n",
      "175/175 [==============================] - 0s - loss: 2.5300 - val_loss: 2.7787\n",
      "Epoch 34/200\n",
      "175/175 [==============================] - 0s - loss: 2.5090 - val_loss: 3.3246\n",
      "Epoch 35/200\n",
      "175/175 [==============================] - 0s - loss: 2.5212 - val_loss: 2.5679\n",
      "Epoch 36/200\n",
      "175/175 [==============================] - 0s - loss: 2.5750 - val_loss: 2.1572\n",
      "Epoch 37/200\n",
      "175/175 [==============================] - 0s - loss: 2.5612 - val_loss: 2.2235\n",
      "Epoch 38/200\n",
      "175/175 [==============================] - 0s - loss: 2.4725 - val_loss: 2.5660\n",
      "Epoch 39/200\n",
      "175/175 [==============================] - 0s - loss: 2.4666 - val_loss: 2.7803\n",
      "Epoch 40/200\n",
      "175/175 [==============================] - 0s - loss: 2.4830 - val_loss: 2.5509\n",
      "Epoch 41/200\n",
      "175/175 [==============================] - 0s - loss: 2.4437 - val_loss: 2.3998\n",
      "Epoch 42/200\n",
      "175/175 [==============================] - 0s - loss: 2.4436 - val_loss: 2.9419\n",
      "Epoch 43/200\n",
      "175/175 [==============================] - 0s - loss: 2.4195 - val_loss: 2.4931\n",
      "Epoch 44/200\n",
      "175/175 [==============================] - 0s - loss: 2.4439 - val_loss: 2.2407\n",
      "Epoch 45/200\n",
      "175/175 [==============================] - 0s - loss: 2.4030 - val_loss: 2.3462\n",
      "Epoch 46/200\n",
      "175/175 [==============================] - 0s - loss: 2.3792 - val_loss: 2.5241\n",
      "Epoch 47/200\n",
      "175/175 [==============================] - 0s - loss: 2.4045 - val_loss: 2.0324\n",
      "Epoch 48/200\n",
      "175/175 [==============================] - 0s - loss: 2.3717 - val_loss: 1.9748\n",
      "Epoch 49/200\n",
      "175/175 [==============================] - 0s - loss: 2.3502 - val_loss: 2.0198\n",
      "Epoch 50/200\n",
      "175/175 [==============================] - 0s - loss: 2.3457 - val_loss: 2.5522\n",
      "Epoch 51/200\n",
      "175/175 [==============================] - 0s - loss: 2.3386 - val_loss: 2.2092\n",
      "Epoch 52/200\n",
      "175/175 [==============================] - 0s - loss: 2.3377 - val_loss: 2.3127\n",
      "Epoch 53/200\n",
      "175/175 [==============================] - 0s - loss: 2.3378 - val_loss: 2.3009\n",
      "Epoch 54/200\n",
      "175/175 [==============================] - 0s - loss: 2.2784 - val_loss: 1.8008\n",
      "Epoch 55/200\n",
      "175/175 [==============================] - 0s - loss: 2.2811 - val_loss: 2.9253\n",
      "Epoch 56/200\n",
      "175/175 [==============================] - 0s - loss: 2.3013 - val_loss: 2.2124\n",
      "Epoch 57/200\n",
      "175/175 [==============================] - 0s - loss: 2.2825 - val_loss: 2.1798\n",
      "Epoch 58/200\n",
      "175/175 [==============================] - 0s - loss: 2.2685 - val_loss: 2.2860\n",
      "Epoch 59/200\n",
      "175/175 [==============================] - 0s - loss: 2.2041 - val_loss: 2.1885\n",
      "Epoch 60/200\n",
      "175/175 [==============================] - 0s - loss: 2.2944 - val_loss: 2.2254\n",
      "Epoch 61/200\n",
      "175/175 [==============================] - 0s - loss: 2.2309 - val_loss: 2.4658\n",
      "Epoch 62/200\n",
      "175/175 [==============================] - 0s - loss: 2.1806 - val_loss: 2.4222\n",
      "Epoch 63/200\n",
      "175/175 [==============================] - 0s - loss: 2.2131 - val_loss: 2.0212\n",
      "Epoch 64/200\n",
      "175/175 [==============================] - 0s - loss: 2.2520 - val_loss: 1.9758\n",
      "Epoch 65/200\n",
      "172/175 [============================>.] - ETA: 0s - loss: 2.2425\n",
      " Reduced learning rate to 0.01\n",
      "175/175 [==============================] - 0s - loss: 2.2461 - val_loss: 2.1764\n",
      "Epoch 66/200\n",
      "175/175 [==============================] - 0s - loss: 1.7557 - val_loss: 1.7310\n",
      "Epoch 67/200\n",
      "175/175 [==============================] - 0s - loss: 1.7326 - val_loss: 1.7272\n",
      "Epoch 68/200\n",
      "175/175 [==============================] - 0s - loss: 1.7450 - val_loss: 1.7056\n",
      "Epoch 69/200\n",
      "156/175 [=========================>....] - ETA: 0s - loss: 1.7536\n",
      " Reduced learning rate to 0.005\n",
      "175/175 [==============================] - 0s - loss: 1.7538 - val_loss: 1.7432\n",
      "Epoch 70/200\n",
      "172/175 [============================>.] - ETA: 0s - loss: 1.7493\n",
      " Reduced learning rate to 0.0025\n",
      "175/175 [==============================] - 0s - loss: 1.7492 - val_loss: 1.7339\n",
      "Epoch 71/200\n",
      "165/175 [===========================>..] - ETA: 0s - loss: 1.7323\n",
      " Reduced learning rate to 0.00125\n",
      "175/175 [==============================] - 0s - loss: 1.7316 - val_loss: 1.7089\n",
      "Epoch 72/200\n",
      "175/175 [==============================] - 0s - loss: 1.7283 - val_loss: 1.6807\n",
      "Epoch 73/200\n",
      "173/175 [============================>.] - ETA: 0s - loss: 1.7353\n",
      " Reduced learning rate to 0.000625\n",
      "175/175 [==============================] - 0s - loss: 1.7352 - val_loss: 1.7028\n",
      "Epoch 74/200\n",
      "172/175 [============================>.] - ETA: 0s - loss: 1.7173\n",
      " Reduced learning rate to 0.0003125\n",
      "175/175 [==============================] - 0s - loss: 1.7165 - val_loss: 1.7203\n",
      "Epoch 75/200\n",
      "156/175 [=========================>....] - ETA: 0s - loss: 1.7356\n",
      " Reduced learning rate to 0.00015625\n",
      "175/175 [==============================] - 0s - loss: 1.7286 - val_loss: 1.7187\n",
      "Epoch 76/200\n",
      "157/175 [=========================>....] - ETA: 0s - loss: 1.7307\n",
      " Reduced learning rate to 7.8125e-05\n",
      "175/175 [==============================] - 0s - loss: 1.7350 - val_loss: 1.7368\n",
      "Epoch 77/200\n",
      "172/175 [============================>.] - ETA: 0s - loss: 1.7350\n",
      " Reduced learning rate to 3.90625e-05\n",
      "175/175 [==============================] - 0s - loss: 1.7347 - val_loss: 1.7259\n",
      "Epoch 78/200\n",
      "174/175 [============================>.] - ETA: 0s - loss: 1.7445\n",
      " Reduced learning rate to 1.95312e-05\n",
      "175/175 [==============================] - 0s - loss: 1.7446 - val_loss: 1.6892\n",
      "Epoch 79/200\n",
      "155/175 [=========================>....] - ETA: 0s - loss: 1.7217\n",
      " Reduced learning rate to 9.76562e-06\n",
      "175/175 [==============================] - 0s - loss: 1.7239 - val_loss: 1.6990\n",
      "Epoch 80/200\n",
      "156/175 [=========================>....] - ETA: 0s - loss: 1.7364\n",
      " Reduced learning rate to 4.88281e-06\n",
      "175/175 [==============================] - 0s - loss: 1.7346 - val_loss: 1.6983\n",
      "Epoch 81/200\n",
      "157/175 [=========================>....] - ETA: 0s - loss: 1.7545\n",
      " Reduced learning rate to 2.44141e-06\n",
      "175/175 [==============================] - 0s - loss: 1.7511 - val_loss: 1.7019\n",
      "Epoch 82/200\n",
      "171/175 [============================>.] - ETA: 0s - loss: 1.7279\n",
      " Reduced learning rate to 1.2207e-06\n",
      "175/175 [==============================] - 0s - loss: 1.7282 - val_loss: 1.7070\n",
      "Epoch 83/200\n",
      "156/175 [=========================>....] - ETA: 0s - loss: 1.7443\n",
      " Reduced learning rate to 6.10352e-07\n",
      "175/175 [==============================] - 0s - loss: 1.7463 - val_loss: 1.7019\n",
      "Epoch 1/200\n",
      "175/175 [==============================] - 1s - loss: 36.2112 - val_loss: 4.8943\n",
      "Epoch 2/200\n",
      "175/175 [==============================] - 0s - loss: 4.0322 - val_loss: 3.5068\n",
      "Epoch 3/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 0s - loss: 3.4124 - val_loss: 3.2563\n",
      "Epoch 4/200\n",
      "175/175 [==============================] - 0s - loss: 3.4261 - val_loss: 3.5306\n",
      "Epoch 5/200\n",
      "175/175 [==============================] - 0s - loss: 3.3054 - val_loss: 3.3155\n",
      "Epoch 6/200\n",
      "175/175 [==============================] - 0s - loss: 3.3353 - val_loss: 3.7799\n",
      "Epoch 7/200\n",
      "175/175 [==============================] - 0s - loss: 3.3109 - val_loss: 3.0553\n",
      "Epoch 8/200\n",
      "175/175 [==============================] - 0s - loss: 3.2817 - val_loss: 3.0389\n",
      "Epoch 9/200\n",
      "175/175 [==============================] - 0s - loss: 3.1936 - val_loss: 3.3408\n",
      "Epoch 10/200\n",
      "175/175 [==============================] - 0s - loss: 3.1268 - val_loss: 3.0382\n",
      "Epoch 11/200\n",
      "175/175 [==============================] - 0s - loss: 3.0976 - val_loss: 2.7269\n",
      "Epoch 12/200\n",
      "175/175 [==============================] - 0s - loss: 3.0589 - val_loss: 2.7124\n",
      "Epoch 13/200\n",
      "175/175 [==============================] - 0s - loss: 3.0271 - val_loss: 2.7626\n",
      "Epoch 14/200\n",
      "175/175 [==============================] - 0s - loss: 2.9633 - val_loss: 2.4105\n",
      "Epoch 15/200\n",
      "175/175 [==============================] - 0s - loss: 3.0101 - val_loss: 2.7109\n",
      "Epoch 16/200\n",
      "175/175 [==============================] - 0s - loss: 2.9341 - val_loss: 2.9339\n",
      "Epoch 17/200\n",
      "175/175 [==============================] - 0s - loss: 2.9354 - val_loss: 2.6426\n",
      "Epoch 18/200\n",
      "175/175 [==============================] - 0s - loss: 2.8888 - val_loss: 2.8251\n",
      "Epoch 19/200\n",
      "175/175 [==============================] - 0s - loss: 2.8968 - val_loss: 3.2334\n",
      "Epoch 20/200\n",
      "175/175 [==============================] - 0s - loss: 2.8590 - val_loss: 2.8991\n",
      "Epoch 21/200\n",
      "175/175 [==============================] - 0s - loss: 2.8962 - val_loss: 3.0309\n",
      "Epoch 22/200\n",
      "175/175 [==============================] - 0s - loss: 2.8042 - val_loss: 2.8369\n",
      "Epoch 23/200\n",
      "175/175 [==============================] - 0s - loss: 2.7488 - val_loss: 2.6481\n",
      "Epoch 24/200\n",
      "175/175 [==============================] - 0s - loss: 2.7685 - val_loss: 3.0787\n",
      "Epoch 25/200\n",
      "155/175 [=========================>....] - ETA: 0s - loss: 2.7351\n",
      " Reduced learning rate to 0.01\n",
      "175/175 [==============================] - 0s - loss: 2.7200 - val_loss: 2.8633\n",
      "Epoch 26/200\n",
      "175/175 [==============================] - 0s - loss: 1.8715 - val_loss: 1.7939\n",
      "Epoch 27/200\n",
      "175/175 [==============================] - 0s - loss: 1.8659 - val_loss: 1.7889\n",
      "Epoch 28/200\n",
      "175/175 [==============================] - 0s - loss: 1.8635 - val_loss: 1.7834\n",
      "Epoch 29/200\n",
      "167/175 [===========================>..] - ETA: 0s - loss: 1.8645\n",
      " Reduced learning rate to 0.005\n",
      "175/175 [==============================] - 0s - loss: 1.8618 - val_loss: 1.8115\n",
      "Epoch 30/200\n",
      "157/175 [=========================>....] - ETA: 0s - loss: 1.8236\n",
      " Reduced learning rate to 0.0025\n",
      "175/175 [==============================] - 0s - loss: 1.8211 - val_loss: 1.7845\n",
      "Epoch 31/200\n",
      "172/175 [============================>.] - ETA: 0s - loss: 1.8232\n",
      " Reduced learning rate to 0.00125\n",
      "175/175 [==============================] - 0s - loss: 1.8235 - val_loss: 1.7861\n",
      "Epoch 32/200\n",
      "175/175 [==============================] - 0s - loss: 1.7971 - val_loss: 1.7567\n",
      "Epoch 33/200\n",
      "171/175 [============================>.] - ETA: 0s - loss: 1.8100\n",
      " Reduced learning rate to 0.000625\n",
      "175/175 [==============================] - 0s - loss: 1.8080 - val_loss: 1.7845\n",
      "Epoch 34/200\n",
      "155/175 [=========================>....] - ETA: 0s - loss: 1.8346\n",
      " Reduced learning rate to 0.0003125\n",
      "175/175 [==============================] - 0s - loss: 1.8328 - val_loss: 1.7675\n",
      "Epoch 35/200\n",
      "170/175 [============================>.] - ETA: 0s - loss: 1.7976\n",
      " Reduced learning rate to 0.00015625\n",
      "175/175 [==============================] - 0s - loss: 1.7961 - val_loss: 1.7707\n",
      "Epoch 36/200\n",
      "173/175 [============================>.] - ETA: 0s - loss: 1.8026\n",
      " Reduced learning rate to 7.8125e-05\n",
      "175/175 [==============================] - 0s - loss: 1.8043 - val_loss: 1.7912\n",
      "Epoch 37/200\n",
      "175/175 [==============================] - 0s - loss: 1.8428 - val_loss: 1.7554\n",
      "Epoch 38/200\n",
      "175/175 [==============================] - 0s - loss: 1.7886 - val_loss: 1.7493\n",
      "Epoch 39/200\n",
      "175/175 [==============================] - 0s - loss: 1.7957 - val_loss: 1.7440\n",
      "Epoch 40/200\n",
      "173/175 [============================>.] - ETA: 0s - loss: 1.8374\n",
      " Reduced learning rate to 3.90625e-05\n",
      "175/175 [==============================] - 0s - loss: 1.8435 - val_loss: 1.7783\n",
      "Epoch 41/200\n",
      "169/175 [===========================>..] - ETA: 0s - loss: 1.7876\n",
      " Reduced learning rate to 1.95312e-05\n",
      "175/175 [==============================] - 0s - loss: 1.7906 - val_loss: 1.7678\n",
      "Epoch 42/200\n",
      "175/175 [==============================] - 0s - loss: 1.7873 - val_loss: 1.7319\n",
      "Epoch 43/200\n",
      "174/175 [============================>.] - ETA: 0s - loss: 1.8132\n",
      " Reduced learning rate to 9.76562e-06\n",
      "175/175 [==============================] - 0s - loss: 1.8130 - val_loss: 1.7505\n",
      "Epoch 44/200\n",
      "160/175 [==========================>...] - ETA: 0s - loss: 1.7909\n",
      " Reduced learning rate to 4.88281e-06\n",
      "175/175 [==============================] - 0s - loss: 1.7971 - val_loss: 1.7950\n",
      "Epoch 45/200\n",
      "158/175 [==========================>...] - ETA: 0s - loss: 1.8209\n",
      " Reduced learning rate to 2.44141e-06\n",
      "175/175 [==============================] - 0s - loss: 1.8184 - val_loss: 1.7641\n",
      "Epoch 46/200\n",
      "171/175 [============================>.] - ETA: 0s - loss: 1.8078\n",
      " Reduced learning rate to 1.2207e-06\n",
      "175/175 [==============================] - 0s - loss: 1.8064 - val_loss: 1.7438\n",
      "Epoch 47/200\n",
      "173/175 [============================>.] - ETA: 0s - loss: 1.8226\n",
      " Reduced learning rate to 6.10352e-07\n",
      "175/175 [==============================] - 0s - loss: 1.8218 - val_loss: 1.7741\n",
      "Epoch 1/200\n",
      "175/175 [==============================] - 1s - loss: 29.8484 - val_loss: 4.4903\n",
      "Epoch 2/200\n",
      "175/175 [==============================] - 0s - loss: 3.8740 - val_loss: 3.6787\n",
      "Epoch 3/200\n",
      "175/175 [==============================] - 0s - loss: 3.3825 - val_loss: 3.5463\n",
      "Epoch 4/200\n",
      "175/175 [==============================] - 0s - loss: 3.3640 - val_loss: 3.3661\n",
      "Epoch 5/200\n",
      "175/175 [==============================] - 0s - loss: 3.3218 - val_loss: 3.3715\n",
      "Epoch 6/200\n",
      "175/175 [==============================] - 0s - loss: 3.3874 - val_loss: 3.1235\n",
      "Epoch 7/200\n",
      "175/175 [==============================] - 0s - loss: 3.2517 - val_loss: 2.7251ss\n",
      "Epoch 8/200\n",
      "175/175 [==============================] - 0s - loss: 3.2591 - val_loss: 3.2344\n",
      "Epoch 9/200\n",
      "175/175 [==============================] - 0s - loss: 3.1714 - val_loss: 3.0053\n",
      "Epoch 10/200\n",
      "175/175 [==============================] - 0s - loss: 3.1048 - val_loss: 2.5985\n",
      "Epoch 11/200\n",
      "175/175 [==============================] - 0s - loss: 3.0651 - val_loss: 2.9692\n",
      "Epoch 12/200\n",
      "175/175 [==============================] - 0s - loss: 3.0238 - val_loss: 2.6283\n",
      "Epoch 13/200\n",
      "175/175 [==============================] - 0s - loss: 2.9889 - val_loss: 3.4511\n",
      "Epoch 14/200\n",
      "175/175 [==============================] - 0s - loss: 2.9853 - val_loss: 2.9073\n",
      "Epoch 15/200\n",
      "175/175 [==============================] - 0s - loss: 2.9951 - val_loss: 3.6774\n",
      "Epoch 16/200\n",
      "175/175 [==============================] - 0s - loss: 2.9188 - val_loss: 3.1426\n",
      "Epoch 17/200\n",
      "175/175 [==============================] - 0s - loss: 2.8949 - val_loss: 2.4336\n",
      "Epoch 18/200\n",
      "175/175 [==============================] - 0s - loss: 2.8798 - val_loss: 2.8936\n",
      "Epoch 19/200\n",
      "175/175 [==============================] - 0s - loss: 2.8743 - val_loss: 2.6738\n",
      "Epoch 20/200\n",
      "175/175 [==============================] - 0s - loss: 2.8113 - val_loss: 2.8364\n",
      "Epoch 21/200\n",
      "175/175 [==============================] - 0s - loss: 2.7876 - val_loss: 2.3385\n",
      "Epoch 22/200\n",
      "175/175 [==============================] - 0s - loss: 2.7793 - val_loss: 2.7583\n",
      "Epoch 23/200\n",
      "175/175 [==============================] - 0s - loss: 2.7137 - val_loss: 2.7972\n",
      "Epoch 24/200\n",
      "175/175 [==============================] - 0s - loss: 2.7161 - val_loss: 3.3268\n",
      "Epoch 25/200\n",
      "175/175 [==============================] - 0s - loss: 2.6842 - val_loss: 2.9850\n",
      "Epoch 26/200\n",
      "175/175 [==============================] - 0s - loss: 2.6998 - val_loss: 2.3379\n",
      "Epoch 27/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 0s - loss: 2.6527 - val_loss: 2.3331\n",
      "Epoch 28/200\n",
      "175/175 [==============================] - 0s - loss: 2.6638 - val_loss: 2.2077\n",
      "Epoch 29/200\n",
      "175/175 [==============================] - 0s - loss: 2.6309 - val_loss: 2.2466\n",
      "Epoch 30/200\n",
      "175/175 [==============================] - 0s - loss: 2.6245 - val_loss: 2.3024\n",
      "Epoch 31/200\n",
      "175/175 [==============================] - 0s - loss: 2.5916 - val_loss: 2.2368\n",
      "Epoch 32/200\n",
      "175/175 [==============================] - 0s - loss: 2.5985 - val_loss: 2.9585\n",
      "Epoch 33/200\n",
      "175/175 [==============================] - 0s - loss: 2.6038 - val_loss: 2.2931\n",
      "Epoch 34/200\n",
      "175/175 [==============================] - 0s - loss: 2.5627 - val_loss: 2.7951\n",
      "Epoch 35/200\n",
      "175/175 [==============================] - 0s - loss: 2.5266 - val_loss: 2.4648\n",
      "Epoch 36/200\n",
      "175/175 [==============================] - 0s - loss: 2.4773 - val_loss: 2.6523\n",
      "Epoch 37/200\n",
      "175/175 [==============================] - 0s - loss: 2.5304 - val_loss: 2.6073\n",
      "Epoch 38/200\n",
      "175/175 [==============================] - 0s - loss: 2.5101 - val_loss: 2.9004\n",
      "Epoch 39/200\n",
      "162/175 [==========================>...] - ETA: 0s - loss: 2.4634\n",
      " Reduced learning rate to 0.01\n",
      "175/175 [==============================] - 0s - loss: 2.4695 - val_loss: 2.3002\n",
      "Epoch 40/200\n",
      "175/175 [==============================] - 0s - loss: 1.7981 - val_loss: 1.7356\n",
      "Epoch 41/200\n",
      "158/175 [==========================>...] - ETA: 0s - loss: 1.7754\n",
      " Reduced learning rate to 0.005\n",
      "175/175 [==============================] - 0s - loss: 1.7761 - val_loss: 1.7526\n",
      "Epoch 42/200\n",
      "159/175 [==========================>...] - ETA: 0s - loss: 1.7626\n",
      " Reduced learning rate to 0.0025\n",
      "175/175 [==============================] - 0s - loss: 1.7655 - val_loss: 1.7674\n",
      "Epoch 43/200\n",
      "175/175 [==============================] - 0s - loss: 1.7675 - val_loss: 1.7063\n",
      "Epoch 44/200\n",
      "158/175 [==========================>...] - ETA: 0s - loss: 1.7667\n",
      " Reduced learning rate to 0.00125\n",
      "175/175 [==============================] - 0s - loss: 1.7747 - val_loss: 1.7659\n",
      "Epoch 45/200\n",
      "174/175 [============================>.] - ETA: 0s - loss: 1.7654\n",
      " Reduced learning rate to 0.000625\n",
      "175/175 [==============================] - 0s - loss: 1.7642 - val_loss: 1.7342\n",
      "Epoch 46/200\n",
      "156/175 [=========================>....] - ETA: 0s - loss: 1.7907\n",
      " Reduced learning rate to 0.0003125\n",
      "175/175 [==============================] - 0s - loss: 1.7866 - val_loss: 1.7454\n",
      "Epoch 47/200\n",
      "171/175 [============================>.] - ETA: 0s - loss: 1.7913\n",
      " Reduced learning rate to 0.00015625\n",
      "175/175 [==============================] - 0s - loss: 1.7898 - val_loss: 1.7278\n",
      "Epoch 48/200\n",
      "171/175 [============================>.] - ETA: 0s - loss: 1.7609\n",
      " Reduced learning rate to 7.8125e-05\n",
      "175/175 [==============================] - 0s - loss: 1.7621 - val_loss: 1.7402\n",
      "Epoch 49/200\n",
      "168/175 [===========================>..] - ETA: 0s - loss: 1.7418\n",
      " Reduced learning rate to 3.90625e-05\n",
      "175/175 [==============================] - 0s - loss: 1.7403 - val_loss: 1.7192\n",
      "Epoch 50/200\n",
      "174/175 [============================>.] - ETA: 0s - loss: 1.7545\n",
      " Reduced learning rate to 1.95312e-05\n",
      "175/175 [==============================] - 0s - loss: 1.7548 - val_loss: 1.7614\n",
      "Epoch 51/200\n",
      "156/175 [=========================>....] - ETA: 0s - loss: 1.8066\n",
      " Reduced learning rate to 9.76562e-06\n",
      "175/175 [==============================] - 0s - loss: 1.8010 - val_loss: 1.7420\n",
      "Epoch 52/200\n",
      "174/175 [============================>.] - ETA: 0s - loss: 1.7656\n",
      " Reduced learning rate to 4.88281e-06\n",
      "175/175 [==============================] - 0s - loss: 1.7652 - val_loss: 1.7119\n",
      "Epoch 53/200\n",
      "172/175 [============================>.] - ETA: 0s - loss: 1.7738\n",
      " Reduced learning rate to 2.44141e-06\n",
      "175/175 [==============================] - 0s - loss: 1.7728 - val_loss: 1.7549\n",
      "Epoch 54/200\n",
      "157/175 [=========================>....] - ETA: 0s - loss: 1.7760\n",
      " Reduced learning rate to 1.2207e-06\n",
      "175/175 [==============================] - 0s - loss: 1.7782 - val_loss: 1.7226\n",
      "Epoch 55/200\n",
      "159/175 [==========================>...] - ETA: 0s - loss: 1.7534\n",
      " Reduced learning rate to 6.10352e-07\n",
      "175/175 [==============================] - 0s - loss: 1.7557 - val_loss: 1.7663\n",
      "Epoch 1/200\n",
      "175/175 [==============================] - 1s - loss: 32.9576 - val_loss: 4.4190\n",
      "Epoch 2/200\n",
      "175/175 [==============================] - 0s - loss: 3.7750 - val_loss: 3.3335\n",
      "Epoch 3/200\n",
      "175/175 [==============================] - 0s - loss: 3.3243 - val_loss: 3.0762\n",
      "Epoch 4/200\n",
      "175/175 [==============================] - 0s - loss: 3.3572 - val_loss: 3.2398\n",
      "Epoch 5/200\n",
      "175/175 [==============================] - 0s - loss: 3.3599 - val_loss: 2.9947\n",
      "Epoch 6/200\n",
      "175/175 [==============================] - 0s - loss: 3.3269 - val_loss: 2.9957\n",
      "Epoch 7/200\n",
      "175/175 [==============================] - 0s - loss: 3.2254 - val_loss: 2.8063\n",
      "Epoch 8/200\n",
      "175/175 [==============================] - 0s - loss: 3.1657 - val_loss: 3.1694\n",
      "Epoch 9/200\n",
      "175/175 [==============================] - 0s - loss: 3.1852 - val_loss: 3.2065\n",
      "Epoch 10/200\n",
      "175/175 [==============================] - 0s - loss: 3.1005 - val_loss: 2.9828\n",
      "Epoch 11/200\n",
      "175/175 [==============================] - 0s - loss: 3.0887 - val_loss: 2.4462\n",
      "Epoch 12/200\n",
      "175/175 [==============================] - 0s - loss: 3.0224 - val_loss: 3.0997\n",
      "Epoch 13/200\n",
      "175/175 [==============================] - 0s - loss: 3.0500 - val_loss: 3.3024\n",
      "Epoch 14/200\n",
      "175/175 [==============================] - 0s - loss: 3.0210 - val_loss: 3.1898\n",
      "Epoch 15/200\n",
      "175/175 [==============================] - 0s - loss: 2.9957 - val_loss: 2.6836\n",
      "Epoch 16/200\n",
      "175/175 [==============================] - 0s - loss: 2.9299 - val_loss: 2.5806\n",
      "Epoch 17/200\n",
      "175/175 [==============================] - 0s - loss: 2.8728 - val_loss: 2.6421\n",
      "Epoch 18/200\n",
      "175/175 [==============================] - 0s - loss: 2.9110 - val_loss: 3.3603\n",
      "Epoch 19/200\n",
      "175/175 [==============================] - 0s - loss: 2.8417 - val_loss: 2.7808\n",
      "Epoch 20/200\n",
      "175/175 [==============================] - 0s - loss: 2.8683 - val_loss: 2.3242\n",
      "Epoch 21/200\n",
      "175/175 [==============================] - 0s - loss: 2.8224 - val_loss: 2.5389\n",
      "Epoch 22/200\n",
      "175/175 [==============================] - 0s - loss: 2.7524 - val_loss: 3.0030\n",
      "Epoch 23/200\n",
      "175/175 [==============================] - 0s - loss: 2.7424 - val_loss: 2.6523\n",
      "Epoch 24/200\n",
      "175/175 [==============================] - 0s - loss: 2.7296 - val_loss: 2.7399\n",
      "Epoch 25/200\n",
      "175/175 [==============================] - 0s - loss: 2.7294 - val_loss: 2.5797\n",
      "Epoch 26/200\n",
      "175/175 [==============================] - 0s - loss: 2.7243 - val_loss: 2.5459\n",
      "Epoch 27/200\n",
      "175/175 [==============================] - 0s - loss: 2.6983 - val_loss: 2.9277\n",
      "Epoch 28/200\n",
      "175/175 [==============================] - 0s - loss: 2.6589 - val_loss: 2.6758\n",
      "Epoch 29/200\n",
      "175/175 [==============================] - 0s - loss: 2.6586 - val_loss: 2.5885\n",
      "Epoch 30/200\n",
      "175/175 [==============================] - 0s - loss: 2.6171 - val_loss: 2.9132\n",
      "Epoch 31/200\n",
      "159/175 [==========================>...] - ETA: 0s - loss: 2.5872\n",
      " Reduced learning rate to 0.01\n",
      "175/175 [==============================] - 1s - loss: 2.5832 - val_loss: 2.5441\n",
      "Epoch 32/200\n",
      "175/175 [==============================] - 0s - loss: 1.8062 - val_loss: 1.7507\n",
      "Epoch 33/200\n",
      "169/175 [===========================>..] - ETA: 0s - loss: 1.8275\n",
      " Reduced learning rate to 0.005\n",
      "175/175 [==============================] - 0s - loss: 1.8261 - val_loss: 1.7601\n",
      "Epoch 34/200\n",
      "173/175 [============================>.] - ETA: 0s - loss: 1.8001\n",
      " Reduced learning rate to 0.0025\n",
      "175/175 [==============================] - 0s - loss: 1.8006 - val_loss: 1.7622\n",
      "Epoch 35/200\n",
      "175/175 [==============================] - 0s - loss: 1.8198 - val_loss: 1.7317\n",
      "Epoch 36/200\n",
      "167/175 [===========================>..] - ETA: 0s - loss: 1.7627\n",
      " Reduced learning rate to 0.00125\n",
      "175/175 [==============================] - 0s - loss: 1.7603 - val_loss: 1.7687\n",
      "Epoch 37/200\n",
      "160/175 [==========================>...] - ETA: 0s - loss: 1.7919\n",
      " Reduced learning rate to 0.000625\n",
      "175/175 [==============================] - 0s - loss: 1.7923 - val_loss: 1.7597\n",
      "Epoch 38/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 0s - loss: 1.7968 - val_loss: 1.7309\n",
      "Epoch 39/200\n",
      "173/175 [============================>.] - ETA: 0s - loss: 1.8028\n",
      " Reduced learning rate to 0.0003125\n",
      "175/175 [==============================] - 0s - loss: 1.8013 - val_loss: 1.7427\n",
      "Epoch 40/200\n",
      "162/175 [==========================>...] - ETA: 0s - loss: 1.7888\n",
      " Reduced learning rate to 0.00015625\n",
      "175/175 [==============================] - 0s - loss: 1.8048 - val_loss: 1.7776\n",
      "Epoch 41/200\n",
      "170/175 [============================>.] - ETA: 0s - loss: 1.7637\n",
      " Reduced learning rate to 7.8125e-05\n",
      "175/175 [==============================] - 0s - loss: 1.7631 - val_loss: 1.7316\n",
      "Epoch 42/200\n",
      "161/175 [==========================>...] - ETA: 0s - loss: 1.8072\n",
      " Reduced learning rate to 3.90625e-05\n",
      "175/175 [==============================] - 0s - loss: 1.8138 - val_loss: 1.7459\n",
      "Epoch 43/200\n",
      "174/175 [============================>.] - ETA: 0s - loss: 1.7921\n",
      " Reduced learning rate to 1.95312e-05\n",
      "175/175 [==============================] - 0s - loss: 1.7921 - val_loss: 1.7502\n",
      "Epoch 44/200\n",
      "174/175 [============================>.] - ETA: 0s - loss: 1.8027\n",
      " Reduced learning rate to 9.76562e-06\n",
      "175/175 [==============================] - 0s - loss: 1.8028 - val_loss: 1.7862\n",
      "Epoch 45/200\n",
      "172/175 [============================>.] - ETA: 0s - loss: 1.7761\n",
      " Reduced learning rate to 4.88281e-06\n",
      "175/175 [==============================] - 0s - loss: 1.7744 - val_loss: 1.7363\n",
      "Epoch 46/200\n",
      "164/175 [===========================>..] - ETA: 0s - loss: 1.8054\n",
      " Reduced learning rate to 2.44141e-06\n",
      "175/175 [==============================] - 0s - loss: 1.8031 - val_loss: 1.7551\n",
      "Epoch 47/200\n",
      "172/175 [============================>.] - ETA: 0s - loss: 1.7853\n",
      " Reduced learning rate to 1.2207e-06\n",
      "175/175 [==============================] - 0s - loss: 1.7859 - val_loss: 1.7346\n",
      "Epoch 48/200\n",
      "163/175 [==========================>...] - ETA: 0s - loss: 1.7733\n",
      " Reduced learning rate to 6.10352e-07\n",
      "175/175 [==============================] - 0s - loss: 1.7778 - val_loss: 1.7314\n",
      "Epoch 1/200\n",
      "175/175 [==============================] - 1s - loss: 34.1951 - val_loss: 4.4688\n",
      "Epoch 2/200\n",
      "175/175 [==============================] - 0s - loss: 3.7763 - val_loss: 3.4663\n",
      "Epoch 3/200\n",
      "175/175 [==============================] - 0s - loss: 3.3557 - val_loss: 3.7126\n",
      "Epoch 4/200\n",
      "175/175 [==============================] - 0s - loss: 3.3411 - val_loss: 2.9009\n",
      "Epoch 5/200\n",
      "175/175 [==============================] - 0s - loss: 3.3396 - val_loss: 3.0990\n",
      "Epoch 6/200\n",
      "175/175 [==============================] - 0s - loss: 3.3005 - val_loss: 3.8314\n",
      "Epoch 7/200\n",
      "175/175 [==============================] - 0s - loss: 3.2023 - val_loss: 3.0361\n",
      "Epoch 8/200\n",
      "175/175 [==============================] - 0s - loss: 3.1665 - val_loss: 3.1806\n",
      "Epoch 9/200\n",
      "175/175 [==============================] - 0s - loss: 3.1463 - val_loss: 2.6400\n",
      "Epoch 10/200\n",
      "175/175 [==============================] - 0s - loss: 3.1507 - val_loss: 2.7910\n",
      "Epoch 11/200\n",
      "175/175 [==============================] - 0s - loss: 3.0440 - val_loss: 3.4154\n",
      "Epoch 12/200\n",
      "175/175 [==============================] - 0s - loss: 3.0841 - val_loss: 2.8146\n",
      "Epoch 13/200\n",
      "175/175 [==============================] - 0s - loss: 3.0319 - val_loss: 3.4437\n",
      "Epoch 14/200\n",
      "175/175 [==============================] - 0s - loss: 2.9779 - val_loss: 2.8423\n",
      "Epoch 15/200\n",
      "175/175 [==============================] - 0s - loss: 2.9472 - val_loss: 3.4523\n",
      "Epoch 16/200\n",
      "175/175 [==============================] - 0s - loss: 2.9162 - val_loss: 2.6831\n",
      "Epoch 17/200\n",
      "175/175 [==============================] - 0s - loss: 2.8813 - val_loss: 3.3574\n",
      "Epoch 18/200\n",
      "175/175 [==============================] - 0s - loss: 2.8478 - val_loss: 2.6059\n",
      "Epoch 19/200\n",
      "175/175 [==============================] - 0s - loss: 2.8474 - val_loss: 2.4496\n",
      "Epoch 20/200\n",
      "175/175 [==============================] - 0s - loss: 2.8502 - val_loss: 2.6445\n",
      "Epoch 21/200\n",
      "175/175 [==============================] - 0s - loss: 2.8067 - val_loss: 2.3965\n",
      "Epoch 22/200\n",
      "175/175 [==============================] - 0s - loss: 2.7816 - val_loss: 2.5781\n",
      "Epoch 23/200\n",
      "175/175 [==============================] - 0s - loss: 2.7928 - val_loss: 2.6739\n",
      "Epoch 24/200\n",
      "175/175 [==============================] - 0s - loss: 2.7659 - val_loss: 2.4390\n",
      "Epoch 25/200\n",
      "175/175 [==============================] - 0s - loss: 2.7308 - val_loss: 2.2681\n",
      "Epoch 26/200\n",
      "175/175 [==============================] - 0s - loss: 2.6569 - val_loss: 2.4787\n",
      "Epoch 27/200\n",
      "175/175 [==============================] - 0s - loss: 2.6687 - val_loss: 2.5375\n",
      "Epoch 28/200\n",
      "175/175 [==============================] - 0s - loss: 2.6875 - val_loss: 2.2108\n",
      "Epoch 29/200\n",
      "175/175 [==============================] - 0s - loss: 2.6187 - val_loss: 2.1901\n",
      "Epoch 30/200\n",
      "175/175 [==============================] - 0s - loss: 2.6060 - val_loss: 2.7262\n",
      "Epoch 31/200\n",
      "175/175 [==============================] - 0s - loss: 2.6079 - val_loss: 2.7081\n",
      "Epoch 32/200\n",
      "175/175 [==============================] - 0s - loss: 2.5489 - val_loss: 2.1335\n",
      "Epoch 33/200\n",
      "175/175 [==============================] - 0s - loss: 2.5628 - val_loss: 2.1865\n",
      "Epoch 34/200\n",
      "175/175 [==============================] - 0s - loss: 2.5450 - val_loss: 2.7819\n",
      "Epoch 35/200\n",
      "175/175 [==============================] - 0s - loss: 2.5438 - val_loss: 2.6523\n",
      "Epoch 36/200\n",
      "175/175 [==============================] - 0s - loss: 2.5387 - val_loss: 1.9585\n",
      "Epoch 37/200\n",
      "175/175 [==============================] - 0s - loss: 2.5429 - val_loss: 2.3490\n",
      "Epoch 38/200\n",
      "175/175 [==============================] - 0s - loss: 2.4710 - val_loss: 2.3247\n",
      "Epoch 39/200\n",
      "175/175 [==============================] - 0s - loss: 2.4478 - val_loss: 2.2488\n",
      "Epoch 40/200\n",
      "175/175 [==============================] - 0s - loss: 2.4630 - val_loss: 2.3668\n",
      "Epoch 41/200\n",
      "175/175 [==============================] - 0s - loss: 2.4361 - val_loss: 2.2349\n",
      "Epoch 42/200\n",
      "175/175 [==============================] - 1s - loss: 2.3999 - val_loss: 2.5000\n",
      "Epoch 43/200\n",
      "175/175 [==============================] - 0s - loss: 2.4033 - val_loss: 2.2283\n",
      "Epoch 44/200\n",
      "175/175 [==============================] - 0s - loss: 2.4292 - val_loss: 2.3506\n",
      "Epoch 45/200\n",
      "175/175 [==============================] - 0s - loss: 2.4102 - val_loss: 2.2884\n",
      "Epoch 46/200\n",
      "175/175 [==============================] - 0s - loss: 2.3867 - val_loss: 1.8730\n",
      "Epoch 47/200\n",
      "175/175 [==============================] - 0s - loss: 2.3639 - val_loss: 2.9127\n",
      "Epoch 48/200\n",
      "175/175 [==============================] - 0s - loss: 2.3627 - val_loss: 2.3817\n",
      "Epoch 49/200\n",
      "175/175 [==============================] - 0s - loss: 2.3509 - val_loss: 2.4233\n",
      "Epoch 50/200\n",
      "175/175 [==============================] - 1s - loss: 2.3170 - val_loss: 2.3315\n",
      "Epoch 51/200\n",
      "175/175 [==============================] - 0s - loss: 2.3266 - val_loss: 2.1836\n",
      "Epoch 52/200\n",
      "175/175 [==============================] - 0s - loss: 2.3591 - val_loss: 2.6027\n",
      "Epoch 53/200\n",
      "175/175 [==============================] - 0s - loss: 2.2658 - val_loss: 2.3953\n",
      "Epoch 54/200\n",
      "175/175 [==============================] - 0s - loss: 2.3038 - val_loss: 2.3592\n",
      "Epoch 55/200\n",
      "175/175 [==============================] - 0s - loss: 2.2562 - val_loss: 1.8652\n",
      "Epoch 56/200\n",
      "175/175 [==============================] - 0s - loss: 2.2678 - val_loss: 2.3045\n",
      "Epoch 57/200\n",
      "175/175 [==============================] - 0s - loss: 2.2478 - val_loss: 2.5595\n",
      "Epoch 58/200\n",
      "175/175 [==============================] - 1s - loss: 2.2772 - val_loss: 2.5733\n",
      "Epoch 59/200\n",
      "175/175 [==============================] - 1s - loss: 2.2133 - val_loss: 2.3390\n",
      "Epoch 60/200\n",
      "175/175 [==============================] - 6s - loss: 2.2643 - val_loss: 2.3120\n",
      "Epoch 61/200\n",
      "175/175 [==============================] - 1s - loss: 2.2213 - val_loss: 2.1452\n",
      "Epoch 62/200\n",
      "175/175 [==============================] - 0s - loss: 2.1966 - val_loss: 2.2936\n",
      "Epoch 63/200\n",
      "175/175 [==============================] - 0s - loss: 2.2114 - val_loss: 2.1324\n",
      "Epoch 64/200\n",
      "175/175 [==============================] - 0s - loss: 2.1874 - val_loss: 2.1698\n",
      "Epoch 65/200\n",
      "175/175 [==============================] - 0s - loss: 2.1908 - val_loss: 2.0254\n",
      "Epoch 66/200\n",
      "164/175 [===========================>..] - ETA: 0s - loss: 2.2012\n",
      " Reduced learning rate to 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 2s - loss: 2.1978 - val_loss: 2.0738\n",
      "Epoch 67/200\n",
      "175/175 [==============================] - 0s - loss: 1.7531 - val_loss: 1.7217\n",
      "Epoch 68/200\n",
      "164/175 [===========================>..] - ETA: 0s - loss: 1.7442\n",
      " Reduced learning rate to 0.005\n",
      "175/175 [==============================] - 0s - loss: 1.7430 - val_loss: 1.7391\n",
      "Epoch 69/200\n",
      "175/175 [==============================] - 1s - loss: 1.7424 - val_loss: 1.7146\n",
      "Epoch 70/200\n",
      "157/175 [=========================>....] - ETA: 0s - loss: 1.7423\n",
      " Reduced learning rate to 0.0025\n",
      "175/175 [==============================] - 3s - loss: 1.7376 - val_loss: 1.7189\n",
      "Epoch 71/200\n",
      "175/175 [==============================] - 0s - loss: 1.7704 - val_loss: 1.6849\n",
      "Epoch 72/200\n",
      "168/175 [===========================>..] - ETA: 0s - loss: 1.7342\n",
      " Reduced learning rate to 0.00125\n",
      "175/175 [==============================] - 1s - loss: 1.7362 - val_loss: 1.7134\n",
      "Epoch 73/200\n",
      "164/175 [===========================>..] - ETA: 0s - loss: 1.7424\n",
      " Reduced learning rate to 0.000625\n",
      "175/175 [==============================] - 0s - loss: 1.7414 - val_loss: 1.7214\n",
      "Epoch 74/200\n",
      "173/175 [============================>.] - ETA: 0s - loss: 1.7426\n",
      " Reduced learning rate to 0.0003125\n",
      "175/175 [==============================] - 0s - loss: 1.7424 - val_loss: 1.7227\n",
      "Epoch 75/200\n",
      "159/175 [==========================>...] - ETA: 0s - loss: 1.7343\n",
      " Reduced learning rate to 0.00015625\n",
      "175/175 [==============================] - 0s - loss: 1.7350 - val_loss: 1.7053\n",
      "Epoch 76/200\n",
      "159/175 [==========================>...] - ETA: 0s - loss: 1.7393\n",
      " Reduced learning rate to 7.8125e-05\n",
      "175/175 [==============================] - 0s - loss: 1.7402 - val_loss: 1.7100\n",
      "Epoch 77/200\n",
      "162/175 [==========================>...] - ETA: 0s - loss: 1.7287\n",
      " Reduced learning rate to 3.90625e-05\n",
      "175/175 [==============================] - 0s - loss: 1.7313 - val_loss: 1.7192\n",
      "Epoch 78/200\n",
      "166/175 [===========================>..] - ETA: 0s - loss: 1.7412\n",
      " Reduced learning rate to 1.95312e-05\n",
      "175/175 [==============================] - 2s - loss: 1.7388 - val_loss: 1.7199\n",
      "Epoch 79/200\n",
      "160/175 [==========================>...] - ETA: 0s - loss: 1.7193\n",
      " Reduced learning rate to 9.76562e-06\n",
      "175/175 [==============================] - 0s - loss: 1.7226 - val_loss: 1.6878\n",
      "Epoch 80/200\n",
      "168/175 [===========================>..] - ETA: 0s - loss: 1.7291\n",
      " Reduced learning rate to 4.88281e-06\n",
      "175/175 [==============================] - 0s - loss: 1.7291 - val_loss: 1.7410\n",
      "Epoch 81/200\n",
      "173/175 [============================>.] - ETA: 0s - loss: 1.7387\n",
      " Reduced learning rate to 2.44141e-06\n",
      "175/175 [==============================] - 0s - loss: 1.7377 - val_loss: 1.7280\n",
      "Epoch 82/200\n",
      "168/175 [===========================>..] - ETA: 0s - loss: 1.7335\n",
      " Reduced learning rate to 1.2207e-06\n",
      "175/175 [==============================] - 0s - loss: 1.7351 - val_loss: 1.7230\n",
      "Epoch 83/200\n",
      "171/175 [============================>.] - ETA: 0s - loss: 1.7709\n",
      " Reduced learning rate to 6.10352e-07\n",
      "175/175 [==============================] - 0s - loss: 1.7684 - val_loss: 1.7276\n",
      "Epoch 1/200\n",
      "878/878 [==============================] - 23s - loss: 8.9315 - val_loss: 2.8868\n",
      "Epoch 2/200\n",
      "878/878 [==============================] - 2s - loss: 3.2304 - val_loss: 3.0221\n",
      "Epoch 3/200\n",
      "878/878 [==============================] - 2s - loss: 3.0315 - val_loss: 2.6098\n",
      "Epoch 4/200\n",
      "878/878 [==============================] - 2s - loss: 2.8763 - val_loss: 2.8512\n",
      "Epoch 5/200\n",
      "878/878 [==============================] - 2s - loss: 2.7929 - val_loss: 2.9062\n",
      "Epoch 6/200\n",
      "878/878 [==============================] - 2s - loss: 2.6634 - val_loss: 2.7536\n",
      "Epoch 7/200\n",
      "878/878 [==============================] - 2s - loss: 2.5896 - val_loss: 3.0352\n",
      "Epoch 8/200\n",
      "878/878 [==============================] - 2s - loss: 2.5338 - val_loss: 2.2045\n",
      "Epoch 9/200\n",
      "878/878 [==============================] - 2s - loss: 2.4530 - val_loss: 2.2970\n",
      "Epoch 10/200\n",
      "878/878 [==============================] - 2s - loss: 2.3964 - val_loss: 2.6305\n",
      "Epoch 11/200\n",
      "878/878 [==============================] - 2s - loss: 2.3331 - val_loss: 2.1699\n",
      "Epoch 12/200\n",
      "878/878 [==============================] - 2s - loss: 2.2514 - val_loss: 2.3394\n",
      "Epoch 13/200\n",
      "878/878 [==============================] - 2s - loss: 2.2133 - val_loss: 2.1386\n",
      "Epoch 14/200\n",
      "878/878 [==============================] - 2s - loss: 2.1845 - val_loss: 2.4824\n",
      "Epoch 15/200\n",
      "878/878 [==============================] - 2s - loss: 2.1569 - val_loss: 2.0385\n",
      "Epoch 16/200\n",
      "878/878 [==============================] - 2s - loss: 2.1002 - val_loss: 2.2442\n",
      "Epoch 17/200\n",
      "878/878 [==============================] - 2s - loss: 2.0832 - val_loss: 2.4159\n",
      "Epoch 18/200\n",
      "878/878 [==============================] - 2s - loss: 2.0486 - val_loss: 1.9650\n",
      "Epoch 19/200\n",
      "878/878 [==============================] - 2s - loss: 2.0524 - val_loss: 1.9908\n",
      "Epoch 20/200\n",
      "878/878 [==============================] - 2s - loss: 1.9987 - val_loss: 2.0625\n",
      "Epoch 21/200\n",
      "878/878 [==============================] - 2s - loss: 1.9953 - val_loss: 2.4630\n",
      "Epoch 22/200\n",
      "878/878 [==============================] - 2s - loss: 1.9537 - val_loss: 1.9946\n",
      "Epoch 23/200\n",
      "878/878 [==============================] - 2s - loss: 1.9569 - val_loss: 1.7292\n",
      "Epoch 24/200\n",
      "878/878 [==============================] - 2s - loss: 1.9283 - val_loss: 1.7226\n",
      "Epoch 25/200\n",
      "878/878 [==============================] - 2s - loss: 1.9354 - val_loss: 2.1425\n",
      "Epoch 26/200\n",
      "878/878 [==============================] - 2s - loss: 1.9201 - val_loss: 1.9217\n",
      "Epoch 27/200\n",
      "878/878 [==============================] - 2s - loss: 1.9088 - val_loss: 2.2280\n",
      "Epoch 28/200\n",
      "878/878 [==============================] - 2s - loss: 1.8732 - val_loss: 1.8443\n",
      "Epoch 29/200\n",
      "878/878 [==============================] - 2s - loss: 1.8886 - val_loss: 2.0195\n",
      "Epoch 30/200\n",
      "878/878 [==============================] - 2s - loss: 1.8885 - val_loss: 1.7651\n",
      "Epoch 31/200\n",
      "878/878 [==============================] - 2s - loss: 1.8432 - val_loss: 2.0314\n",
      "Epoch 32/200\n",
      "878/878 [==============================] - 2s - loss: 1.8675 - val_loss: 1.9180\n",
      "Epoch 33/200\n",
      "878/878 [==============================] - 2s - loss: 1.8364 - val_loss: 1.9243\n",
      "Epoch 34/200\n",
      "878/878 [==============================] - 2s - loss: 1.8504 - val_loss: 1.8556\n",
      "Epoch 35/200\n",
      "873/878 [============================>.] - ETA: 0s - loss: 1.8414\n",
      " Reduced learning rate to 0.01\n",
      "878/878 [==============================] - 3s - loss: 1.8406 - val_loss: 1.7278\n",
      "Epoch 36/200\n",
      "878/878 [==============================] - 2s - loss: 1.7045 - val_loss: 1.7053\n",
      "Epoch 37/200\n",
      "864/878 [============================>.] - ETA: 0s - loss: 1.7031\n",
      " Reduced learning rate to 0.005\n",
      "878/878 [==============================] - 2s - loss: 1.7027 - val_loss: 1.7200\n",
      "Epoch 38/200\n",
      "878/878 [==============================] - 2s - loss: 1.6949 - val_loss: 1.7006\n",
      "Epoch 39/200\n",
      "878/878 [==============================] - 2s - loss: 1.6877 - val_loss: 1.6955\n",
      "Epoch 40/200\n",
      "866/878 [============================>.] - ETA: 0s - loss: 1.6976\n",
      " Reduced learning rate to 0.0025\n",
      "878/878 [==============================] - 2s - loss: 1.6976 - val_loss: 1.7010\n",
      "Epoch 41/200\n",
      "878/878 [==============================] - 2s - loss: 1.6887 - val_loss: 1.6946\n",
      "Epoch 42/200\n",
      "866/878 [============================>.] - ETA: 0s - loss: 1.6912\n",
      " Reduced learning rate to 0.00125\n",
      "878/878 [==============================] - 2s - loss: 1.6912 - val_loss: 1.7076\n",
      "Epoch 43/200\n",
      "873/878 [============================>.] - ETA: 0s - loss: 1.6995\n",
      " Reduced learning rate to 0.000625\n",
      "878/878 [==============================] - 2s - loss: 1.6999 - val_loss: 1.7089\n",
      "Epoch 44/200\n",
      "860/878 [============================>.] - ETA: 0s - loss: 1.7032\n",
      " Reduced learning rate to 0.0003125\n",
      "878/878 [==============================] - 2s - loss: 1.7027 - val_loss: 1.7045\n",
      "Epoch 45/200\n",
      "861/878 [============================>.] - ETA: 0s - loss: 1.6860\n",
      " Reduced learning rate to 0.00015625\n",
      "878/878 [==============================] - 2s - loss: 1.6872 - val_loss: 1.7052\n",
      "Epoch 46/200\n",
      "878/878 [==============================] - 2s - loss: 1.6850 - val_loss: 1.6894\n",
      "Epoch 47/200\n",
      "860/878 [============================>.] - ETA: 0s - loss: 1.6831\n",
      " Reduced learning rate to 7.8125e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878/878 [==============================] - 2s - loss: 1.6824 - val_loss: 1.6940\n",
      "Epoch 48/200\n",
      "862/878 [============================>.] - ETA: 0s - loss: 1.6953\n",
      " Reduced learning rate to 3.90625e-05\n",
      "878/878 [==============================] - 2s - loss: 1.6957 - val_loss: 1.7070\n",
      "Epoch 49/200\n",
      "867/878 [============================>.] - ETA: 0s - loss: 1.6891\n",
      " Reduced learning rate to 1.95312e-05\n",
      "878/878 [==============================] - 2s - loss: 1.6885 - val_loss: 1.6933\n",
      "Epoch 50/200\n",
      "864/878 [============================>.] - ETA: 0s - loss: 1.6862\n",
      " Reduced learning rate to 9.76562e-06\n",
      "878/878 [==============================] - 2s - loss: 1.6866 - val_loss: 1.7012\n",
      "Epoch 51/200\n",
      "871/878 [============================>.] - ETA: 0s - loss: 1.6891\n",
      " Reduced learning rate to 4.88281e-06\n",
      "878/878 [==============================] - 2s - loss: 1.6890 - val_loss: 1.7037\n",
      "Epoch 52/200\n",
      "866/878 [============================>.] - ETA: 0s - loss: 1.6934\n",
      " Reduced learning rate to 2.44141e-06\n",
      "878/878 [==============================] - 2s - loss: 1.6925 - val_loss: 1.7019\n",
      "Epoch 53/200\n",
      "864/878 [============================>.] - ETA: 0s - loss: 1.6933\n",
      " Reduced learning rate to 1.2207e-06\n",
      "878/878 [==============================] - 2s - loss: 1.6933 - val_loss: 1.7065\n",
      "Epoch 54/200\n",
      "860/878 [============================>.] - ETA: 0s - loss: 1.6870\n",
      " Reduced learning rate to 6.10352e-07\n",
      "878/878 [==============================] - 2s - loss: 1.6875 - val_loss: 1.7052\n",
      "Epoch 1/200\n",
      "878/878 [==============================] - 3s - loss: 9.1249 - val_loss: 3.1810\n",
      "Epoch 2/200\n",
      "878/878 [==============================] - 2s - loss: 3.2283 - val_loss: 2.8125\n",
      "Epoch 3/200\n",
      "878/878 [==============================] - 2s - loss: 3.0190 - val_loss: 3.3465\n",
      "Epoch 4/200\n",
      "878/878 [==============================] - 2s - loss: 2.8563 - val_loss: 2.7266\n",
      "Epoch 5/200\n",
      "878/878 [==============================] - 2s - loss: 2.7580 - val_loss: 2.8307\n",
      "Epoch 6/200\n",
      "878/878 [==============================] - 2s - loss: 2.6483 - val_loss: 2.6152\n",
      "Epoch 7/200\n",
      "878/878 [==============================] - 2s - loss: 2.5460 - val_loss: 2.5005\n",
      "Epoch 8/200\n",
      "878/878 [==============================] - 2s - loss: 2.5033 - val_loss: 2.7440\n",
      "Epoch 9/200\n",
      "878/878 [==============================] - 2s - loss: 2.3987 - val_loss: 2.2853\n",
      "Epoch 10/200\n",
      "878/878 [==============================] - 2s - loss: 2.3575 - val_loss: 2.1471\n",
      "Epoch 11/200\n",
      "878/878 [==============================] - 2s - loss: 2.3250 - val_loss: 2.9947\n",
      "Epoch 12/200\n",
      "878/878 [==============================] - 2s - loss: 2.2278 - val_loss: 2.3729\n",
      "Epoch 13/200\n",
      "878/878 [==============================] - 2s - loss: 2.2080 - val_loss: 2.4849\n",
      "Epoch 14/200\n",
      "878/878 [==============================] - 2s - loss: 2.1866 - val_loss: 2.4907\n",
      "Epoch 15/200\n",
      "878/878 [==============================] - 2s - loss: 2.1522 - val_loss: 1.8326\n",
      "Epoch 16/200\n",
      "878/878 [==============================] - 2s - loss: 2.1031 - val_loss: 2.0405\n",
      "Epoch 17/200\n",
      "878/878 [==============================] - 2s - loss: 2.0912 - val_loss: 2.3419\n",
      "Epoch 18/200\n",
      "878/878 [==============================] - 2s - loss: 2.0354 - val_loss: 2.4458\n",
      "Epoch 19/200\n",
      "878/878 [==============================] - 2s - loss: 2.0283 - val_loss: 1.8335\n",
      "Epoch 20/200\n",
      "878/878 [==============================] - 2s - loss: 1.9848 - val_loss: 1.8178\n",
      "Epoch 21/200\n",
      "878/878 [==============================] - 2s - loss: 1.9884 - val_loss: 2.1250\n",
      "Epoch 22/200\n",
      "878/878 [==============================] - 2s - loss: 1.9533 - val_loss: 1.7576\n",
      "Epoch 23/200\n",
      "878/878 [==============================] - 2s - loss: 1.9549 - val_loss: 1.9087\n",
      "Epoch 24/200\n",
      "878/878 [==============================] - 2s - loss: 1.9093 - val_loss: 1.7970\n",
      "Epoch 25/200\n",
      "878/878 [==============================] - 2s - loss: 1.9123 - val_loss: 1.8803\n",
      "Epoch 26/200\n",
      "878/878 [==============================] - 2s - loss: 1.9012 - val_loss: 1.7611\n",
      "Epoch 27/200\n",
      "878/878 [==============================] - 2s - loss: 1.9066 - val_loss: 1.7226\n",
      "Epoch 28/200\n",
      "878/878 [==============================] - 2s - loss: 1.8800 - val_loss: 1.9419\n",
      "Epoch 29/200\n",
      "878/878 [==============================] - 2s - loss: 1.8786 - val_loss: 1.7597\n",
      "Epoch 30/200\n",
      "878/878 [==============================] - 2s - loss: 1.8659 - val_loss: 2.2812\n",
      "Epoch 31/200\n",
      "878/878 [==============================] - 2s - loss: 1.8626 - val_loss: 1.7830\n",
      "Epoch 32/200\n",
      "878/878 [==============================] - 2s - loss: 1.8364 - val_loss: 2.1368\n",
      "Epoch 33/200\n",
      "878/878 [==============================] - 2s - loss: 1.8499 - val_loss: 2.1193\n",
      "Epoch 34/200\n",
      "878/878 [==============================] - 2s - loss: 1.8472 - val_loss: 1.8034\n",
      "Epoch 35/200\n",
      "878/878 [==============================] - 2s - loss: 1.8262 - val_loss: 1.7161\n",
      "Epoch 36/200\n",
      "878/878 [==============================] - 2s - loss: 1.8225 - val_loss: 1.8358\n",
      "Epoch 37/200\n",
      "878/878 [==============================] - 2s - loss: 1.8264 - val_loss: 2.2186\n",
      "Epoch 38/200\n",
      "878/878 [==============================] - 2s - loss: 1.8197 - val_loss: 1.7051\n",
      "Epoch 39/200\n",
      "878/878 [==============================] - 2s - loss: 1.8250 - val_loss: 1.7617\n",
      "Epoch 40/200\n",
      "878/878 [==============================] - 2s - loss: 1.8201 - val_loss: 1.7083\n",
      "Epoch 41/200\n",
      "878/878 [==============================] - 2s - loss: 1.8041 - val_loss: 1.9506\n",
      "Epoch 42/200\n",
      "878/878 [==============================] - 2s - loss: 1.7963 - val_loss: 2.0425\n",
      "Epoch 43/200\n",
      "878/878 [==============================] - 2s - loss: 1.7974 - val_loss: 2.0658\n",
      "Epoch 44/200\n",
      "878/878 [==============================] - 2s - loss: 1.7837 - val_loss: 1.7819\n",
      "Epoch 45/200\n",
      "878/878 [==============================] - 2s - loss: 1.7891 - val_loss: 1.7383\n",
      "Epoch 46/200\n",
      "878/878 [==============================] - 2s - loss: 1.7893 - val_loss: 2.1294\n",
      "Epoch 47/200\n",
      "878/878 [==============================] - 2s - loss: 1.7877 - val_loss: 1.7474\n",
      "Epoch 48/200\n",
      "878/878 [==============================] - 2s - loss: 1.7797 - val_loss: 1.7889\n",
      "Epoch 49/200\n",
      "876/878 [============================>.] - ETA: 0s - loss: 1.7670\n",
      " Reduced learning rate to 0.01\n",
      "878/878 [==============================] - 3s - loss: 1.7676 - val_loss: 1.8670\n",
      "Epoch 50/200\n",
      "878/878 [==============================] - 2s - loss: 1.6939 - val_loss: 1.6965\n",
      "Epoch 51/200\n",
      "878/878 [==============================] - 2s - loss: 1.6984 - val_loss: 1.6933\n",
      "Epoch 52/200\n",
      "860/878 [============================>.] - ETA: 0s - loss: 1.6884\n",
      " Reduced learning rate to 0.005\n",
      "878/878 [==============================] - 2s - loss: 1.6885 - val_loss: 1.6951\n",
      "Epoch 53/200\n",
      "878/878 [==============================] - 2s - loss: 1.6787 - val_loss: 1.6896\n",
      "Epoch 54/200\n",
      "864/878 [============================>.] - ETA: 0s - loss: 1.6882\n",
      " Reduced learning rate to 0.0025\n",
      "878/878 [==============================] - 2s - loss: 1.6881 - val_loss: 1.6980\n",
      "Epoch 55/200\n",
      "867/878 [============================>.] - ETA: 0s - loss: 1.6794\n",
      " Reduced learning rate to 0.00125\n",
      "878/878 [==============================] - 2s - loss: 1.6798 - val_loss: 1.6922\n",
      "Epoch 56/200\n",
      "878/878 [==============================] - 2s - loss: 1.6793 - val_loss: 1.6863\n",
      "Epoch 57/200\n",
      "872/878 [============================>.] - ETA: 0s - loss: 1.6810\n",
      " Reduced learning rate to 0.000625\n",
      "878/878 [==============================] - 2s - loss: 1.6807 - val_loss: 1.6939\n",
      "Epoch 58/200\n",
      "878/878 [==============================] - 2s - loss: 1.6733 - val_loss: 1.6854\n",
      "Epoch 59/200\n",
      "875/878 [============================>.] - ETA: 0s - loss: 1.6828\n",
      " Reduced learning rate to 0.0003125\n",
      "878/878 [==============================] - 2s - loss: 1.6831 - val_loss: 1.6978\n",
      "Epoch 60/200\n",
      "867/878 [============================>.] - ETA: 0s - loss: 1.6743\n",
      " Reduced learning rate to 0.00015625\n",
      "878/878 [==============================] - 2s - loss: 1.6739 - val_loss: 1.6862\n",
      "Epoch 61/200\n",
      "876/878 [============================>.] - ETA: 0s - loss: 1.6807\n",
      " Reduced learning rate to 7.8125e-05\n",
      "878/878 [==============================] - 2s - loss: 1.6808 - val_loss: 1.6946\n",
      "Epoch 62/200\n",
      "878/878 [==============================] - 2s - loss: 1.6804 - val_loss: 1.6803\n",
      "Epoch 63/200\n",
      "877/878 [============================>.] - ETA: 0s - loss: 1.6833\n",
      " Reduced learning rate to 3.90625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878/878 [==============================] - 2s - loss: 1.6832 - val_loss: 1.6855\n",
      "Epoch 64/200\n",
      "863/878 [============================>.] - ETA: 0s - loss: 1.6763\n",
      " Reduced learning rate to 1.95312e-05\n",
      "878/878 [==============================] - 2s - loss: 1.6766 - val_loss: 1.6848\n",
      "Epoch 65/200\n",
      "862/878 [============================>.] - ETA: 0s - loss: 1.6790\n",
      " Reduced learning rate to 9.76562e-06\n",
      "878/878 [==============================] - 2s - loss: 1.6792 - val_loss: 1.6999\n",
      "Epoch 66/200\n",
      "864/878 [============================>.] - ETA: 0s - loss: 1.6831\n",
      " Reduced learning rate to 4.88281e-06\n",
      "878/878 [==============================] - 2s - loss: 1.6828 - val_loss: 1.6817\n",
      "Epoch 67/200\n",
      "862/878 [============================>.] - ETA: 0s - loss: 1.6788\n",
      " Reduced learning rate to 2.44141e-06\n",
      "878/878 [==============================] - 2s - loss: 1.6810 - val_loss: 1.6837\n",
      "Epoch 68/200\n",
      "869/878 [============================>.] - ETA: 0s - loss: 1.6915\n",
      " Reduced learning rate to 1.2207e-06\n",
      "878/878 [==============================] - 2s - loss: 1.6912 - val_loss: 1.6848\n",
      "Epoch 69/200\n",
      "878/878 [==============================] - 2s - loss: 1.6769 - val_loss: 1.6732\n",
      "Epoch 70/200\n",
      "869/878 [============================>.] - ETA: 0s - loss: 1.6798\n",
      " Reduced learning rate to 6.10352e-07\n",
      "878/878 [==============================] - 2s - loss: 1.6795 - val_loss: 1.6817\n",
      "Epoch 1/200\n",
      "878/878 [==============================] - 3s - loss: 9.7063 - val_loss: 3.2888\n",
      "Epoch 2/200\n",
      "878/878 [==============================] - 2s - loss: 3.2242 - val_loss: 3.1433\n",
      "Epoch 3/200\n",
      "878/878 [==============================] - 2s - loss: 3.0224 - val_loss: 3.4168\n",
      "Epoch 4/200\n",
      "878/878 [==============================] - 2s - loss: 2.8922 - val_loss: 2.0873\n",
      "Epoch 5/200\n",
      "878/878 [==============================] - 2s - loss: 2.7543 - val_loss: 2.6715\n",
      "Epoch 6/200\n",
      "878/878 [==============================] - 2s - loss: 2.6558 - val_loss: 2.4357\n",
      "Epoch 7/200\n",
      "878/878 [==============================] - 2s - loss: 2.5636 - val_loss: 2.1568\n",
      "Epoch 8/200\n",
      "878/878 [==============================] - 2s - loss: 2.4933 - val_loss: 2.3832\n",
      "Epoch 9/200\n",
      "878/878 [==============================] - 2s - loss: 2.4039 - val_loss: 2.4551\n",
      "Epoch 10/200\n",
      "878/878 [==============================] - 2s - loss: 2.3649 - val_loss: 2.1447\n",
      "Epoch 11/200\n",
      "878/878 [==============================] - 2s - loss: 2.3144 - val_loss: 2.0861\n",
      "Epoch 12/200\n",
      "878/878 [==============================] - 2s - loss: 2.2676 - val_loss: 2.1265\n",
      "Epoch 13/200\n",
      "878/878 [==============================] - 2s - loss: 2.1994 - val_loss: 2.4389\n",
      "Epoch 14/200\n",
      "878/878 [==============================] - 2s - loss: 2.1847 - val_loss: 2.6234\n",
      "Epoch 15/200\n",
      "878/878 [==============================] - 2s - loss: 2.1479 - val_loss: 1.8748\n",
      "Epoch 16/200\n",
      "878/878 [==============================] - 2s - loss: 2.1031 - val_loss: 2.2324\n",
      "Epoch 17/200\n",
      "878/878 [==============================] - 2s - loss: 2.0953 - val_loss: 1.9043\n",
      "Epoch 18/200\n",
      "878/878 [==============================] - 2s - loss: 2.0482 - val_loss: 1.7598\n",
      "Epoch 19/200\n",
      "878/878 [==============================] - 2s - loss: 2.0037 - val_loss: 1.7617\n",
      "Epoch 20/200\n",
      "878/878 [==============================] - 2s - loss: 1.9790 - val_loss: 2.3500\n",
      "Epoch 21/200\n",
      "878/878 [==============================] - 2s - loss: 2.0096 - val_loss: 1.7826\n",
      "Epoch 22/200\n",
      "878/878 [==============================] - 2s - loss: 1.9753 - val_loss: 1.9707\n",
      "Epoch 23/200\n",
      "878/878 [==============================] - 2s - loss: 1.9509 - val_loss: 1.7865\n",
      "Epoch 24/200\n",
      "878/878 [==============================] - 2s - loss: 1.9381 - val_loss: 2.0933\n",
      "Epoch 25/200\n",
      "878/878 [==============================] - 2s - loss: 1.9227 - val_loss: 1.9331\n",
      "Epoch 26/200\n",
      "878/878 [==============================] - 2s - loss: 1.9105 - val_loss: 1.7702\n",
      "Epoch 27/200\n",
      "878/878 [==============================] - 2s - loss: 1.9056 - val_loss: 1.8248\n",
      "Epoch 28/200\n",
      "878/878 [==============================] - 2s - loss: 1.8804 - val_loss: 1.9004\n",
      "Epoch 29/200\n",
      "862/878 [============================>.] - ETA: 0s - loss: 1.8888\n",
      " Reduced learning rate to 0.01\n",
      "878/878 [==============================] - 2s - loss: 1.8894 - val_loss: 1.9546\n",
      "Epoch 30/200\n",
      "878/878 [==============================] - 2s - loss: 1.7026 - val_loss: 1.7103\n",
      "Epoch 31/200\n",
      "878/878 [==============================] - 2s - loss: 1.7096 - val_loss: 1.7038\n",
      "Epoch 32/200\n",
      "873/878 [============================>.] - ETA: 0s - loss: 1.7098\n",
      " Reduced learning rate to 0.005\n",
      "878/878 [==============================] - 2s - loss: 1.7097 - val_loss: 1.7187\n",
      "Epoch 33/200\n",
      "878/878 [==============================] - 2s - loss: 1.7001 - val_loss: 1.6999\n",
      "Epoch 34/200\n",
      "878/878 [==============================] - 2s - loss: 1.7114 - val_loss: 1.6908\n",
      "Epoch 35/200\n",
      "873/878 [============================>.] - ETA: 0s - loss: 1.7065\n",
      " Reduced learning rate to 0.0025\n",
      "878/878 [==============================] - 2s - loss: 1.7064 - val_loss: 1.7072\n",
      "Epoch 36/200\n",
      "871/878 [============================>.] - ETA: 0s - loss: 1.7016\n",
      " Reduced learning rate to 0.00125\n",
      "878/878 [==============================] - 2s - loss: 1.7015 - val_loss: 1.7127\n",
      "Epoch 37/200\n",
      "860/878 [============================>.] - ETA: 0s - loss: 1.6901\n",
      " Reduced learning rate to 0.000625\n",
      "878/878 [==============================] - 2s - loss: 1.6895 - val_loss: 1.7013\n",
      "Epoch 38/200\n",
      "870/878 [============================>.] - ETA: 0s - loss: 1.7062\n",
      " Reduced learning rate to 0.0003125\n",
      "878/878 [==============================] - 2s - loss: 1.7055 - val_loss: 1.7030\n",
      "Epoch 39/200\n",
      "876/878 [============================>.] - ETA: 0s - loss: 1.6990\n",
      " Reduced learning rate to 0.00015625\n",
      "878/878 [==============================] - 2s - loss: 1.6990 - val_loss: 1.6953\n",
      "Epoch 40/200\n",
      "872/878 [============================>.] - ETA: 0s - loss: 1.6859\n",
      " Reduced learning rate to 7.8125e-05\n",
      "878/878 [==============================] - 2s - loss: 1.6856 - val_loss: 1.6960\n",
      "Epoch 41/200\n",
      "860/878 [============================>.] - ETA: 0s - loss: 1.6906\n",
      " Reduced learning rate to 3.90625e-05\n",
      "878/878 [==============================] - 2s - loss: 1.6904 - val_loss: 1.7084\n",
      "Epoch 42/200\n",
      "865/878 [============================>.] - ETA: 0s - loss: 1.7026\n",
      " Reduced learning rate to 1.95312e-05\n",
      "878/878 [==============================] - 2s - loss: 1.7022 - val_loss: 1.7045\n",
      "Epoch 43/200\n",
      "873/878 [============================>.] - ETA: 0s - loss: 1.7032\n",
      " Reduced learning rate to 9.76562e-06\n",
      "878/878 [==============================] - 2s - loss: 1.7031 - val_loss: 1.6993\n",
      "Epoch 44/200\n",
      "857/878 [============================>.] - ETA: 0s - loss: 1.6893\n",
      " Reduced learning rate to 4.88281e-06\n",
      "878/878 [==============================] - 2s - loss: 1.6896 - val_loss: 1.6979\n",
      "Epoch 45/200\n",
      "862/878 [============================>.] - ETA: 0s - loss: 1.6957\n",
      " Reduced learning rate to 2.44141e-06\n",
      "878/878 [==============================] - 2s - loss: 1.6948 - val_loss: 1.6992\n",
      "Epoch 46/200\n",
      "870/878 [============================>.] - ETA: 0s - loss: 1.6916\n",
      " Reduced learning rate to 1.2207e-06\n",
      "878/878 [==============================] - 2s - loss: 1.6916 - val_loss: 1.7071\n",
      "Epoch 47/200\n",
      "878/878 [==============================] - 2s - loss: 1.6932 - val_loss: 1.6816\n",
      "Epoch 48/200\n",
      "860/878 [============================>.] - ETA: 0s - loss: 1.6900\n",
      " Reduced learning rate to 6.10352e-07\n",
      "878/878 [==============================] - 2s - loss: 1.6908 - val_loss: 1.7024\n",
      "Epoch 1/200\n",
      "878/878 [==============================] - 4s - loss: 8.9809 - val_loss: 3.3777\n",
      "Epoch 2/200\n",
      "878/878 [==============================] - 2s - loss: 3.2548 - val_loss: 2.9974\n",
      "Epoch 3/200\n",
      "878/878 [==============================] - 2s - loss: 3.0636 - val_loss: 2.9557\n",
      "Epoch 4/200\n",
      "878/878 [==============================] - 2s - loss: 2.9021 - val_loss: 3.0260\n",
      "Epoch 5/200\n",
      "878/878 [==============================] - 2s - loss: 2.7735 - val_loss: 2.7365\n",
      "Epoch 6/200\n",
      "878/878 [==============================] - 2s - loss: 2.6563 - val_loss: 2.2637\n",
      "Epoch 7/200\n",
      "878/878 [==============================] - 2s - loss: 2.6150 - val_loss: 2.6871\n",
      "Epoch 8/200\n",
      "878/878 [==============================] - 2s - loss: 2.5344 - val_loss: 2.8710\n",
      "Epoch 9/200\n",
      "878/878 [==============================] - 2s - loss: 2.4541 - val_loss: 2.8481\n",
      "Epoch 10/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878/878 [==============================] - 2s - loss: 2.3812 - val_loss: 3.2966\n",
      "Epoch 11/200\n",
      "878/878 [==============================] - 2s - loss: 2.3401 - val_loss: 2.2480\n",
      "Epoch 12/200\n",
      "878/878 [==============================] - 2s - loss: 2.2867 - val_loss: 2.0272\n",
      "Epoch 13/200\n",
      "878/878 [==============================] - 2s - loss: 2.2397 - val_loss: 2.3883\n",
      "Epoch 14/200\n",
      "878/878 [==============================] - 2s - loss: 2.2187 - val_loss: 2.3750\n",
      "Epoch 15/200\n",
      "878/878 [==============================] - 2s - loss: 2.1667 - val_loss: 2.1776\n",
      "Epoch 16/200\n",
      "878/878 [==============================] - 2s - loss: 2.1264 - val_loss: 1.9479\n",
      "Epoch 17/200\n",
      "878/878 [==============================] - 2s - loss: 2.1021 - val_loss: 1.8677\n",
      "Epoch 18/200\n",
      "878/878 [==============================] - 2s - loss: 2.0872 - val_loss: 1.9570\n",
      "Epoch 19/200\n",
      "878/878 [==============================] - 2s - loss: 2.0335 - val_loss: 2.0533\n",
      "Epoch 20/200\n",
      "878/878 [==============================] - 2s - loss: 2.0275 - val_loss: 2.4156\n",
      "Epoch 21/200\n",
      "878/878 [==============================] - 2s - loss: 2.0007 - val_loss: 1.7481\n",
      "Epoch 22/200\n",
      "878/878 [==============================] - 2s - loss: 1.9916 - val_loss: 2.1681\n",
      "Epoch 23/200\n",
      "878/878 [==============================] - 2s - loss: 1.9663 - val_loss: 1.9279\n",
      "Epoch 24/200\n",
      "878/878 [==============================] - 2s - loss: 1.9374 - val_loss: 1.7755\n",
      "Epoch 25/200\n",
      "878/878 [==============================] - 2s - loss: 1.9404 - val_loss: 1.9959\n",
      "Epoch 26/200\n",
      "878/878 [==============================] - 2s - loss: 1.9185 - val_loss: 1.7916\n",
      "Epoch 27/200\n",
      "878/878 [==============================] - 2s - loss: 1.9092 - val_loss: 2.1817\n",
      "Epoch 28/200\n",
      "878/878 [==============================] - 2s - loss: 1.9052 - val_loss: 1.9595\n",
      "Epoch 29/200\n",
      "878/878 [==============================] - 2s - loss: 1.8946 - val_loss: 1.8432\n",
      "Epoch 30/200\n",
      "878/878 [==============================] - 2s - loss: 1.8967 - val_loss: 1.7377\n",
      "Epoch 31/200\n",
      "878/878 [==============================] - 2s - loss: 1.8698 - val_loss: 1.9101\n",
      "Epoch 32/200\n",
      "878/878 [==============================] - 2s - loss: 1.8670 - val_loss: 1.9335\n",
      "Epoch 33/200\n",
      "878/878 [==============================] - 2s - loss: 1.8756 - val_loss: 1.7084\n",
      "Epoch 34/200\n",
      "878/878 [==============================] - 2s - loss: 1.8406 - val_loss: 1.7513\n",
      "Epoch 35/200\n",
      "878/878 [==============================] - 2s - loss: 1.8332 - val_loss: 1.7552\n",
      "Epoch 36/200\n",
      "878/878 [==============================] - 2s - loss: 1.8523 - val_loss: 1.8647\n",
      "Epoch 37/200\n",
      "878/878 [==============================] - 2s - loss: 1.8397 - val_loss: 1.9466\n",
      "Epoch 38/200\n",
      "878/878 [==============================] - 2s - loss: 1.8229 - val_loss: 1.7553\n",
      "Epoch 39/200\n",
      "878/878 [==============================] - 2s - loss: 1.8400 - val_loss: 1.9487\n",
      "Epoch 40/200\n",
      "878/878 [==============================] - 2s - loss: 1.8040 - val_loss: 1.7533\n",
      "Epoch 41/200\n",
      "878/878 [==============================] - 2s - loss: 1.8110 - val_loss: 1.8001\n",
      "Epoch 42/200\n",
      "878/878 [==============================] - 2s - loss: 1.8054 - val_loss: 1.8881\n",
      "Epoch 43/200\n",
      "878/878 [==============================] - 2s - loss: 1.8193 - val_loss: 1.7461\n",
      "Epoch 44/200\n",
      "877/878 [============================>.] - ETA: 0s - loss: 1.8072\n",
      " Reduced learning rate to 0.01\n",
      "878/878 [==============================] - 3s - loss: 1.8073 - val_loss: 1.9311\n",
      "Epoch 45/200\n",
      "878/878 [==============================] - 2s - loss: 1.7088 - val_loss: 1.6993\n",
      "Epoch 46/200\n",
      "864/878 [============================>.] - ETA: 0s - loss: 1.6932\n",
      " Reduced learning rate to 0.005\n",
      "878/878 [==============================] - 2s - loss: 1.6931 - val_loss: 1.7123\n",
      "Epoch 47/200\n",
      "878/878 [==============================] - 2s - loss: 1.6893 - val_loss: 1.6914\n",
      "Epoch 48/200\n",
      "878/878 [==============================] - 2s - loss: 1.6834 - val_loss: 1.6889\n",
      "Epoch 49/200\n",
      "871/878 [============================>.] - ETA: 0s - loss: 1.6894\n",
      " Reduced learning rate to 0.0025\n",
      "878/878 [==============================] - 2s - loss: 1.6895 - val_loss: 1.6990\n",
      "Epoch 50/200\n",
      "865/878 [============================>.] - ETA: 0s - loss: 1.6840\n",
      " Reduced learning rate to 0.00125\n",
      "878/878 [==============================] - 2s - loss: 1.6852 - val_loss: 1.6908\n",
      "Epoch 51/200\n",
      "878/878 [==============================] - 2s - loss: 1.6870 - val_loss: 1.6869\n",
      "Epoch 52/200\n",
      "878/878 [==============================] - 2s - loss: 1.6898 - val_loss: 1.6843\n",
      "Epoch 53/200\n",
      "857/878 [============================>.] - ETA: 0s - loss: 1.6843\n",
      " Reduced learning rate to 0.000625\n",
      "878/878 [==============================] - 2s - loss: 1.6837 - val_loss: 1.6908\n",
      "Epoch 54/200\n",
      "875/878 [============================>.] - ETA: 0s - loss: 1.6818\n",
      " Reduced learning rate to 0.0003125\n",
      "878/878 [==============================] - 2s - loss: 1.6817 - val_loss: 1.6915\n",
      "Epoch 55/200\n",
      "878/878 [==============================] - 2s - loss: 1.6858 - val_loss: 1.6822\n",
      "Epoch 56/200\n",
      "876/878 [============================>.] - ETA: 0s - loss: 1.6892\n",
      " Reduced learning rate to 0.00015625\n",
      "878/878 [==============================] - 2s - loss: 1.6892 - val_loss: 1.6932\n",
      "Epoch 57/200\n",
      "878/878 [==============================] - 2s - loss: 1.6806 - val_loss: 1.6804\n",
      "Epoch 58/200\n",
      "870/878 [============================>.] - ETA: 0s - loss: 1.6721\n",
      " Reduced learning rate to 7.8125e-05\n",
      "878/878 [==============================] - 2s - loss: 1.6747 - val_loss: 1.6896\n",
      "Epoch 59/200\n",
      "875/878 [============================>.] - ETA: 0s - loss: 1.6785\n",
      " Reduced learning rate to 3.90625e-05\n",
      "878/878 [==============================] - 2s - loss: 1.6789 - val_loss: 1.6895\n",
      "Epoch 60/200\n",
      "874/878 [============================>.] - ETA: 0s - loss: 1.6793\n",
      " Reduced learning rate to 1.95312e-05\n",
      "878/878 [==============================] - 2s - loss: 1.6793 - val_loss: 1.6941\n",
      "Epoch 61/200\n",
      "869/878 [============================>.] - ETA: 0s - loss: 1.6847\n",
      " Reduced learning rate to 9.76562e-06\n",
      "878/878 [==============================] - 2s - loss: 1.6849 - val_loss: 1.7076\n",
      "Epoch 62/200\n",
      "873/878 [============================>.] - ETA: 0s - loss: 1.6833\n",
      " Reduced learning rate to 4.88281e-06\n",
      "878/878 [==============================] - 2s - loss: 1.6833 - val_loss: 1.6854\n",
      "Epoch 63/200\n",
      "872/878 [============================>.] - ETA: 0s - loss: 1.6843\n",
      " Reduced learning rate to 2.44141e-06\n",
      "878/878 [==============================] - 2s - loss: 1.6846 - val_loss: 1.6824\n",
      "Epoch 64/200\n",
      "861/878 [============================>.] - ETA: 0s - loss: 1.6862\n",
      " Reduced learning rate to 1.2207e-06\n",
      "878/878 [==============================] - 2s - loss: 1.6855 - val_loss: 1.6973\n",
      "Epoch 65/200\n",
      "867/878 [============================>.] - ETA: 0s - loss: 1.6859\n",
      " Reduced learning rate to 6.10352e-07\n",
      "878/878 [==============================] - 2s - loss: 1.6856 - val_loss: 1.6907\n",
      "Epoch 1/200\n",
      "878/878 [==============================] - 3s - loss: 9.3186 - val_loss: 3.6637\n",
      "Epoch 2/200\n",
      "878/878 [==============================] - 2s - loss: 3.2430 - val_loss: 3.1149\n",
      "Epoch 3/200\n",
      "878/878 [==============================] - 2s - loss: 3.0244 - val_loss: 3.3739\n",
      "Epoch 4/200\n",
      "878/878 [==============================] - 2s - loss: 2.8813 - val_loss: 2.6407\n",
      "Epoch 5/200\n",
      "878/878 [==============================] - 2s - loss: 2.7596 - val_loss: 2.9061\n",
      "Epoch 6/200\n",
      "878/878 [==============================] - 2s - loss: 2.6662 - val_loss: 2.9595\n",
      "Epoch 7/200\n",
      "878/878 [==============================] - 2s - loss: 2.5794 - val_loss: 2.3301\n",
      "Epoch 8/200\n",
      "878/878 [==============================] - 2s - loss: 2.5248 - val_loss: 2.4453\n",
      "Epoch 9/200\n",
      "878/878 [==============================] - 2s - loss: 2.4463 - val_loss: 2.2033\n",
      "Epoch 10/200\n",
      "878/878 [==============================] - 2s - loss: 2.3771 - val_loss: 2.2690\n",
      "Epoch 11/200\n",
      "878/878 [==============================] - 2s - loss: 2.3336 - val_loss: 2.0918\n",
      "Epoch 12/200\n",
      "878/878 [==============================] - 2s - loss: 2.2790 - val_loss: 2.1562\n",
      "Epoch 13/200\n",
      "878/878 [==============================] - 2s - loss: 2.2259 - val_loss: 1.9830\n",
      "Epoch 14/200\n",
      "878/878 [==============================] - 2s - loss: 2.1897 - val_loss: 1.9478\n",
      "Epoch 15/200\n",
      "878/878 [==============================] - 2s - loss: 2.1410 - val_loss: 1.8555\n",
      "Epoch 16/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878/878 [==============================] - 2s - loss: 2.1294 - val_loss: 1.9720\n",
      "Epoch 17/200\n",
      "878/878 [==============================] - 2s - loss: 2.0972 - val_loss: 2.0640\n",
      "Epoch 18/200\n",
      "878/878 [==============================] - 2s - loss: 2.0491 - val_loss: 1.9669\n",
      "Epoch 19/200\n",
      "878/878 [==============================] - 2s - loss: 2.0522 - val_loss: 1.8517\n",
      "Epoch 20/200\n",
      "878/878 [==============================] - 2s - loss: 2.0316 - val_loss: 2.1059\n",
      "Epoch 21/200\n",
      "878/878 [==============================] - 2s - loss: 2.0181 - val_loss: 2.2403\n",
      "Epoch 22/200\n",
      "878/878 [==============================] - 2s - loss: 1.9824 - val_loss: 2.0487\n",
      "Epoch 23/200\n",
      "878/878 [==============================] - 2s - loss: 1.9709 - val_loss: 1.7733\n",
      "Epoch 24/200\n",
      "878/878 [==============================] - 2s - loss: 1.9469 - val_loss: 1.7324\n",
      "Epoch 25/200\n",
      "878/878 [==============================] - 2s - loss: 1.9450 - val_loss: 1.8535\n",
      "Epoch 26/200\n",
      "878/878 [==============================] - 2s - loss: 1.9164 - val_loss: 1.8315\n",
      "Epoch 27/200\n",
      "878/878 [==============================] - 2s - loss: 1.9180 - val_loss: 1.7580\n",
      "Epoch 28/200\n",
      "878/878 [==============================] - 2s - loss: 1.8836 - val_loss: 1.9043\n",
      "Epoch 29/200\n",
      "878/878 [==============================] - 2s - loss: 1.9025 - val_loss: 2.1684\n",
      "Epoch 30/200\n",
      "878/878 [==============================] - 2s - loss: 1.8703 - val_loss: 2.1405\n",
      "Epoch 31/200\n",
      "878/878 [==============================] - 2s - loss: 1.8704 - val_loss: 1.7193\n",
      "Epoch 32/200\n",
      "878/878 [==============================] - 2s - loss: 1.8466 - val_loss: 1.7929\n",
      "Epoch 33/200\n",
      "878/878 [==============================] - 2s - loss: 1.8517 - val_loss: 2.1343\n",
      "Epoch 34/200\n",
      "878/878 [==============================] - 2s - loss: 1.8455 - val_loss: 1.9388\n",
      "Epoch 35/200\n",
      "878/878 [==============================] - 2s - loss: 1.8507 - val_loss: 2.1113\n",
      "Epoch 36/200\n",
      "878/878 [==============================] - 2s - loss: 1.8432 - val_loss: 1.7857\n",
      "Epoch 37/200\n",
      "878/878 [==============================] - 2s - loss: 1.8286 - val_loss: 1.7225\n",
      "Epoch 38/200\n",
      "878/878 [==============================] - 2s - loss: 1.8373 - val_loss: 1.7038\n",
      "Epoch 39/200\n",
      "878/878 [==============================] - 2s - loss: 1.8100 - val_loss: 1.7605\n",
      "Epoch 40/200\n",
      "878/878 [==============================] - 2s - loss: 1.8385 - val_loss: 1.7070\n",
      "Epoch 41/200\n",
      "878/878 [==============================] - 2s - loss: 1.8187 - val_loss: 1.7397\n",
      "Epoch 42/200\n",
      "878/878 [==============================] - 2s - loss: 1.8184 - val_loss: 1.8576\n",
      "Epoch 43/200\n",
      "878/878 [==============================] - 2s - loss: 1.8175 - val_loss: 1.7441\n",
      "Epoch 44/200\n",
      "878/878 [==============================] - 2s - loss: 1.7992 - val_loss: 1.7612\n",
      "Epoch 45/200\n",
      "878/878 [==============================] - 2s - loss: 1.7981 - val_loss: 1.7872\n",
      "Epoch 46/200\n",
      "878/878 [==============================] - 2s - loss: 1.7790 - val_loss: 1.8185\n",
      "Epoch 47/200\n",
      "878/878 [==============================] - 2s - loss: 1.7844 - val_loss: 1.8052\n",
      "Epoch 48/200\n",
      "878/878 [==============================] - 2s - loss: 1.7727 - val_loss: 1.7070\n",
      "Epoch 49/200\n",
      "862/878 [============================>.] - ETA: 0s - loss: 1.7792\n",
      " Reduced learning rate to 0.01\n",
      "878/878 [==============================] - 3s - loss: 1.7792 - val_loss: 1.7166\n",
      "Epoch 50/200\n",
      "871/878 [============================>.] - ETA: 0s - loss: 1.6896\n",
      " Reduced learning rate to 0.005\n",
      "878/878 [==============================] - 2s - loss: 1.6894 - val_loss: 1.7180\n",
      "Epoch 51/200\n",
      "878/878 [==============================] - 2s - loss: 1.6800 - val_loss: 1.6906\n",
      "Epoch 52/200\n",
      "872/878 [============================>.] - ETA: 0s - loss: 1.6801\n",
      " Reduced learning rate to 0.0025\n",
      "878/878 [==============================] - 2s - loss: 1.6798 - val_loss: 1.6975\n",
      "Epoch 53/200\n",
      "874/878 [============================>.] - ETA: 0s - loss: 1.6756\n",
      " Reduced learning rate to 0.00125\n",
      "878/878 [==============================] - 2s - loss: 1.6754 - val_loss: 1.6915\n",
      "Epoch 54/200\n",
      "878/878 [==============================] - 2s - loss: 1.6757 - val_loss: 1.6836\n",
      "Epoch 55/200\n",
      "866/878 [============================>.] - ETA: 0s - loss: 1.6932\n",
      " Reduced learning rate to 0.000625\n",
      "878/878 [==============================] - 2s - loss: 1.6926 - val_loss: 1.6896\n",
      "Epoch 56/200\n",
      "864/878 [============================>.] - ETA: 0s - loss: 1.6833\n",
      " Reduced learning rate to 0.0003125\n",
      "878/878 [==============================] - 2s - loss: 1.6826 - val_loss: 1.6856\n",
      "Epoch 57/200\n",
      "878/878 [==============================] - 2s - loss: 1.6768 - val_loss: 1.6804\n",
      "Epoch 58/200\n",
      "870/878 [============================>.] - ETA: 0s - loss: 1.6861\n",
      " Reduced learning rate to 0.00015625\n",
      "878/878 [==============================] - 2s - loss: 1.6865 - val_loss: 1.6971\n",
      "Epoch 59/200\n",
      "878/878 [==============================] - 2s - loss: 1.6824 - val_loss: 1.6778\n",
      "Epoch 60/200\n",
      "875/878 [============================>.] - ETA: 0s - loss: 1.6823\n",
      " Reduced learning rate to 7.8125e-05\n",
      "878/878 [==============================] - 2s - loss: 1.6823 - val_loss: 1.6940\n",
      "Epoch 61/200\n",
      "862/878 [============================>.] - ETA: 0s - loss: 1.6886\n",
      " Reduced learning rate to 3.90625e-05\n",
      "878/878 [==============================] - 2s - loss: 1.6880 - val_loss: 1.6862\n",
      "Epoch 62/200\n",
      "876/878 [============================>.] - ETA: 0s - loss: 1.6827\n",
      " Reduced learning rate to 1.95312e-05\n",
      "878/878 [==============================] - 2s - loss: 1.6827 - val_loss: 1.6854\n",
      "Epoch 63/200\n",
      "869/878 [============================>.] - ETA: 0s - loss: 1.6817\n",
      " Reduced learning rate to 9.76562e-06\n",
      "878/878 [==============================] - 2s - loss: 1.6811 - val_loss: 1.6883\n",
      "Epoch 64/200\n",
      "870/878 [============================>.] - ETA: 0s - loss: 1.6792\n",
      " Reduced learning rate to 4.88281e-06\n",
      "878/878 [==============================] - 2s - loss: 1.6797 - val_loss: 1.6799\n",
      "Epoch 65/200\n",
      "873/878 [============================>.] - ETA: 0s - loss: 1.6801\n",
      " Reduced learning rate to 2.44141e-06\n",
      "878/878 [==============================] - 2s - loss: 1.6800 - val_loss: 1.6895\n",
      "Epoch 66/200\n",
      "866/878 [============================>.] - ETA: 0s - loss: 1.6820\n",
      " Reduced learning rate to 1.2207e-06\n",
      "878/878 [==============================] - 2s - loss: 1.6816 - val_loss: 1.6850\n",
      "Epoch 67/200\n",
      "866/878 [============================>.] - ETA: 0s - loss: 1.6721\n",
      " Reduced learning rate to 6.10352e-07\n",
      "878/878 [==============================] - 2s - loss: 1.6723 - val_loss: 1.6921\n",
      "Epoch 1/200\n",
      "1757/1757 [==============================] - 6s - loss: 6.2621 - val_loss: 3.4431\n",
      "Epoch 2/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.9572 - val_loss: 2.8237\n",
      "Epoch 3/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.7120 - val_loss: 2.4851\n",
      "Epoch 4/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.5330 - val_loss: 2.0745\n",
      "Epoch 5/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.3782 - val_loss: 2.3203\n",
      "Epoch 6/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.2882 - val_loss: 2.0989\n",
      "Epoch 7/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.1973 - val_loss: 2.3156\n",
      "Epoch 8/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.1252 - val_loss: 2.0381\n",
      "Epoch 9/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.0644 - val_loss: 2.2344\n",
      "Epoch 10/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.0152 - val_loss: 1.8373\n",
      "Epoch 11/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.9824 - val_loss: 1.9310\n",
      "Epoch 12/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.9290 - val_loss: 2.2941\n",
      "Epoch 13/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.9127 - val_loss: 1.9756\n",
      "Epoch 14/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8865 - val_loss: 1.7589\n",
      "Epoch 15/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8791 - val_loss: 1.7801\n",
      "Epoch 16/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8548 - val_loss: 1.8163\n",
      "Epoch 17/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8513 - val_loss: 1.8007\n",
      "Epoch 18/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8330 - val_loss: 1.7589\n",
      "Epoch 19/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8205 - val_loss: 1.7425\n",
      "Epoch 20/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1757/1757 [==============================] - 4s - loss: 1.8227 - val_loss: 1.7723\n",
      "Epoch 21/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7965 - val_loss: 1.8242\n",
      "Epoch 22/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7942 - val_loss: 1.8410\n",
      "Epoch 23/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7976 - val_loss: 1.7567\n",
      "Epoch 24/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7841 - val_loss: 1.9612\n",
      "Epoch 25/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7875 - val_loss: 1.7035\n",
      "Epoch 26/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7700 - val_loss: 1.6942\n",
      "Epoch 27/200\n",
      "1757/1757 [==============================] - 5s - loss: 1.7724 - val_loss: 1.8997\n",
      "Epoch 28/200\n",
      "1757/1757 [==============================] - 5s - loss: 1.7633 - val_loss: 1.7539\n",
      "Epoch 29/200\n",
      "1757/1757 [==============================] - 5s - loss: 1.7648 - val_loss: 1.8267\n",
      "Epoch 30/200\n",
      "1757/1757 [==============================] - 5s - loss: 1.7633 - val_loss: 1.7671\n",
      "Epoch 31/200\n",
      "1757/1757 [==============================] - 5s - loss: 1.7509 - val_loss: 1.6980\n",
      "Epoch 32/200\n",
      "1757/1757 [==============================] - 5s - loss: 1.7555 - val_loss: 1.7145\n",
      "Epoch 33/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7500 - val_loss: 1.7046\n",
      "Epoch 34/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7523 - val_loss: 1.8317\n",
      "Epoch 35/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7437 - val_loss: 1.7056\n",
      "Epoch 36/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7328 - val_loss: 1.8409\n",
      "Epoch 37/200\n",
      "1742/1757 [============================>.] - ETA: 0s - loss: 1.7358\n",
      " Reduced learning rate to 0.01\n",
      "1757/1757 [==============================] - 5s - loss: 1.7356 - val_loss: 1.7265\n",
      "Epoch 38/200\n",
      "1757/1757 [==============================] - 5s - loss: 1.6816 - val_loss: 1.6785\n",
      "Epoch 39/200\n",
      "1748/1757 [============================>.] - ETA: 0s - loss: 1.6831\n",
      " Reduced learning rate to 0.005\n",
      "1757/1757 [==============================] - 4s - loss: 1.6829 - val_loss: 1.6837\n",
      "Epoch 40/200\n",
      "1746/1757 [============================>.] - ETA: 0s - loss: 1.6766\n",
      " Reduced learning rate to 0.0025\n",
      "1757/1757 [==============================] - 5s - loss: 1.6775 - val_loss: 1.6889\n",
      "Epoch 41/200\n",
      "1756/1757 [============================>.] - ETA: 0s - loss: 1.6700\n",
      " Reduced learning rate to 0.00125\n",
      "1757/1757 [==============================] - 4s - loss: 1.6700 - val_loss: 1.6890\n",
      "Epoch 42/200\n",
      "1755/1757 [============================>.] - ETA: 0s - loss: 1.6719\n",
      " Reduced learning rate to 0.000625\n",
      "1757/1757 [==============================] - 4s - loss: 1.6719 - val_loss: 1.6801\n",
      "Epoch 43/200\n",
      "1741/1757 [============================>.] - ETA: 0s - loss: 1.6661\n",
      " Reduced learning rate to 0.0003125\n",
      "1757/1757 [==============================] - 4s - loss: 1.6660 - val_loss: 1.6953\n",
      "Epoch 44/200\n",
      "1745/1757 [============================>.] - ETA: 0s - loss: 1.6724\n",
      " Reduced learning rate to 0.00015625\n",
      "1757/1757 [==============================] - 4s - loss: 1.6725 - val_loss: 1.6860\n",
      "Epoch 45/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6731 - val_loss: 1.6758\n",
      "Epoch 46/200\n",
      "1755/1757 [============================>.] - ETA: 0s - loss: 1.6756\n",
      " Reduced learning rate to 7.8125e-05\n",
      "1757/1757 [==============================] - 4s - loss: 1.6756 - val_loss: 1.6848\n",
      "Epoch 47/200\n",
      "1741/1757 [============================>.] - ETA: 0s - loss: 1.6624\n",
      " Reduced learning rate to 3.90625e-05\n",
      "1757/1757 [==============================] - 4s - loss: 1.6630 - val_loss: 1.6875\n",
      "Epoch 48/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6735 - val_loss: 1.6745\n",
      "Epoch 49/200\n",
      "1741/1757 [============================>.] - ETA: 0s - loss: 1.6638\n",
      " Reduced learning rate to 1.95312e-05\n",
      "1757/1757 [==============================] - 4s - loss: 1.6639 - val_loss: 1.6811\n",
      "Epoch 50/200\n",
      "1740/1757 [============================>.] - ETA: 0s - loss: 1.6710\n",
      " Reduced learning rate to 9.76562e-06\n",
      "1757/1757 [==============================] - 4s - loss: 1.6708 - val_loss: 1.6811\n",
      "Epoch 51/200\n",
      "1740/1757 [============================>.] - ETA: 0s - loss: 1.6727\n",
      " Reduced learning rate to 4.88281e-06\n",
      "1757/1757 [==============================] - 4s - loss: 1.6729 - val_loss: 1.6835\n",
      "Epoch 52/200\n",
      "1739/1757 [============================>.] - ETA: 0s - loss: 1.6712\n",
      " Reduced learning rate to 2.44141e-06\n",
      "1757/1757 [==============================] - 4s - loss: 1.6712 - val_loss: 1.6839\n",
      "Epoch 53/200\n",
      "1753/1757 [============================>.] - ETA: 0s - loss: 1.6731\n",
      " Reduced learning rate to 1.2207e-06\n",
      "1757/1757 [==============================] - 4s - loss: 1.6732 - val_loss: 1.6889\n",
      "Epoch 54/200\n",
      "1748/1757 [============================>.] - ETA: 0s - loss: 1.6738\n",
      " Reduced learning rate to 6.10352e-07\n",
      "1757/1757 [==============================] - 4s - loss: 1.6736 - val_loss: 1.6837\n",
      "Epoch 1/200\n",
      "1757/1757 [==============================] - 5s - loss: 5.9754 - val_loss: 2.7308\n",
      "Epoch 2/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.9344 - val_loss: 2.5928\n",
      "Epoch 3/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.7059 - val_loss: 2.9382\n",
      "Epoch 4/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.5390 - val_loss: 2.8368\n",
      "Epoch 5/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.4109 - val_loss: 2.5576\n",
      "Epoch 6/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.2994 - val_loss: 2.0508\n",
      "Epoch 7/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.2110 - val_loss: 2.0587\n",
      "Epoch 8/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.1326 - val_loss: 2.0730\n",
      "Epoch 9/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.0717 - val_loss: 2.3985\n",
      "Epoch 10/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.0250 - val_loss: 2.1593\n",
      "Epoch 11/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.9695 - val_loss: 1.9639\n",
      "Epoch 12/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.9355 - val_loss: 1.7738\n",
      "Epoch 13/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.9156 - val_loss: 1.7280\n",
      "Epoch 14/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8956 - val_loss: 1.8111\n",
      "Epoch 15/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8834 - val_loss: 2.1326\n",
      "Epoch 16/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8568 - val_loss: 1.9076\n",
      "Epoch 17/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8434 - val_loss: 1.7567\n",
      "Epoch 18/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8263 - val_loss: 1.8661\n",
      "Epoch 19/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8183 - val_loss: 1.7709\n",
      "Epoch 20/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8030 - val_loss: 1.7162\n",
      "Epoch 21/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7924 - val_loss: 1.8243\n",
      "Epoch 22/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8007 - val_loss: 1.7656\n",
      "Epoch 23/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7907 - val_loss: 1.7785\n",
      "Epoch 24/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7827 - val_loss: 1.7134\n",
      "Epoch 25/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7783 - val_loss: 1.7447\n",
      "Epoch 26/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7759 - val_loss: 1.8282\n",
      "Epoch 27/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7710 - val_loss: 1.7862\n",
      "Epoch 28/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7617 - val_loss: 1.6992\n",
      "Epoch 29/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7645 - val_loss: 1.7514\n",
      "Epoch 30/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7597 - val_loss: 1.7319\n",
      "Epoch 31/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7549 - val_loss: 1.7004\n",
      "Epoch 32/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7554 - val_loss: 1.7667\n",
      "Epoch 33/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7403 - val_loss: 1.7187\n",
      "Epoch 34/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7407 - val_loss: 1.7097\n",
      "Epoch 35/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7436 - val_loss: 1.7631\n",
      "Epoch 36/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1757/1757 [==============================] - 4s - loss: 1.7504 - val_loss: 1.7223\n",
      "Epoch 37/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7427 - val_loss: 1.6965\n",
      "Epoch 38/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7410 - val_loss: 1.7070\n",
      "Epoch 39/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7288 - val_loss: 1.6910\n",
      "Epoch 40/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7280 - val_loss: 1.7330\n",
      "Epoch 41/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7359 - val_loss: 1.6989\n",
      "Epoch 42/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7324 - val_loss: 1.8268\n",
      "Epoch 43/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7286 - val_loss: 1.7252\n",
      "Epoch 44/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7186 - val_loss: 1.7538\n",
      "Epoch 45/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7298 - val_loss: 1.6772\n",
      "Epoch 46/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7188 - val_loss: 1.6875\n",
      "Epoch 47/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7221 - val_loss: 1.9076\n",
      "Epoch 48/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7212 - val_loss: 1.6889\n",
      "Epoch 49/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7218 - val_loss: 1.7017\n",
      "Epoch 50/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7249 - val_loss: 1.7397\n",
      "Epoch 51/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7162 - val_loss: 1.6722\n",
      "Epoch 52/200\n",
      "1757/1757 [==============================] - 5s - loss: 1.7145 - val_loss: 1.6956\n",
      "Epoch 53/200\n",
      "1757/1757 [==============================] - 5s - loss: 1.7124 - val_loss: 1.6902\n",
      "Epoch 54/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7201 - val_loss: 1.7380\n",
      "Epoch 55/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7160 - val_loss: 1.8281\n",
      "Epoch 56/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7056 - val_loss: 1.6941\n",
      "Epoch 57/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7197 - val_loss: 1.8370\n",
      "Epoch 58/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7107 - val_loss: 1.6902\n",
      "Epoch 59/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7050 - val_loss: 1.6914\n",
      "Epoch 60/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6989 - val_loss: 1.7032\n",
      "Epoch 61/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7048 - val_loss: 1.6887\n",
      "Epoch 62/200\n",
      "1740/1757 [============================>.] - ETA: 0s - loss: 1.7062\n",
      " Reduced learning rate to 0.01\n",
      "1757/1757 [==============================] - 5s - loss: 1.7058 - val_loss: 1.7280\n",
      "Epoch 63/200\n",
      "1743/1757 [============================>.] - ETA: 0s - loss: 1.6684\n",
      " Reduced learning rate to 0.005\n",
      "1757/1757 [==============================] - 4s - loss: 1.6685 - val_loss: 1.6782\n",
      "Epoch 64/200\n",
      "1740/1757 [============================>.] - ETA: 0s - loss: 1.6640\n",
      " Reduced learning rate to 0.0025\n",
      "1757/1757 [==============================] - 4s - loss: 1.6636 - val_loss: 1.6794\n",
      "Epoch 65/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6613 - val_loss: 1.6692\n",
      "Epoch 66/200\n",
      "1743/1757 [============================>.] - ETA: 0s - loss: 1.6641\n",
      " Reduced learning rate to 0.00125\n",
      "1757/1757 [==============================] - 4s - loss: 1.6641 - val_loss: 1.6743\n",
      "Epoch 67/200\n",
      "1736/1757 [============================>.] - ETA: 0s - loss: 1.6619\n",
      " Reduced learning rate to 0.000625\n",
      "1757/1757 [==============================] - 4s - loss: 1.6624 - val_loss: 1.6722\n",
      "Epoch 68/200\n",
      "1753/1757 [============================>.] - ETA: 0s - loss: 1.6652\n",
      " Reduced learning rate to 0.0003125\n",
      "1757/1757 [==============================] - 4s - loss: 1.6650 - val_loss: 1.6706\n",
      "Epoch 69/200\n",
      "1753/1757 [============================>.] - ETA: 0s - loss: 1.6610\n",
      " Reduced learning rate to 0.00015625\n",
      "1757/1757 [==============================] - 4s - loss: 1.6610 - val_loss: 1.6743\n",
      "Epoch 70/200\n",
      "1756/1757 [============================>.] - ETA: 0s - loss: 1.6630\n",
      " Reduced learning rate to 7.8125e-05\n",
      "1757/1757 [==============================] - 4s - loss: 1.6630 - val_loss: 1.6715\n",
      "Epoch 71/200\n",
      "1740/1757 [============================>.] - ETA: 0s - loss: 1.6577\n",
      " Reduced learning rate to 3.90625e-05\n",
      "1757/1757 [==============================] - 4s - loss: 1.6572 - val_loss: 1.6800\n",
      "Epoch 72/200\n",
      "1740/1757 [============================>.] - ETA: 0s - loss: 1.6641\n",
      " Reduced learning rate to 1.95312e-05\n",
      "1757/1757 [==============================] - 4s - loss: 1.6638 - val_loss: 1.6762\n",
      "Epoch 73/200\n",
      "1753/1757 [============================>.] - ETA: 0s - loss: 1.6640\n",
      " Reduced learning rate to 9.76562e-06\n",
      "1757/1757 [==============================] - 4s - loss: 1.6641 - val_loss: 1.6785\n",
      "Epoch 74/200\n",
      "1741/1757 [============================>.] - ETA: 0s - loss: 1.6616\n",
      " Reduced learning rate to 4.88281e-06\n",
      "1757/1757 [==============================] - 4s - loss: 1.6613 - val_loss: 1.6801\n",
      "Epoch 75/200\n",
      "1738/1757 [============================>.] - ETA: 0s - loss: 1.6569\n",
      " Reduced learning rate to 2.44141e-06\n",
      "1757/1757 [==============================] - 4s - loss: 1.6570 - val_loss: 1.6745\n",
      "Epoch 76/200\n",
      "1755/1757 [============================>.] - ETA: 0s - loss: 1.6617\n",
      " Reduced learning rate to 1.2207e-06\n",
      "1757/1757 [==============================] - 4s - loss: 1.6617 - val_loss: 1.6784\n",
      "Epoch 77/200\n",
      "1739/1757 [============================>.] - ETA: 0s - loss: 1.6590\n",
      " Reduced learning rate to 6.10352e-07\n",
      "1757/1757 [==============================] - 4s - loss: 1.6591 - val_loss: 1.6782\n",
      "Epoch 1/200\n",
      "1757/1757 [==============================] - 5s - loss: 6.2159 - val_loss: 3.2994\n",
      "Epoch 2/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.9969 - val_loss: 3.0045\n",
      "Epoch 3/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.7325 - val_loss: 2.2266\n",
      "Epoch 4/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.5444 - val_loss: 2.3282\n",
      "Epoch 5/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.4261 - val_loss: 2.3943\n",
      "Epoch 6/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.2885 - val_loss: 2.3435\n",
      "Epoch 7/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.1957 - val_loss: 2.7109\n",
      "Epoch 8/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.1165 - val_loss: 1.9454\n",
      "Epoch 9/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.0630 - val_loss: 1.7862\n",
      "Epoch 10/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.0108 - val_loss: 1.7809\n",
      "Epoch 11/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.9795 - val_loss: 1.7851\n",
      "Epoch 12/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.9482 - val_loss: 2.2776\n",
      "Epoch 13/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.9059 - val_loss: 1.9138\n",
      "Epoch 14/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8941 - val_loss: 1.9841\n",
      "Epoch 15/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8649 - val_loss: 1.7269\n",
      "Epoch 16/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8636 - val_loss: 1.9443\n",
      "Epoch 17/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8412 - val_loss: 1.7866\n",
      "Epoch 18/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8348 - val_loss: 1.7734\n",
      "Epoch 19/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8105 - val_loss: 1.7982\n",
      "Epoch 20/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8026 - val_loss: 1.9648\n",
      "Epoch 21/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8042 - val_loss: 1.7134\n",
      "Epoch 22/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7794 - val_loss: 1.9180\n",
      "Epoch 23/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7774 - val_loss: 1.6965\n",
      "Epoch 24/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7736 - val_loss: 1.8217\n",
      "Epoch 25/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7723 - val_loss: 1.6967\n",
      "Epoch 26/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7761 - val_loss: 1.7017\n",
      "Epoch 27/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7692 - val_loss: 1.7016\n",
      "Epoch 28/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7628 - val_loss: 1.7410\n",
      "Epoch 29/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1757/1757 [==============================] - 4s - loss: 1.7617 - val_loss: 1.7074\n",
      "Epoch 30/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7547 - val_loss: 1.8345\n",
      "Epoch 31/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7460 - val_loss: 1.8253\n",
      "Epoch 32/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7543 - val_loss: 1.6980\n",
      "Epoch 33/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7432 - val_loss: 1.7085\n",
      "Epoch 34/200\n",
      "1744/1757 [============================>.] - ETA: 0s - loss: 1.7441\n",
      " Reduced learning rate to 0.01\n",
      "1757/1757 [==============================] - 6s - loss: 1.7437 - val_loss: 1.7347\n",
      "Epoch 35/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6814 - val_loss: 1.6840\n",
      "Epoch 36/200\n",
      "1747/1757 [============================>.] - ETA: 0s - loss: 1.6765\n",
      " Reduced learning rate to 0.005\n",
      "1757/1757 [==============================] - 4s - loss: 1.6766 - val_loss: 1.6864\n",
      "Epoch 37/200\n",
      "1739/1757 [============================>.] - ETA: 0s - loss: 1.6699\n",
      " Reduced learning rate to 0.0025\n",
      "1757/1757 [==============================] - 4s - loss: 1.6698 - val_loss: 1.6967\n",
      "Epoch 38/200\n",
      "1737/1757 [============================>.] - ETA: 0s - loss: 1.6708\n",
      " Reduced learning rate to 0.00125\n",
      "1757/1757 [==============================] - 4s - loss: 1.6706 - val_loss: 1.6851\n",
      "Epoch 39/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6705 - val_loss: 1.6797\n",
      "Epoch 40/200\n",
      "1746/1757 [============================>.] - ETA: 0s - loss: 1.6746\n",
      " Reduced learning rate to 0.000625\n",
      "1757/1757 [==============================] - 4s - loss: 1.6747 - val_loss: 1.6861\n",
      "Epoch 41/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6708 - val_loss: 1.6785\n",
      "Epoch 42/200\n",
      "1753/1757 [============================>.] - ETA: 0s - loss: 1.6715\n",
      " Reduced learning rate to 0.0003125\n",
      "1757/1757 [==============================] - 4s - loss: 1.6715 - val_loss: 1.6860\n",
      "Epoch 43/200\n",
      "1747/1757 [============================>.] - ETA: 0s - loss: 1.6674\n",
      " Reduced learning rate to 0.00015625\n",
      "1757/1757 [==============================] - 4s - loss: 1.6678 - val_loss: 1.6811\n",
      "Epoch 44/200\n",
      "1743/1757 [============================>.] - ETA: 0s - loss: 1.6797\n",
      " Reduced learning rate to 7.8125e-05\n",
      "1757/1757 [==============================] - 4s - loss: 1.6800 - val_loss: 1.6835\n",
      "Epoch 45/200\n",
      "1745/1757 [============================>.] - ETA: 0s - loss: 1.6717\n",
      " Reduced learning rate to 3.90625e-05\n",
      "1757/1757 [==============================] - 4s - loss: 1.6716 - val_loss: 1.6840\n",
      "Epoch 46/200\n",
      "1743/1757 [============================>.] - ETA: 0s - loss: 1.6735\n",
      " Reduced learning rate to 1.95312e-05\n",
      "1757/1757 [==============================] - 4s - loss: 1.6734 - val_loss: 1.6914\n",
      "Epoch 47/200\n",
      "1744/1757 [============================>.] - ETA: 0s - loss: 1.6669\n",
      " Reduced learning rate to 9.76562e-06\n",
      "1757/1757 [==============================] - 4s - loss: 1.6667 - val_loss: 1.6796\n",
      "Epoch 48/200\n",
      "1741/1757 [============================>.] - ETA: 0s - loss: 1.6676\n",
      " Reduced learning rate to 4.88281e-06\n",
      "1757/1757 [==============================] - 4s - loss: 1.6677 - val_loss: 1.6835\n",
      "Epoch 49/200\n",
      "1751/1757 [============================>.] - ETA: 0s - loss: 1.6712\n",
      " Reduced learning rate to 2.44141e-06\n",
      "1757/1757 [==============================] - 4s - loss: 1.6712 - val_loss: 1.6824\n",
      "Epoch 50/200\n",
      "1756/1757 [============================>.] - ETA: 0s - loss: 1.6713\n",
      " Reduced learning rate to 1.2207e-06\n",
      "1757/1757 [==============================] - 4s - loss: 1.6713 - val_loss: 1.6800\n",
      "Epoch 51/200\n",
      "1743/1757 [============================>.] - ETA: 0s - loss: 1.6691\n",
      " Reduced learning rate to 6.10352e-07\n",
      "1757/1757 [==============================] - 4s - loss: 1.6688 - val_loss: 1.6889\n",
      "Epoch 1/200\n",
      "1757/1757 [==============================] - 5s - loss: 6.2141 - val_loss: 3.3822\n",
      "Epoch 2/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.9639 - val_loss: 3.1244\n",
      "Epoch 3/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.7053 - val_loss: 2.2277\n",
      "Epoch 4/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.5330 - val_loss: 2.4268\n",
      "Epoch 5/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.3936 - val_loss: 2.9660\n",
      "Epoch 6/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.2924 - val_loss: 2.0665\n",
      "Epoch 7/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.1941 - val_loss: 2.4581\n",
      "Epoch 8/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.1404 - val_loss: 2.1379\n",
      "Epoch 9/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.0736 - val_loss: 2.2070\n",
      "Epoch 10/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.0020 - val_loss: 1.8516\n",
      "Epoch 11/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.9835 - val_loss: 1.9909\n",
      "Epoch 12/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.9383 - val_loss: 1.9612\n",
      "Epoch 13/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.9213 - val_loss: 1.8254\n",
      "Epoch 14/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8915 - val_loss: 1.9622\n",
      "Epoch 15/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8803 - val_loss: 2.0192\n",
      "Epoch 16/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8589 - val_loss: 1.7969\n",
      "Epoch 17/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8477 - val_loss: 2.1210\n",
      "Epoch 18/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8184 - val_loss: 1.8111\n",
      "Epoch 19/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8236 - val_loss: 1.9961\n",
      "Epoch 20/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8074 - val_loss: 1.7148\n",
      "Epoch 21/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7949 - val_loss: 1.7148\n",
      "Epoch 22/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7971 - val_loss: 2.1097\n",
      "Epoch 23/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7849 - val_loss: 1.7148\n",
      "Epoch 24/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7746 - val_loss: 1.6941\n",
      "Epoch 25/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7710 - val_loss: 1.7486\n",
      "Epoch 26/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7680 - val_loss: 1.7160\n",
      "Epoch 27/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7656 - val_loss: 1.7503\n",
      "Epoch 28/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7655 - val_loss: 1.7020\n",
      "Epoch 29/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7570 - val_loss: 1.8668\n",
      "Epoch 30/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7535 - val_loss: 1.7515\n",
      "Epoch 31/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7487 - val_loss: 1.7058\n",
      "Epoch 32/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7516 - val_loss: 1.8973\n",
      "Epoch 33/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7478 - val_loss: 1.9404\n",
      "Epoch 34/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7389 - val_loss: 1.7110\n",
      "Epoch 35/200\n",
      "1741/1757 [============================>.] - ETA: 0s - loss: 1.7496\n",
      " Reduced learning rate to 0.01\n",
      "1757/1757 [==============================] - 5s - loss: 1.7497 - val_loss: 1.7187\n",
      "Epoch 36/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6752 - val_loss: 1.6793\n",
      "Epoch 37/200\n",
      "1740/1757 [============================>.] - ETA: 0s - loss: 1.6743\n",
      " Reduced learning rate to 0.005\n",
      "1757/1757 [==============================] - 4s - loss: 1.6743 - val_loss: 1.6821\n",
      "Epoch 38/200\n",
      "1750/1757 [============================>.] - ETA: 0s - loss: 1.6758\n",
      " Reduced learning rate to 0.0025\n",
      "1757/1757 [==============================] - 4s - loss: 1.6758 - val_loss: 1.6837\n",
      "Epoch 39/200\n",
      "1753/1757 [============================>.] - ETA: 0s - loss: 1.6688\n",
      " Reduced learning rate to 0.00125\n",
      "1757/1757 [==============================] - 4s - loss: 1.6688 - val_loss: 1.6899\n",
      "Epoch 40/200\n",
      "1742/1757 [============================>.] - ETA: 0s - loss: 1.6748\n",
      " Reduced learning rate to 0.000625\n",
      "1757/1757 [==============================] - 4s - loss: 1.6749 - val_loss: 1.6837\n",
      "Epoch 41/200\n",
      "1747/1757 [============================>.] - ETA: 0s - loss: 1.6725\n",
      " Reduced learning rate to 0.0003125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1757/1757 [==============================] - 4s - loss: 1.6728 - val_loss: 1.6812\n",
      "Epoch 42/200\n",
      "1743/1757 [============================>.] - ETA: 0s - loss: 1.6727\n",
      " Reduced learning rate to 0.00015625\n",
      "1757/1757 [==============================] - 4s - loss: 1.6726 - val_loss: 1.6861\n",
      "Epoch 43/200\n",
      "1748/1757 [============================>.] - ETA: 0s - loss: 1.6696\n",
      " Reduced learning rate to 7.8125e-05\n",
      "1757/1757 [==============================] - 4s - loss: 1.6694 - val_loss: 1.6851\n",
      "Epoch 44/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6657 - val_loss: 1.6746\n",
      "Epoch 45/200\n",
      "1746/1757 [============================>.] - ETA: 0s - loss: 1.6749\n",
      " Reduced learning rate to 3.90625e-05\n",
      "1757/1757 [==============================] - 4s - loss: 1.6748 - val_loss: 1.6824\n",
      "Epoch 46/200\n",
      "1737/1757 [============================>.] - ETA: 0s - loss: 1.6709\n",
      " Reduced learning rate to 1.95312e-05\n",
      "1757/1757 [==============================] - 4s - loss: 1.6721 - val_loss: 1.6850\n",
      "Epoch 47/200\n",
      "1755/1757 [============================>.] - ETA: 0s - loss: 1.6682\n",
      " Reduced learning rate to 9.76562e-06\n",
      "1757/1757 [==============================] - 4s - loss: 1.6682 - val_loss: 1.6950\n",
      "Epoch 48/200\n",
      "1755/1757 [============================>.] - ETA: 0s - loss: 1.6705\n",
      " Reduced learning rate to 4.88281e-06\n",
      "1757/1757 [==============================] - 4s - loss: 1.6705 - val_loss: 1.6832\n",
      "Epoch 49/200\n",
      "1743/1757 [============================>.] - ETA: 0s - loss: 1.6699\n",
      " Reduced learning rate to 2.44141e-06\n",
      "1757/1757 [==============================] - 4s - loss: 1.6700 - val_loss: 1.6890\n",
      "Epoch 50/200\n",
      "1748/1757 [============================>.] - ETA: 0s - loss: 1.6717\n",
      " Reduced learning rate to 1.2207e-06\n",
      "1757/1757 [==============================] - 4s - loss: 1.6718 - val_loss: 1.6821\n",
      "Epoch 51/200\n",
      "1754/1757 [============================>.] - ETA: 0s - loss: 1.6714\n",
      " Reduced learning rate to 6.10352e-07\n",
      "1757/1757 [==============================] - 4s - loss: 1.6713 - val_loss: 1.6758\n",
      "Epoch 1/200\n",
      "1757/1757 [==============================] - 5s - loss: 6.1979 - val_loss: 3.3799\n",
      "Epoch 2/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.9570 - val_loss: 2.6757\n",
      "Epoch 3/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.7185 - val_loss: 3.0131\n",
      "Epoch 4/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.5259 - val_loss: 2.0713\n",
      "Epoch 5/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.4201 - val_loss: 2.5771\n",
      "Epoch 6/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.2962 - val_loss: 2.4755\n",
      "Epoch 7/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.2053 - val_loss: 2.8473\n",
      "Epoch 8/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.1398 - val_loss: 1.7720\n",
      "Epoch 9/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.0761 - val_loss: 2.2646\n",
      "Epoch 10/200\n",
      "1757/1757 [==============================] - 4s - loss: 2.0273 - val_loss: 2.0378\n",
      "Epoch 11/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.9970 - val_loss: 1.9258\n",
      "Epoch 12/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.9569 - val_loss: 1.7965\n",
      "Epoch 13/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.9330 - val_loss: 1.8722\n",
      "Epoch 14/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.9118 - val_loss: 1.8229\n",
      "Epoch 15/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8838 - val_loss: 2.1202\n",
      "Epoch 16/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8819 - val_loss: 1.8593\n",
      "Epoch 17/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8638 - val_loss: 1.7344\n",
      "Epoch 18/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8373 - val_loss: 1.7227\n",
      "Epoch 19/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8201 - val_loss: 1.9990\n",
      "Epoch 20/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8163 - val_loss: 1.7997\n",
      "Epoch 21/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8120 - val_loss: 1.7301\n",
      "Epoch 22/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.8088 - val_loss: 1.8893\n",
      "Epoch 23/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7901 - val_loss: 1.7148\n",
      "Epoch 24/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7941 - val_loss: 1.7383\n",
      "Epoch 25/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7810 - val_loss: 1.7056\n",
      "Epoch 26/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7912 - val_loss: 1.8893\n",
      "Epoch 27/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7777 - val_loss: 1.8206\n",
      "Epoch 28/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7760 - val_loss: 1.7059\n",
      "Epoch 29/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7785 - val_loss: 1.6957\n",
      "Epoch 30/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7607 - val_loss: 1.7110\n",
      "Epoch 31/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7579 - val_loss: 1.7329\n",
      "Epoch 32/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7599 - val_loss: 1.7446\n",
      "Epoch 33/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7573 - val_loss: 1.8320\n",
      "Epoch 34/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7470 - val_loss: 1.9493\n",
      "Epoch 35/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7479 - val_loss: 1.7489\n",
      "Epoch 36/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7406 - val_loss: 1.7542\n",
      "Epoch 37/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7396 - val_loss: 1.8167\n",
      "Epoch 38/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7465 - val_loss: 1.6957\n",
      "Epoch 39/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7366 - val_loss: 1.7188\n",
      "Epoch 40/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7446 - val_loss: 1.7188\n",
      "Epoch 41/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7418 - val_loss: 1.7108\n",
      "Epoch 42/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7424 - val_loss: 1.7344\n",
      "Epoch 43/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7359 - val_loss: 1.6954\n",
      "Epoch 44/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7282 - val_loss: 1.8047\n",
      "Epoch 45/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7315 - val_loss: 1.7421\n",
      "Epoch 46/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7295 - val_loss: 1.7316\n",
      "Epoch 47/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7199 - val_loss: 1.6918\n",
      "Epoch 48/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7250 - val_loss: 1.6953\n",
      "Epoch 49/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7232 - val_loss: 1.8402\n",
      "Epoch 50/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7269 - val_loss: 1.7280\n",
      "Epoch 51/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7178 - val_loss: 1.6872\n",
      "Epoch 52/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7233 - val_loss: 1.8126\n",
      "Epoch 53/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7186 - val_loss: 1.7173\n",
      "Epoch 54/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7196 - val_loss: 1.7358\n",
      "Epoch 55/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7140 - val_loss: 1.7045\n",
      "Epoch 56/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7127 - val_loss: 1.6871\n",
      "Epoch 57/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7063 - val_loss: 1.6993\n",
      "Epoch 58/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7138 - val_loss: 1.6911\n",
      "Epoch 59/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7048 - val_loss: 1.7760\n",
      "Epoch 60/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7148 - val_loss: 1.7095\n",
      "Epoch 61/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7111 - val_loss: 1.7004\n",
      "Epoch 62/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7095 - val_loss: 1.7020\n",
      "Epoch 63/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7145 - val_loss: 1.7293\n",
      "Epoch 64/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7114 - val_loss: 1.7146\n",
      "Epoch 65/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7119 - val_loss: 1.6836\n",
      "Epoch 66/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1757/1757 [==============================] - 4s - loss: 1.7078 - val_loss: 1.6770\n",
      "Epoch 67/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7092 - val_loss: 1.6793\n",
      "Epoch 68/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7127 - val_loss: 1.8060\n",
      "Epoch 69/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7087 - val_loss: 1.6863\n",
      "Epoch 70/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6999 - val_loss: 1.8214\n",
      "Epoch 71/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7000 - val_loss: 1.6957\n",
      "Epoch 72/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7070 - val_loss: 1.6836\n",
      "Epoch 73/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7047 - val_loss: 1.6773\n",
      "Epoch 74/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6980 - val_loss: 1.7223\n",
      "Epoch 75/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6959 - val_loss: 1.7620\n",
      "Epoch 76/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6997 - val_loss: 1.7070\n",
      "Epoch 77/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7014 - val_loss: 1.6757\n",
      "Epoch 78/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7021 - val_loss: 1.6886\n",
      "Epoch 79/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6996 - val_loss: 1.6798\n",
      "Epoch 80/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6960 - val_loss: 1.7425\n",
      "Epoch 81/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6943 - val_loss: 1.9140\n",
      "Epoch 82/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.7038 - val_loss: 1.7148\n",
      "Epoch 83/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6933 - val_loss: 1.7215\n",
      "Epoch 84/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6970 - val_loss: 1.6992\n",
      "Epoch 85/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6922 - val_loss: 1.6836\n",
      "Epoch 86/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6971 - val_loss: 1.6981\n",
      "Epoch 87/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6967 - val_loss: 1.6822\n",
      "Epoch 88/200\n",
      "1737/1757 [============================>.] - ETA: 0s - loss: 1.6962\n",
      " Reduced learning rate to 0.01\n",
      "1757/1757 [==============================] - 5s - loss: 1.6967 - val_loss: 1.7121\n",
      "Epoch 89/200\n",
      "1746/1757 [============================>.] - ETA: 0s - loss: 1.6625\n",
      " Reduced learning rate to 0.005\n",
      "1757/1757 [==============================] - 4s - loss: 1.6623 - val_loss: 1.6762\n",
      "Epoch 90/200\n",
      "1751/1757 [============================>.] - ETA: 0s - loss: 1.6572\n",
      " Reduced learning rate to 0.0025\n",
      "1757/1757 [==============================] - 4s - loss: 1.6570 - val_loss: 1.6769\n",
      "Epoch 91/200\n",
      "1751/1757 [============================>.] - ETA: 0s - loss: 1.6600\n",
      " Reduced learning rate to 0.00125\n",
      "1757/1757 [==============================] - 4s - loss: 1.6600 - val_loss: 1.6821\n",
      "Epoch 92/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6596 - val_loss: 1.6703\n",
      "Epoch 93/200\n",
      "1748/1757 [============================>.] - ETA: 0s - loss: 1.6579\n",
      " Reduced learning rate to 0.000625\n",
      "1757/1757 [==============================] - 4s - loss: 1.6579 - val_loss: 1.6782\n",
      "Epoch 94/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6634 - val_loss: 1.6680\n",
      "Epoch 95/200\n",
      "1748/1757 [============================>.] - ETA: 0s - loss: 1.6582\n",
      " Reduced learning rate to 0.0003125\n",
      "1757/1757 [==============================] - 4s - loss: 1.6580 - val_loss: 1.6716\n",
      "Epoch 96/200\n",
      "1750/1757 [============================>.] - ETA: 0s - loss: 1.6579\n",
      " Reduced learning rate to 0.00015625\n",
      "1757/1757 [==============================] - 4s - loss: 1.6579 - val_loss: 1.6718\n",
      "Epoch 97/200\n",
      "1754/1757 [============================>.] - ETA: 0s - loss: 1.6601\n",
      " Reduced learning rate to 7.8125e-05\n",
      "1757/1757 [==============================] - 4s - loss: 1.6599 - val_loss: 1.6692\n",
      "Epoch 98/200\n",
      "1741/1757 [============================>.] - ETA: 0s - loss: 1.6586\n",
      " Reduced learning rate to 3.90625e-05\n",
      "1757/1757 [==============================] - 4s - loss: 1.6590 - val_loss: 1.6719\n",
      "Epoch 99/200\n",
      "1739/1757 [============================>.] - ETA: 0s - loss: 1.6582\n",
      " Reduced learning rate to 1.95312e-05\n",
      "1757/1757 [==============================] - 4s - loss: 1.6580 - val_loss: 1.6733\n",
      "Epoch 100/200\n",
      "1743/1757 [============================>.] - ETA: 0s - loss: 1.6587\n",
      " Reduced learning rate to 9.76562e-06\n",
      "1757/1757 [==============================] - 4s - loss: 1.6586 - val_loss: 1.6722\n",
      "Epoch 101/200\n",
      "1744/1757 [============================>.] - ETA: 0s - loss: 1.6594\n",
      " Reduced learning rate to 4.88281e-06\n",
      "1757/1757 [==============================] - 4s - loss: 1.6598 - val_loss: 1.6680\n",
      "Epoch 102/200\n",
      "1741/1757 [============================>.] - ETA: 0s - loss: 1.6551\n",
      " Reduced learning rate to 2.44141e-06\n",
      "1757/1757 [==============================] - 4s - loss: 1.6549 - val_loss: 1.6680\n",
      "Epoch 103/200\n",
      "1742/1757 [============================>.] - ETA: 0s - loss: 1.6583\n",
      " Reduced learning rate to 1.2207e-06\n",
      "1757/1757 [==============================] - 4s - loss: 1.6582 - val_loss: 1.6782\n",
      "Epoch 104/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6603 - val_loss: 1.6676\n",
      "Epoch 105/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6551 - val_loss: 1.6665\n",
      "Epoch 106/200\n",
      "1757/1757 [==============================] - 4s - loss: 1.6568 - val_loss: 1.6617\n",
      "Epoch 107/200\n",
      "1739/1757 [============================>.] - ETA: 0s - loss: 1.6591\n",
      " Reduced learning rate to 6.10352e-07\n",
      "1757/1757 [==============================] - 4s - loss: 1.6587 - val_loss: 1.6720\n"
     ]
    }
   ],
   "source": [
    "n_train = [5000, 10000, 50000, 100000, 500000, 1000000]\n",
    "#n_train = [1000000]\n",
    "for n in n_train:\n",
    "    qrnn = QRNN(5, quantiles, 3, 128, ensemble_size = 5)\n",
    "    qrnn.fit(x_train[:n,:], y_train[:n], 1.0, initial_learning_rate = 0.01, minimum_learning_rate = 1e-5)\n",
    "    qrnn.save(\"qrnn_\"+ str(qrnn.input_dim) + \"_\" + str(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qrnn.model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sign([-0.0, 2342.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qrnn.model.get_losses_for(x_train[1:10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs =  qrnn.model.model._feed_inputs + qrnn.model.model._feed_targets + qrnn.model.model._feed_sample_weights\n",
    "f = K.function(inputs, [qrnn.model.model.total_loss])\n",
    "df = K.function(inputs, K.gradients([qrnn.model.model.total_loss], qrnn.model.input))\n",
    "\n",
    "df([x_train[1:4, :], y_train[1:4].reshape(-1,1), np.ones(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dx = np.zeros((3, 5))\n",
    "dx[0, 0] = 0.01\n",
    "f1 = f([x_train[1:4, :] + dx, y_train[1:4].reshape(-1,1), np.ones(1)])[0]\n",
    "f2 = f([x_train[1:4, :] - dx, y_train[1:4].reshape(-1,1), np.ones(1)])[0]\n",
    "(f1 - f2) / 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import keras.backend as K\n",
    "f = K.function([qrnn.model.input], [qrnn.model.model.total_loss])\n",
    "f([np.array([[1.0, 2.0, 3.0, 4.0, 5.0]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10  Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qrnn.model.save(\"qrnn_5_1000000_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = np.load(\"data/x_train_10.npy\")\n",
    "y_train = np.load(\"data/y_train_10.npy\")\n",
    "quantiles = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_train = [5000, 10000, 50000, 100000, 500000, 1000000]\n",
    "for n in n_train:\n",
    "    qrnn = QRNN(10, quantiles, 3, 256)\n",
    "    qrnn.fit(x_train[:n,:], y_train[:n], 1.0)\n",
    "    qrnn.save(\"qrnn_\"+ str(qrnn.input_dim) + \"_\" + str(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
